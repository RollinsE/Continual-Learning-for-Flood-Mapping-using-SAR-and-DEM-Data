# -*- coding: utf-8 -*-
"""CL_For_Flood_Extent_Mapping.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18eN8dFrQ7zv5qGuOFs13GQVaxmLlrgLV

### Continual Learning for Flood Extent Mapping using Sentinel-1 SAR and DEM Data, with Rehearsal and Uncertainty-Guided Sampling

This project implements a continual learning framework using Sentinel-1 SAR and DEM data for adaptive flood extent mapping. It investigates rehearsal-based strategies, comparing uncertainty-guided sampling (least confidence, margin, entropy) against random sampling and a non-CL baseline to improve generalization, mitigate catastrophic forgetting, and address class imbalance in dynamic flood scenarios.

Dataset url: https://drive.google.com/file/d/1ggEyYqmONE5Mc_LtgpvZBzmcoohfUZ1n/view?usp=sharing

# Prepare Notebook: Mount Google Drive, Copy Raw or Preprocessed Files
"""

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Copy already preprocessed, or raw data to local runtime
"""
TICK BOX IF YOU WANT TO USE PREPROCESSED DATA:
This will unzip already preprocessed files to the local runtime.
You can skip the preprocessing in Part 1 and move to Part 2 of the script.

LEAVE UNTICKED: This will unzip raw files to the
local runtime to be preprocessed. Do not skip Part 1!
"""
use_preprocessed_data = True  #@param {type:"boolean"}

if use_preprocessed_data:
  !unzip /content/drive/MyDrive/prep_roc5.zip -d /
  PREPROCESSED_DATA_DIR = '/content/prep_roc5'
else:
  !unzip /content/drive/MyDrive/mmflood.zip -d /content
  PREPROCESSED_DATA_DIR = "/content/prep_roc"

# Cell 1: Installs, Imports, Constants, Paths, Manual Device Setup

# --- Installations ---
!pip install -q torch torchvision torchaudio accelerate rasterio tqdm Pillow
!pip install -q -U Pillow albumentations
!pip install -q -U segmentation-models-pytorch timm

# --- Core Imports ---
import concurrent.futures
import logging
import os, glob, time, random, gc, traceback, re
from collections import deque
import numpy as np
import matplotlib.pyplot as plt
import rasterio
from rasterio.enums import Resampling
from rasterio.io import MemoryFile, DatasetReader
from rasterio.windows import Window
from rasterio.transform import Affine
import torch
import torch.nn as nn
import torch.optim as optim
from torch.amp import autocast
import torch.nn.functional as F
import torch.utils.data
from torch.utils.data import Dataset, DataLoader, Subset
from torch.utils.data import ConcatDataset
from torch.cuda.amp import GradScaler, autocast
import albumentations as A
from albumentations.pytorch import ToTensorV2
import copy
from tqdm.notebook import tqdm
import segmentation_models_pytorch as smp
import timm
import json
from datetime import datetime
import math
from pathlib import Path
from collections import Counter
from typing import Callable, Dict, Generator, List, Optional, Set, Tuple, Union
import cv2



# --- Setup Logging ---
logging.basicConfig(
    level=logging.INFO, format='[%(asctime)s] %(levelname)-8s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S',
    force=True
)
LOG = logging.getLogger("CL_FloodMapping")

LOG.info("Libraries Installed and Imported")

"""# PART 1: PREPROCESSING"""

# Cell P1: Configuration

# --- Input/Output Paths ---
DATA_SOURCE_DIR = Path("/content/mmflood") # Path to the raw data
ACTIVATIONS_JSON_PATH = Path("/content/activations.json") # Path to the JSON file containing activation info and train/val/test splits

# --- Processing Parameters ---
TARGET_SUBSETS = {'train', 'test', 'val'}
TILE_SIZE = 512
TILE_MAX_OVERLAP_THRESHOLD = 128
APPLY_MORPHOLOGY = True
MORPH_KERNEL_SIZE = 5
APPLY_DECIBEL = True
CLIP_DEM = True
NAN_FILTER_THRESHOLD = 0.75
F16_EPS = 1e-7

# --- Create Output Directory ---
os.makedirs(PREPROCESSED_DATA_DIR, exist_ok=True)

# --- Logs ---
LOG.info(f"Source Data Directory: {DATA_SOURCE_DIR}")
LOG.info(f"Activation File: {ACTIVATIONS_JSON_PATH}")
LOG.info(f"Processed Data Directory: {PREPROCESSED_DATA_DIR}")
LOG.info(f"Tile Size: {TILE_SIZE}")
LOG.info(f"Dynamic Overlap Threshold: {TILE_MAX_OVERLAP_THRESHOLD}")
LOG.info(f"Apply Morphology: {APPLY_MORPHOLOGY} (Kernel: {MORPH_KERNEL_SIZE})")
LOG.info(f"Apply Decibel Conversion: {APPLY_DECIBEL}")
LOG.info(f"Clip DEM: {CLIP_DEM}")
LOG.info(f"NaN Filter Threshold: {NAN_FILTER_THRESHOLD}")

# Cell P2: Helper Functions


def no_op(data, **kwargs):
  # A function that does nothing, used as a default
  return data


def count_files_recursive(path):
    """Counts the number of files recursively in a directory."""
    count = 0
    for root, _, files in os.walk(path):
        count += len(files)
    return count


def ensure_dir(path):
    """Checks if a directory exists, creates it if not, returns Path object."""
    # Convert to Path object if input is a string
    path = Path(path)

    path.mkdir(parents=True, exist_ok=True)
    return path

def read_geotiff(path, channels_first: bool = True):
    """Reads a GeoTIFF file, returns image array and profile metadata."""
    try:
        with rasterio.open(str(path), mode="r") as src:
            image = src.read()
            profile = src.profile.copy()
            # Ensure driver is GTiff, remove potentially problematic block/tiled info
            profile['driver'] = 'GTiff'
            profile.pop('blockxsize', None)
            profile.pop('blockysize', None)
            profile.pop('tiled', None)
        image_out = image if channels_first else np.moveaxis(image, 0, -1)
        return image_out, profile
    except Exception as e:
        LOG.error(f"Error reading GeoTIFF {path}: {e}")
        raise

def save_geotiff_window(output_path, source_profile, source_transform, window, data_to_write):
    """Writes data from a window to a new GeoTIFF file."""
    try:
        window_transform = rasterio.windows.transform(window, source_transform)
        profile = source_profile.copy()
        profile.update({
            'height': window.height,
            'width': window.width,
            'transform': window_transform,
            'count': data_to_write.shape[0], # Channels first assumed for data_to_write
            'dtype': data_to_write.dtype
        })
        with rasterio.open(str(output_path), "w", **profile) as dst:
            dst.write(data_to_write)
    except Exception as e:
        LOG.error(f"Error writing window to {output_path}: {e}")
        raise

def apply_mask_to_raster(raster_path, mask, fill_value):
    """Applies a mask to an existing raster file in place."""
    try:
        with rasterio.open(str(raster_path), mode="r+") as dst: # Open in read/write
            current_data = dst.read()
            if not (current_data.ndim == 3 and mask.ndim == 2 and current_data.shape[1:] == mask.shape):
                 raise ValueError(f"Mask shape {mask.shape} incompatible with raster shape {current_data.shape} for {raster_path}")
            current_data[:, mask] = fill_value
            dst.write(current_data)
    except Exception as e:
        LOG.error(f"Error applying mask to {raster_path}: {e}")
        raise

def convert_to_decibel(data):
    """Applies log10 transformation, ensuring input > 0."""
    data_float = data.astype(np.float32)
    return np.log10(1 + data_float + F16_EPS)

def clip_elevation(data):
    """Clips DEM values between -100 and 6000."""
    return np.clip(data, -100, 6000)

LOG.info("Helper functions defined.")

# Cell P3: Data Exploration

LOG.info("Starting Data Exploration")

# --- 1. Exploration from ACTIVATIONS_JSON_PATH ---
LOG.info("Activation Data Summary")
LOG.info("====================================")
if 'ACTIVATIONS_JSON_PATH' in globals() and isinstance(ACTIVATIONS_JSON_PATH, Path) and ACTIVATIONS_JSON_PATH.exists():
    try:
        with open(ACTIVATIONS_JSON_PATH, 'r', encoding='utf-8') as f:
            activations_data = json.load(f)

        # Total number of activations
        num_activations = len(activations_data)
        LOG.info(f"Total number of activations: {num_activations}")

        activations_by_subset = Counter(subsets)
        LOG.info("Activations per subset:")
        LOG.info(f"Number of 'train' data entries: {activations_by_subset.get('train', 0)}")
        LOG.info(f"Number of 'test' data entries: {activations_by_subset.get('test', 0)}")
        LOG.info(f"Number of 'val' data entries: {activations_by_subset.get('val', 0)}")

        # Activations by year
        years = []
        countries = []
        subsets = []
        missing_start_date_count = 0
        missing_country_count = 0
        missing_subset_count = 0

        for activation_id, details in activations_data.items():
            if 'start' in details and details['start']:
                try:
                    years.append(details['start'][:4]) # Extract YYYY
                except TypeError: # Handles if details['start'] is not a string or None
                    LOG.warning(f"Malformed 'start' date for activation {activation_id}: {details['start']}")
                    missing_start_date_count +=1
            else:
                missing_start_date_count += 1

            # Activations by location (country)
            if 'country' in details and details['country']:
                countries.append(details['country'])
            else:
                missing_country_count += 1

            # Activations by subset (train, test, val)
            if 'subset' in details and details['subset']:
                subsets.append(details['subset'])
            else:
                missing_subset_count += 1

        if missing_start_date_count > 0:
            LOG.warning(f"{missing_start_date_count} activations are missing 'start' date information or it's malformed.")
        if missing_country_count > 0:
            LOG.warning(f"{missing_country_count} activations are missing 'country' information.")
        if missing_subset_count > 0:
            LOG.warning(f"{missing_subset_count} activations are missing 'subset' information.")

        activations_by_year = Counter(years)
        LOG.info("------------------------------------")
        LOG.info("Activations per year:")
        LOG.info("------------------------------------")
        for year, count in sorted(activations_by_year.items()):
            LOG.info(f"{year}: {count}")

        activations_by_country = Counter(countries)
        LOG.info("------------------------------------")
        LOG.info("Activations per location:")
        LOG.info("------------------------------------")
        for country, count in sorted(activations_by_country.items(), key=lambda item: item[1], reverse=True): # Sort by count desc
            LOG.info(f"{country}: {count}")
        LOG.info(f'Total number of locations: {len(activations_by_country)}')

    except json.JSONDecodeError:
        LOG.error(f"Could not decode JSON from {ACTIVATIONS_JSON_PATH}. Please check the file content and format.")
    except Exception as e:
        LOG.error(f"An unexpected error occurred while processing {ACTIVATIONS_JSON_PATH}: {e}", exc_info=True)
elif 'ACTIVATIONS_JSON_PATH' in globals() and isinstance(ACTIVATIONS_JSON_PATH, Path):
    LOG.error(f"Activations JSON file not found at the specified path: {ACTIVATIONS_JSON_PATH}")
elif 'ACTIVATIONS_JSON_PATH' not in globals():
    LOG.error("ACTIVATIONS_JSON_PATH variable is not defined. Please ensure it's set in a previous cell.")
else: # ACTIVATIONS_JSON_PATH is defined but not a Path object
    LOG.error(f"ACTIVATIONS_JSON_PATH is not a Path object. Current type: {type(ACTIVATIONS_JSON_PATH)}. Please ensure it's set correctly.")

# --- 2. Exploration of EMSR folders in mmflood ---
LOG.info("------------------------------------")
LOG.info("EMSR Folder Summary")
LOG.info("------------------------------------")
if 'DATA_SOURCE_DIR' in globals() and isinstance(DATA_SOURCE_DIR, Path):
    if DATA_SOURCE_DIR.exists() and DATA_SOURCE_DIR.is_dir():
        try:
            emsr_folders = [d for d in DATA_SOURCE_DIR.iterdir() if d.is_dir() and d.name.startswith('EMSR')]
            num_emsr_folders = len(emsr_folders)
            LOG.info(f"Number of EMSR folders found in '{DATA_SOURCE_DIR}': {num_emsr_folders}")
        except Exception as e:
            LOG.error(f"Error listing EMSR folders in '{DATA_SOURCE_DIR}': {e}", exc_info=True)
    elif not DATA_SOURCE_DIR.exists():
        LOG.error(f"The specified data source directory '{DATA_SOURCE_DIR}' does not exist.")
    else:
        LOG.error(f"The specified data source path '{DATA_SOURCE_DIR}' is not a directory.")
elif 'DATA_SOURCE_DIR' not in globals():
    LOG.error("DATA_SOURCE_DIR variable is not defined. Please ensure it's set in a previous cell.")
else: # DATA_SOURCE_DIR is defined but not a Path object
    LOG.error(f"DATA_SOURCE_DIR is not a Path object. Current type: {type(DATA_SOURCE_DIR)}. Please ensure it's set correctly.")

LOG.info("--- Data Exploration Complete ---")

# Cell P4: Tiling Functions

def create_kernel(kernel_size):
    """Creates a kernel for morphology."""
    center = kernel_size // 2
    radius = min(center, kernel_size - center)
    y, x = np.ogrid[:kernel_size, :kernel_size]
    dist_from_center = np.sqrt((x - center)**2 + (y - center)**2)
    mask = dist_from_center <= radius
    return mask.astype(np.uint8)

def apply_morphology(image_data, kernel_size):
    """Applies morphological closing to a single-channel mask."""
    if image_data.shape[0] != 1:
        LOG.warning(f"Morphology function expected single channel, got {image_data.shape[0]}. Using first channel.")
    mask_2d = image_data[0].astype(np.uint8)
    kernel = create_kernel(kernel_size)
    closed_mask = cv2.morphologyEx(mask_2d, cv2.MORPH_CLOSE, kernel)
    return np.expand_dims((closed_mask > 0).astype(np.uint8), axis=0)

def generate_dynamic_windows(image_shape, tile_shape, overlap_threshold):
    """
    Generates overlapping tile windows
    """
    height, width = image_shape
    tile_h, tile_w = tile_shape
    proc_height = max(height, tile_h)
    proc_width = max(width, tile_w)

    # Calculate tiling parameters
    tile_count_h = math.ceil(proc_height / tile_h)
    tile_count_w = math.ceil(proc_width / tile_w)
    remainder_h = (tile_count_h * tile_h) - proc_height
    remainder_w = (tile_count_w * tile_w) - proc_width
    num_gaps_h = max(0, tile_count_h - 1)
    num_gaps_w = max(0, tile_count_w - 1)
    overlap_h = math.floor(remainder_h / num_gaps_h) if num_gaps_h > 0 else 0
    overlap_w = math.floor(remainder_w / num_gaps_w) if num_gaps_w > 0 else 0

    # Apply overlap threshold logic and calculate offset
    offset_h, offset_w = 0, 0
    if num_gaps_h > 0 and overlap_h >= overlap_threshold:
        LOG.debug(f"H overlap ({overlap_h}) >= threshold ({overlap_threshold}), reducing tile count & adding offset.")
        tile_count_h -= 1
        overlap_h = 0
        offset_h = (proc_height - (tile_count_h * tile_h)) // 2
    if num_gaps_w > 0 and overlap_w >= overlap_threshold:
        LOG.debug(f"W overlap ({overlap_w}) >= threshold ({overlap_threshold}), reducing tile count & adding offset.")
        tile_count_w -= 1
        overlap_w = 0
        offset_w = (proc_width - (tile_count_w * tile_w)) // 2 #

    # Iterate and calculate window coordinates
    for row in range(tile_count_h):
        for col in range(tile_count_w):
            # Calculate top-left coordinates (x=row_offset, y=col_offset)
            x = max(row * tile_h - overlap_h * row, 0) + offset_h
            y = max(col * tile_w - overlap_w * col, 0) + offset_w
            # Apply edge handling
            if (x + tile_h) > height: # Use original image height for boundary check
                 x -= abs(x + tile_h - height)
            if (y + tile_w) > width: # Use original image width for boundary check
                 y -= abs(y + tile_w - width)

            # Ensure coordinates are non-negative after adjustment
            x = max(0, x)
            y = max(0, y)

            # Create rasterio Window (col_off, row_off, width, height)
            # Dimensions shouldn't exceed original image boundaries if start was adjusted
            window_h = min(tile_h, height - x)
            window_w = min(tile_w, width - y)
            window = Window(y, x, window_w, window_h)

            # Window dimensions shouldn't be 0 or -ve
            if window.width > 0 and window.height > 0:
                 yield (row, col), window
            else:
                 LOG.warning(f"Skipping zero-dimension window at row {row}, col {col} for image shape {image_shape}")


def generate_full_image_window(image_shape, **kwargs):
    """Yields a single window covering the whole image. Ignores tile_shape/overlap."""
    height, width = image_shape
    yield (0, 0), Window(0, 0, width, height)


def process_and_tile_image(source_path, output_dir, image_type, processing_func, resampling_method, tile_generator_func, tile_shape, overlap_thresh_for_dynamic, fill_value):
    """Reads, processes, tiles, and saves one image."""
    image_id = source_path.stem
    num_tiles_saved = 0
    try:
        # Read source image and profile
        image_data, profile = read_geotiff(source_path, channels_first=True)
        img_h, img_w = image_data.shape[1:]
        source_transform = profile['transform'] # Get original transform

        # Apply per-image processing
        process_kwargs = {}
        if processing_func.__name__ == 'apply_morphology':
             process_kwargs['kernel_size'] = MORPH_KERNEL_SIZE
        processed_data = processing_func(image_data, **process_kwargs)

        # Update profile details for the MemoryFile
        mem_profile = profile.copy()
        mem_profile.update({
            'height': img_h,
            'width': img_w,
            'count': processed_data.shape[0],
            'dtype': processed_data.dtype,
            'nodata': fill_value
        })

        # Use MemoryFile
        with MemoryFile() as memfile:
            with memfile.open(**mem_profile) as mem_dataset:
                mem_dataset.write(processed_data)

                # Generate tile windows using the selected generator
                # Pass necessary args expected by the specific generator
                tiling_args = {
                    'image_shape': (img_h, img_w),
                    'tile_shape': tile_shape,
                    'overlap_threshold': overlap_thresh_for_dynamic # Pass threshold
                }
                tile_windows = tile_generator_func(**tiling_args)

                for (tile_row, tile_col), window in tile_windows:
                    # Ensure window offsets are integers
                    row_off, col_off = int(window.row_off), int(window.col_off)
                    win_h, win_w = int(window.height), int(window.width)

                    # Check if window is valid before reading
                    if win_w <= 0 or win_h <= 0:
                        LOG.warning(f"Skipping invalid window: {window} for {image_id}")
                        continue

                    # Read window data from the in-memory dataset
                    try:
                        window_data = mem_dataset.read(window=window)
                    except rasterio.errors.RasterioIOError as rio_err:
                        LOG.error(f"Rasterio read error for window {window} on {image_id}: {rio_err}")
                        continue # Skip this tile

                    # Construct output path
                    tile_filename = f"{image_id}_{row_off}_{col_off}.tif"
                    output_path = output_dir / tile_filename

                    # Save the tile window
                    save_geotiff_window(output_path=output_path,
                                        source_profile=mem_profile,
                                        source_transform=source_transform,
                                        window=window,
                                        data_to_write=window_data)
                    num_tiles_saved += 1

    except Exception as e:
        LOG.error(f"Failed to process/tile {source_path}: {e}", exc_info=True)

    return num_tiles_saved


LOG.info("Tiling Functions Defined.")

# Cell P5: Filtering Logic Functions

def get_emsr_code(path):
    """Extracts EMSR code from filename stem."""
    image_id_stem = path.stem
    parts = image_id_stem.split("_")[0].split("-")
    if len(parts) > 0 and parts[0].startswith("EMSR"):
        return parts[0]
    return image_id_stem # Fallback

def find_matching_files(sar_glob_pattern: str,
                        dem_glob_pattern: str,
                        mask_glob_pattern: str,
                        allowed_emsr_codes: Optional[Set[str]] = None,
                        check_stems_match: bool = True) -> List[Tuple[Path, Path, Path]]:
    """Finds triplets of SAR, DEM, Mask files, optionally filters and checks stems."""
    sar_files = sorted([Path(p) for p in glob.glob(sar_glob_pattern)])
    dem_files = sorted([Path(p) for p in glob.glob(dem_glob_pattern)])
    msk_files = sorted([Path(p) for p in glob.glob(mask_glob_pattern)])

    LOG.info(f"Found Raw Files - SAR: {len(sar_files)}, DEM: {len(dem_files)}, Mask: {len(msk_files)}")

    # Filter by allowed EMSR codes if provided
    valid_triplets = []
    if allowed_emsr_codes:
        # Create dictionaries for quick lookup by stem (assuming unique stems per type)
        sar_map = {p.stem: p for p in sar_files if get_emsr_code(p) in allowed_emsr_codes}
        dem_map = {p.stem: p for p in dem_files if get_emsr_code(p) in allowed_emsr_codes}
        msk_map = {p.stem: p for p in msk_files if get_emsr_code(p) in allowed_emsr_codes}
        # Find common stems
        common_stems = set(sar_map.keys()) & set(dem_map.keys()) & set(msk_map.keys())
        LOG.info(f"Found {len(common_stems)} common stems for allowed EMSR codes.")
        for stem in sorted(list(common_stems)):
            valid_triplets.append((sar_map[stem], dem_map[stem], msk_map[stem]))
    else:
        # If no filtering, assume lists are aligned (perform stem check if enabled)
        if not (len(sar_files) == len(dem_files) == len(msk_files)):
            raise ValueError("Mismatching file counts found and no subset filtering applied.")
        for s_f, d_f, m_f in zip(sar_files, dem_files, msk_files):
             if check_stems_match and not (s_f.stem == d_f.stem == m_f.stem):
                  LOG.warning(f"Stem mismatch detected: {s_f.stem}, {d_f.stem}, {m_f.stem}. Skipping triplet.")
                  continue
             valid_triplets.append((s_f, d_f, m_f))

    if not valid_triplets and (allowed_emsr_codes or sar_files): # Check if we expected files but got none
         LOG.warning("No valid file triplets found after filtering/checking.")

    return valid_triplets


def delete_files(paths_to_delete: List[Path]):
    """Safely deletes a list of files."""
    for path in paths_to_delete:
        try:
            if path.exists() and path.is_file():
                path.unlink()
        except OSError as e:
            LOG.error(f"Error deleting file {path}: {e}")

def filter_tiles_by_nan(subset_output_dir: Path, nan_threshold: float, tile_area: int, is_test_set: bool):
    """Filters generated tiles based on NaN content."""
    LOG.info(f"Starting NaN filtering for {subset_output_dir.name}...")
    sar_dir = subset_output_dir / "sar"
    dem_dir = subset_output_dir / "dem"
    mask_dir = subset_output_dir / "mask"

    # Gather tiles using find_matching_files logic (stem check off)
    tile_triplets = find_matching_files(str(sar_dir / "*.tif"),
                                        str(dem_dir / "*.tif"),
                                        str(mask_dir / "*.tif"),
                                        check_stems_match=False)

    removed_count = 0
    valid_count = 0
    fill_values = {'sar': 0, 'dem': 0, 'mask': 255} # Fill values based on ImageType enum

    for sar_path, dem_path, mask_path in tqdm(tile_triplets, desc=f"NaN Filtering {subset_output_dir.name}                  "):
        try:
            # Read only SAR for NaN check (as per original logic)
            sar_data, _ = read_geotiff(sar_path, channels_first=True)
            # Sum channels and check for NaNs
            nan_mask = np.isnan(sar_data.sum(axis=0))
            empty_pixels = np.count_nonzero(nan_mask)
            nan_ratio = empty_pixels / float(tile_area) if tile_area > 0 else 0

            # Apply deletion condition (only for non-test sets)
            if not is_test_set and nan_ratio >= nan_threshold:
                delete_files([sar_path, dem_path, mask_path])
                removed_count += 1
            else:
                # If kept and has NaNs, apply mask
                if empty_pixels > 0:
                    apply_mask_to_raster(sar_path, nan_mask, fill_values['sar'])
                    apply_mask_to_raster(dem_path, nan_mask, fill_values['dem'])
                    apply_mask_to_raster(mask_path, nan_mask, fill_values['mask'])
                valid_count += 1
        except Exception as e:
            LOG.error(f"Error during NaN filter for {sar_path.name}: {e}. Removing triplet.")
            delete_files([sar_path, dem_path, mask_path])
            removed_count += 1

    total_processed = valid_count + removed_count
    kept_perc = (valid_count / total_processed * 100.0) if total_processed > 0 else 0.0
    LOG.info(f"NaN Filtering Complete for {subset_output_dir.name}: Kept={valid_count}, Removed={removed_count} ({kept_perc:.2f}% Kept)")


LOG.info("Filtering logic functions defined.")

# Cell P6: Main Execution Pipeline

LOG.info("Starting Preprocessing Pipeline")
LOG.info("===============================================================================")

# 1. Load Activation Information
try:
    with open(ACTIVATIONS_JSON_PATH, "r", encoding="utf-8") as f:
        activation_data = json.load(f)
    # Create mapping: EMSR code (e.g., EMSR548) -> subset ('train', 'val', 'test')
    emsr_to_subset = {k: v.get("subset", "unknown") for k, v in activation_data.items()}
    subset_counts = Counter(emsr_to_subset.values())
    LOG.info(f"Loaded activation info. Subset counts: {dict(subset_counts)}")
    if "unknown" in subset_counts:
         LOG.warning(f"{subset_counts['unknown']} activations have no 'subset' defined.")
except Exception as e:
    LOG.error(f"Failed to load or parse activation file {ACTIVATIONS_JSON_PATH}: {e}")
    raise # Stop execution if activation file is critical

# 2. Process Each Target Subset
for subset_name in TARGET_SUBSETS:
    LOG.info(f"Processing Subset: {subset_name}")
    LOG.info("----------------------------------------------------------------------------")
    subset_output_dir = ensure_dir(Path(PREPROCESSED_DATA_DIR) / subset_name)
    is_test = (subset_name == 'test')

    # Get EMSR codes for this subset (e.g., {'EMSR345', 'EMSR400'})
    emsr_codes_for_subset = {k for k, v in emsr_to_subset.items() if v == subset_name}
    if not emsr_codes_for_subset:
        LOG.warning(f"No activations found for subset '{subset_name}'. Skipping.")
        continue
    LOG.info(f"Found {len(emsr_codes_for_subset)} EMSR codes for subset '{subset_name}'.")

    # Define glob patterns based on data structure description
    emsr_patterns = [f"{code}-*" for code in emsr_codes_for_subset]

    # Find all matching raw file triplets for the subset
    LOG.info("Gathering raw input files for the subset...")
    raw_sar_glob = str(DATA_SOURCE_DIR / "EMSR*" / "s1_raw" / "*.tif")
    raw_dem_glob = str(DATA_SOURCE_DIR / "EMSR*" / "DEM" / "*.tif")
    raw_mask_glob = str(DATA_SOURCE_DIR / "EMSR*" / "mask" / "*.tif")

    raw_file_triplets = find_matching_files(raw_sar_glob, raw_dem_glob, raw_mask_glob,
                                            allowed_emsr_codes=emsr_codes_for_subset,
                                            check_stems_match=True)

    if not raw_file_triplets:
         LOG.warning(f"No matching raw file triplets found for subset '{subset_name}'. Skipping tiling.")
         continue
    LOG.info(f"Found {len(raw_file_triplets)} raw file triplets for subset '{subset_name}'.")

    # 3. Tiling & Initial Processing
    # Define Tiler function and processing functions based on subset
    tile_shape = (TILE_SIZE, TILE_SIZE)
    if is_test:
        tile_generator = generate_full_image_window
        LOG.info("Using full image window generation for test set.")
    else:
        tile_generator = generate_dynamic_windows
        LOG.info(f"Using dynamic overlap window generation (Overlap Threshold: {TILE_MAX_OVERLAP_THRESHOLD}) for {subset_name} set.")

    # Define processing functions based on config
    sar_processing_func = convert_to_decibel if APPLY_DECIBEL else no_op
    dem_processing_func = clip_elevation if CLIP_DEM else no_op
    mask_processing_func = apply_morphology if APPLY_MORPHOLOGY else no_op # apply_morphology checks flag internally

    # Create output subdirs (sar, dem, mask)
    sar_out = ensure_dir(subset_output_dir / "sar")
    dem_out = ensure_dir(subset_output_dir / "dem")
    mask_out = ensure_dir(subset_output_dir / "mask")

    LOG.info(f"Starting tiling for {len(raw_file_triplets)} triplets...")
    # Process triplets
    total_tiles_generated = Counter() # Keep track per type
    for sar_path, dem_path, mask_path in tqdm(raw_file_triplets, desc=f"Tiling {subset_name}                         "):
        # Process SAR
        total_tiles_generated['sar'] += process_and_tile_image(
            source_path=sar_path, output_dir=sar_out, image_type='sar',
            processing_func=sar_processing_func, resampling_method=Resampling.bilinear,
            tile_generator_func=tile_generator, tile_shape=tile_shape,
            overlap_thresh_for_dynamic=TILE_MAX_OVERLAP_THRESHOLD, fill_value=0)
        # Process DEM
        total_tiles_generated['dem'] += process_and_tile_image(
            source_path=dem_path, output_dir=dem_out, image_type='dem',
            processing_func=dem_processing_func, resampling_method=Resampling.bilinear,
            tile_generator_func=tile_generator, tile_shape=tile_shape,
            overlap_thresh_for_dynamic=TILE_MAX_OVERLAP_THRESHOLD, fill_value=0)
        # Process Mask
        total_tiles_generated['mask'] += process_and_tile_image(
            source_path=mask_path, output_dir=mask_out, image_type='mask',
            processing_func=mask_processing_func, resampling_method=Resampling.nearest,
            tile_generator_func=tile_generator, tile_shape=tile_shape,
            overlap_thresh_for_dynamic=TILE_MAX_OVERLAP_THRESHOLD, fill_value=255) # Fill mask NaNs with 255

    LOG.info(f"Tiling complete for {subset_name}. Tiles generated: {dict(total_tiles_generated)}")
    if not (total_tiles_generated['sar'] == total_tiles_generated['dem'] == total_tiles_generated['mask']):
        LOG.warning("Mismatch in generated tile counts between types!")

    # 4. NaN Filtering
    if total_tiles_generated['sar'] > 0: # Only filter if tiles were generated
        filter_tiles_by_nan(subset_output_dir=subset_output_dir,
                            nan_threshold=NAN_FILTER_THRESHOLD,
                            tile_area=(TILE_SIZE * TILE_SIZE),
                            is_test_set=is_test)
    else:
        LOG.info(f"Skipping NaN filtering for {subset_name} as no tiles were generated.")

    LOG.info(f"Finished Processing Subset: {subset_name}")

LOG.info("----------------------------------------------------------------------------")
LOG.info("Preprocessing Pipeline Complete")

"""# PART 2: MODEL TRAINING & EVALUATION"""

# Cell 1A: Imports, Constants, Paths, Manual Device Setup

# --- Data Paths ---
TRAIN_DIR = os.path.join(PREPROCESSED_DATA_DIR, 'train')
VAL_DIR = os.path.join(PREPROCESSED_DATA_DIR, 'val')
TEST_DIR = os.path.join(PREPROCESSED_DATA_DIR, 'test')
SAVED_MODELS_DIR = '/content/drive/MyDrive/trained_models/new_amp/'
ACTIVATIONS_JSON_PATH = '/content/drive/MyDrive/mmflood/activations.json'


# --- Model & Training Params ---
INPUT_CHANNELS = 3
OUTPUT_CLASSES = 1
IMAGE_HEIGHT = 512
IMAGE_WIDTH = 512
BATCH_SIZE = 16
LEARNING_RATE = 1e-4
PATIENCE = 7
SAVE_EPOCH_CHECKPOINTS = True # Or False, to not save epoch checkpoints
NUM_TASKS = 10
EPOCHS_BASELINE = 30
EPOCHS_PER_TASK = 5
REPLAY_BUFFER_SIZE = 100
REPLAY_BATCH_SIZE = 16

# Fraction of the replay buffer to score for uncertainty sampling (1.0 = full, < 1.0 = approximate)
# Implemented to see the effect on training time. Used 0.5 for earlier experiments but discarded those and went with 1.0 for the full replay buffer
UNCERTAINTY_SUBSET_FRACTION = 1.0
LOG.info(f"Uncertainty Sampling Subset Fraction set to: {UNCERTAINTY_SUBSET_FRACTION}")

# --- Encoder and Weights Selection ---
# Choose your Encoder and Pretrained Weights here
# Options: 'resnet18', 'resnet34', 'timm-tf_efficientnet_lite1', 'resnet50', 'mobilenet_v2', 'densenet121', 'tresnet_m', 'tresnet_l', 'tresnet_xl', 'tresnet_m_448'
ENCODER_NAME = 'resnet34'
# Options: 'imagenet', None (train from scratch)
ENCODER_WEIGHTS = 'imagenet'
LOG.info(f"Selected Encoder: {ENCODER_NAME}, Weights: {ENCODER_WEIGHTS}")

# --- Enable/Disable Early Stopping
USE_EARLY_STOPPING = True # Set to False to disable, True to enable
LOG.info(f"Early Stopping Enabled: {USE_EARLY_STOPPING}")

# Loss
LOSS_TYPE = 'tversky'
FOCAL_GAMMA = 2.0
TVERSKY_ALPHA = 0.3
TVERSKY_BETA = 0.7

# Reproducibility
SEED = 42
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)

LOG.info("Configuration Constants Set.")

# --- Manual Device Setup ---
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
LOG.info(f"Using device: {DEVICE}")
if DEVICE.type == 'cuda':
  torch.cuda.manual_seed_all(SEED)

# --- Create Save Directory ---
os.makedirs(SAVED_MODELS_DIR, exist_ok=True)
LOG.info(f"Model weights will be saved in: {SAVED_MODELS_DIR}")

# Cell 1B: Utility Functions

def find_latest_checkpoint(base_save_path):
    """ Finds the latest epoch checkpoint file based on task/epoch numbers in filename."""
    base_name = os.path.splitext(base_save_path)[0]; search_pattern = f"{base_name}_task*_epoch*_ckpt.pth"; checkpoint_files = glob.glob(search_pattern)
    latest_checkpoint, latest_task, latest_epoch = None, -1, -1
    if not checkpoint_files: LOG.info(f"No epoch checkpoints found"); return None
    LOG.info(f"Found {len(checkpoint_files)} potential checkpoints for {os.path.basename(base_name)}"); pattern = re.compile(r"_task(\d+)_epoch(\d+)_ckpt\.pth$")
    for ckpt_file in checkpoint_files:
        match = pattern.search(ckpt_file)
        if match:
            try: task_num, epoch_num = int(match.group(1)), int(match.group(2)); current_task_k, current_epoch_e = task_num - 1, epoch_num - 1
            except (ValueError, IndexError) as e: LOG.warning(f"Parse error '{os.path.basename(ckpt_file)}': {e}"); continue
            if current_task_k > latest_task or (current_task_k == latest_task and current_epoch_e > latest_epoch): latest_task, latest_epoch, latest_checkpoint = current_task_k, current_epoch_e, ckpt_file
        else: LOG.warning(f"Filename pattern mismatch: '{os.path.basename(ckpt_file)}'")
    if latest_checkpoint: LOG.info(f"Latest checkpoint: {os.path.basename(latest_checkpoint)} (T:{latest_task+1}, E:{latest_epoch+1})")
    else: LOG.info(f"Could not determine latest checkpoint.")
    return latest_checkpoint
LOG.info("find_latest_checkpoint helper function defined.")


def get_split_data_paths_mmflood(split_dir):
    sar_dir = os.path.join(split_dir, 'sar'); dem_dir = os.path.join(split_dir, 'dem'); mask_dir = os.path.join(split_dir, 'mask')
    LOG.info(f"Looking for data in: {sar_dir}, {dem_dir}, {mask_dir}")
    if not all(os.path.exists(d) for d in [sar_dir, dem_dir, mask_dir]): LOG.warning(f"WARNING: Missing subdirs in {split_dir}."); return [], [], []
    sar_paths = sorted(glob.glob(os.path.join(sar_dir, '*.tif')));
    if not sar_paths: LOG.warning(f"WARNING: No .tif files in {sar_dir}"); return [], [], []
    dem_paths = []; mask_paths = []; valid_sar_paths = []
    for sar_p in sar_paths:
        base_name = os.path.basename(sar_p); dem_p = os.path.join(dem_dir, base_name); mask_p = os.path.join(mask_dir, base_name)
        if os.path.exists(dem_p) and os.path.exists(mask_p): valid_sar_paths.append(sar_p); dem_paths.append(dem_p); mask_paths.append(mask_p)
    LOG.info(f"Found {len(valid_sar_paths)} valid SAR/DEM/mask triplets in {split_dir}.")
    return valid_sar_paths, dem_paths, mask_paths
LOG.info("Defined get_split_data_paths_mmflood.")

def collate_fn_skip_error(batch):
    """Custom collate function to handle errors during batch creation."""
    original_count = len(batch); batch = [item for item in batch if item is not None and item[0].nelement() > 0 and item[1].nelement() > 0]; filtered_count = len(batch)
    if not batch: return torch.empty((0, INPUT_CHANNELS, IMAGE_HEIGHT, IMAGE_WIDTH)), torch.empty((0, OUTPUT_CLASSES, IMAGE_HEIGHT, IMAGE_WIDTH))
    try: return torch.utils.data.dataloader.default_collate(batch)
    except Exception as e: LOG.error(f"Error during default_collate: {e}"); return torch.empty((0, INPUT_CHANNELS, IMAGE_HEIGHT, IMAGE_WIDTH)), torch.empty((0, OUTPUT_CLASSES, IMAGE_HEIGHT, IMAGE_WIDTH))
LOG.info("Defined collate_fn_skip_error.")

def create_poly_optimizer(model, base_lr=LEARNING_RATE, weight_decay=1e-4):
    """Creates an Adam optimizer."""
    LOG.info(f"Creating Adam optimizer base_lr={base_lr}, weight_decay={weight_decay}")
    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=base_lr, weight_decay=weight_decay)
    return optimizer
LOG.info("Defined create_poly_optimizer.")

# --- Robust Scaling ---
def robust_scale(channel_data, q_min=1.0, q_max=99.0, p_min=None, p_max=None):
    """
    Clips to percentiles and scales channel data to [0, 1].
    """
    # Calculate percentiles only if not provided
    if p_min is None or p_max is None:
        # Fallback to runtime calculation if pre-calculated values are missing
        p_min_calc, p_max_calc = np.percentile(channel_data[~np.isnan(channel_data)], [q_min, q_max])
    else:
        p_min_calc, p_max_calc = p_min, p_max # Use pre-calculated

    channel_data_clipped = np.clip(channel_data, p_min_calc, p_max_calc)
    # Handle potential division by zero if p_min == p_max
    scale = (p_max_calc - p_min_calc) if not np.isclose(p_max_calc, p_min_calc) else 1.0
    # Prevent division by zero explicitly
    if np.isclose(scale, 0.0):
        # If min and max are the same, the data is constant after clipping.
        # Return 0.0 or 0.5, depending on desired behavior for constant input. 0.0 is safer.
        return np.zeros_like(channel_data_clipped, dtype=np.float32)
    else:
        return ((channel_data_clipped - p_min_calc) / scale).astype(np.float32)
LOG.info("Defined robust_scale helper (modified for pre-calculated percentiles).")


def get_emsr_id_from_path(filepath):
    """Extracts 'EMSRXXX' from a filepath string."""
    match = re.search(r'(EMSR\d+)', os.path.basename(filepath))
    return match.group(1) if match else None
LOG.info("Defined get_emsr_id_from_path.")

def parse_datetime(date_string):
    """Parses the datetime string from the JSON file."""
    if not date_string: return None
    try: return datetime.strptime(date_string, '%Y-%m-%dT%H:%M:%S')
    except ValueError: LOG.warning(f"Could not parse date string: {date_string}"); return None
LOG.info("Defined parse_datetime.")

# --- Helper Function to Convert NaN/Tensors ---
def json_serialize(obj):
    """Recursively converts NaN, numpy types, and Tensors in nested structures."""
    if isinstance(obj, dict):
        return {k: json_serialize(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [json_serialize(elem) for elem in obj]
    elif isinstance(obj, torch.Tensor):
        # Handle tensors: Convert to list for multi-element, item() for scalar
        return obj.tolist() if obj.numel() > 1 else obj.item()
    # Handle numpy NaN and Python float NaN
    elif isinstance(obj, (float, np.floating)) and math.isnan(obj):
        return None # Represent NaN as null in JSON
    elif isinstance(obj, np.integer):
        return int(obj) # Convert numpy integers to standard Python int
    # Add handling for numpy bool
    elif isinstance(obj, np.bool_):
         return bool(obj)
    # Add handling for Path objects if they sneak in
    elif isinstance(obj, Path):
         return str(obj)
    return obj
LOG.info("Defined json_serialize.")

# --- Save Function Definition (Modified) ---
def save_metrics_to_json(
    filename: str,
    run_name: str,
    run_result_summary: Optional[dict],
    run_cl_detail_matrix: Optional[list], # Specific matrix for this CL run
    run_history: Optional[dict], # Specific history for this run
    experiment_details: dict
):
    """Saves the metrics for a single specified run to its own JSON file."""
    LOG.info(f"Attempting to save metrics for run '{run_name}' to: {os.path.basename(filename)}")
    save_dir = os.path.dirname(filename)
    os.makedirs(save_dir, exist_ok=True)

    # Create the data structure specifically for this run
    run_snapshot_data = {
        "run_name": run_name,
        "experiment_details": experiment_details, # Include overall details for context
        "final_test_metrics": {}, # Initialize empty dicts
        "detailed_cl_matrix": [], # Initialize empty list/None
        "validation_history": {} # Initialize empty dict
    }

    try:
        # Safely populate with data if available, using json_serialize
        if run_result_summary:
            run_snapshot_data["final_test_metrics"] = json_serialize(run_result_summary)
        # Only include detailed matrix if it's relevant (i.e., for CL runs)
        if run_cl_detail_matrix:
             run_snapshot_data["detailed_cl_matrix"] = json_serialize(run_cl_detail_matrix)
        else:
             # Remove the key if no detailed matrix provided (e.g., for baseline)
             run_snapshot_data.pop("detailed_cl_matrix", None)
        if run_history:
            run_snapshot_data["validation_history"] = json_serialize(run_history)

        # Save the snapshot for this run, overwriting this specific file
        with open(filename, 'w') as f:
            json.dump(run_snapshot_data, f, indent=4)

        LOG.info(f"Metrics for run '{run_name}' saved successfully to: {os.path.basename(filename)}")
        return True # Indicate success

    except Exception as e:
        LOG.error(f"ERROR saving metrics for run '{run_name}' to {os.path.basename(filename)}: {e}\n{traceback.format_exc()}")
        return False # Indicate failure
LOG.info("Defined save_metrics_to_json.")

# --- Parallel JSON Loading ---
def load_json_file(f_path):
    try:
        with open(f_path, 'r') as f:
            data = json.load(f)
        run_name = data.get("run_name") or os.path.basename(f_path).replace(file_pattern_suffix, "")
        return {
            "data": data,
            "run_name": run_name,
            "error": None,
            "f_path": f_path
        }
    except Exception as e:
        return {
            "data": None,
            "run_name": None,
            "error": str(e),
            "f_path": f_path
        }
LOG.info("Defined load_json_file.")

LOG.info("Utilities Defined")

# Cell 1C: Verify DataPaths

# --- Load DataPaths ---
LOG.info(f"--- Loading DataPaths ---")
try:
    train_sar_paths, train_dem_paths, train_mask_paths = get_split_data_paths_mmflood(TRAIN_DIR)
    val_sar_paths, val_dem_paths, val_mask_paths = get_split_data_paths_mmflood(VAL_DIR)
    test_sar_paths, test_dem_paths, test_mask_paths = get_split_data_paths_mmflood(TEST_DIR)
    if not train_sar_paths:
      LOG.warning("CRITICAL WARNING: No training DataPaths found.")
      LOG.info("DataPaths loaded.")
except Exception as e:
     LOG.error(f"ERROR loading DataPaths: {e}")
     train_sar_paths, train_dem_paths, train_mask_paths = [], [], []
     val_sar_paths, val_dem_paths, val_mask_paths = [], [], []
     test_sar_paths, test_dem_paths, test_mask_paths = [], [], []

LOG.info("DataPaths Loaded")

# Cell 2: Statistics Calculation & Scaling Percentiles

# --- Helper Function: Statistics & Pos Weight ---
def calculate_stats_and_pos_weight_mmflood(sar_paths, dem_paths, mask_paths):
    """Calculates mean/std for SAR(VV,VH)+DEM and pos_weight using rasterio."""
    LOG.info("Calculating normalization stats (VV, VH, DEM) and pos_weight...")
    if not sar_paths or not dem_paths or not mask_paths:
        LOG.error("ERROR: Empty data paths. Returning defaults.")
        return None, None, torch.tensor([1.0], device=DEVICE)
    pixel_sum = torch.zeros(INPUT_CHANNELS, dtype=torch.float64)
    pixel_sum_sq = torch.zeros(INPUT_CHANNELS, dtype=torch.float64)
    total_pixels_per_channel = 0; positive_pixels = 0; total_mask_pixels = 0
    num_files = len(sar_paths)
    for i in tqdm(range(num_files), desc="Processing Stats (Mean/Std/Wgt)     "):
        sar_path, dem_path, mask_path = sar_paths[i], dem_paths[i], mask_paths[i]
        try:
            with rasterio.open(sar_path) as src_sar:
              sar_data = src_sar.read().astype(np.float32)
              assert src_sar.count == 2
            with rasterio.open(dem_path) as src_dem:
              dem_data = src_dem.read(1, out_shape=(1, sar_data.shape[1], sar_data.shape[2])).astype(np.float32)
              dem_data = np.expand_dims(dem_data, axis=0)
              assert src_dem.count == 1
            with rasterio.open(mask_path) as src_mask:
              mask_data = src_mask.read(1, out_shape=(sar_data.shape[1], sar_data.shape[2])).astype(np.uint8)
              assert src_mask.count == 1
            assert sar_data.shape[1:] == dem_data.shape[1:] == mask_data.shape
            img_combined_np = np.concatenate((sar_data, dem_data), axis=0)
            img_combined = torch.from_numpy(img_combined_np).to(torch.float64)
            img_combined = torch.nan_to_num(img_combined, nan=0.0, posinf=0.0, neginf=0.0)
            pixel_sum += img_combined.sum(dim=[1, 2])
            pixel_sum_sq += (img_combined ** 2).sum(dim=[1, 2])
            total_pixels_per_channel += img_combined.shape[1] * img_combined.shape[2]
            mask = torch.from_numpy(mask_data)
            positive_pixels += (mask > 0).sum().item()
            total_mask_pixels += mask.numel()
        except Exception as e: LOG.error(f"Error processing index {i} for stats ({sar_path}): {e}")
        continue
    if total_pixels_per_channel == 0:
      LOG.error("ERROR: No pixels processed for stats.")
      return None, None, torch.tensor([1.0], device=DEVICE)
    mean = (pixel_sum / total_pixels_per_channel).float()
    variance = (pixel_sum_sq / total_pixels_per_channel) - (mean.double() ** 2)
    std = torch.sqrt(torch.clamp(variance, min=1e-8)).float()
    negative_pixels = total_mask_pixels - positive_pixels
    if positive_pixels == 0 or total_mask_pixels == 0:
      LOG.warning("No positive/mask pixels. pos_weight=1.0")
      pos_weight = torch.tensor([1.0], device=DEVICE)
    else:
      pos_weight_val = negative_pixels / positive_pixels
      pos_weight = torch.tensor([pos_weight_val], device=DEVICE)
      LOG.info(f"Positive Pixels: {positive_pixels}, Negative: {negative_pixels}")
    LOG.info(f"Calculated Mean (VV, VH, DEM): {mean.tolist()}")
    LOG.info(f"Calculated Std (VV, VH, DEM): {std.tolist()}")
    LOG.info(f"Calculated pos_weight: {pos_weight.item():.4f} (device: {pos_weight.device})")
    if torch.any(std <= 1e-6):
      LOG.warning("WARNING: Calculated std near zero.")
    return mean, std, pos_weight


# --- Helper Function: Calculate Scaling Percentiles ---
def calculate_scaling_percentiles(sar_paths, dem_paths, q_min=1.0, q_max=99.0):
    """Calculates the q_min and q_max percentiles for SAR(VV,VH) and DEM channels."""
    LOG.info(f"Calculating scaling percentiles ({q_min}th, {q_max}th) across training data...")
    if not sar_paths or not dem_paths:
        LOG.error("ERROR: Empty SAR or DEM paths for percentile calculation. Returning None.")
        return None

    # Use lists to accumulate pixel values
    vv_pixels = []
    vh_pixels = []
    dem_pixels = []
    num_files = len(sar_paths)

    for i in tqdm(range(num_files), desc="Processing Stats (Percentiles)        "):
        sar_path, dem_path = sar_paths[i], dem_paths[i]
        try:
            with rasterio.open(sar_path) as src_sar:
                if src_sar.count != 2: continue
                sar_data = src_sar.read().astype(np.float32) # Read VV [0] and VH [1]
                # Ignore NaNs during percentile calculation
                vv_pixels.append(sar_data[0][~np.isnan(sar_data[0])].flatten())
                vh_pixels.append(sar_data[1][~np.isnan(sar_data[1])].flatten())

            with rasterio.open(dem_path) as src_dem:
                 if src_dem.count != 1: continue
                 # Ensure DEM read matches shape for consistency
                 dem_data = src_dem.read(1).astype(np.float32)
                 dem_pixels.append(dem_data[~np.isnan(dem_data)].flatten())

        except Exception as e:
          LOG.error(f"Error processing index {i} for percentiles ({sar_path}): {e}"); continue

    if not vv_pixels or not vh_pixels or not dem_pixels:
        LOG.error("ERROR: No pixel data collected for percentile calculation.")
        return None

    # Concatenate all pixels and calculate percentiles
    try:
        all_vv = np.concatenate(vv_pixels)
        all_vh = np.concatenate(vh_pixels)
        all_dem = np.concatenate(dem_pixels)

        p_vv = np.percentile(all_vv, [q_min, q_max])
        p_vh = np.percentile(all_vh, [q_min, q_max])
        p_dem = np.percentile(all_dem, [q_min, q_max])

        percentiles = {
            'vv': {'p_min': p_vv[0], 'p_max': p_vv[1]},
            'vh': {'p_min': p_vh[0], 'p_max': p_vh[1]},
            'dem': {'p_min': p_dem[0], 'p_max': p_dem[1]},
        }
        LOG.info(f"Calculated Percentiles: VV={percentiles['vv']}, VH={percentiles['vh']}, DEM={percentiles['dem']}")
        # Cleanup large arrays
        del all_vv, all_vh, all_dem, vv_pixels, vh_pixels, dem_pixels; gc.collect()
        return percentiles

    except Exception as e:
        LOG.error(f"Error calculating final percentiles: {e}"); gc.collect()
        return None

# --- Execute Calculation ---
NORM_MEAN = [0.5] * INPUT_CHANNELS # Default
NORM_STD = [0.5] * INPUT_CHANNELS  # Default
pos_weight = torch.tensor([1.0], device=DEVICE) # Default
NORM_PERCENTILES = None # Default

if train_sar_paths:
    # Calculate Mean/Std/Weight
    train_mean, train_std, pos_weight_calc = calculate_stats_and_pos_weight_mmflood(
        train_sar_paths, train_dem_paths, train_mask_paths
    )
    if train_mean is not None and train_std is not None and pos_weight_calc is not None:
         NORM_MEAN = train_mean.tolist()
         NORM_STD = train_std.tolist()
         pos_weight = pos_weight_calc
    else:
         LOG.error("Mean/Std/Weight calculation failed. Using defaults.")

    # Calculate Percentiles
    NORM_PERCENTILES = calculate_scaling_percentiles(
        train_sar_paths, train_dem_paths
    )
    if NORM_PERCENTILES is None:
         LOG.error("Percentile calculation failed. robust_scale will use runtime calculation.")
         # Keep NORM_PERCENTILES as None, robust_scale needs to handle this

else:
    LOG.info("Skipping stats and percentile calculation as no training paths were loaded.")

LOG.info(f"Final NORM_MEAN: {NORM_MEAN}")
LOG.info(f"Final NORM_STD: {NORM_STD}")
LOG.info(f"Final pos_weight: {pos_weight.item():.4f} (Device: {pos_weight.device})")
LOG.info(f"Final NORM_PERCENTILES: {NORM_PERCENTILES}")

LOG.info("--- Statistics & Percentiles Calculations Complete ---")

# Cell 3: Augmentation Definitions

# --- Helper Function: Get Transforms ---
def get_transforms(is_train, norm_mean, norm_std):
    """Gets augmentation pipeline components (No resizing, MultiplicativeNoise removed)."""
    if not isinstance(norm_mean, list) or len(norm_mean) != INPUT_CHANNELS or \
       not isinstance(norm_std, list) or len(norm_std) != INPUT_CHANNELS:
        LOG.warning(f"WARNING: NORM_MEAN/STD invalid. Using defaults [0.5].")
        norm_mean = [0.5] * INPUT_CHANNELS; norm_std = [0.5] * INPUT_CHANNELS

    spatial_transforms = [] # No Resize here
    pixel_transforms = []

    if is_train:
        spatial_transforms.extend([
            A.HorizontalFlip(p=0.5), A.VerticalFlip(p=0.5), A.RandomRotate90(p=0.5),
            A.ElasticTransform(alpha=1, sigma=50, interpolation=1, p=0.2, approximate=False),
            A.GridDistortion(num_steps=5, distort_limit=(-0.3, 0.3), interpolation=1, p=0.2),
            A.Rotate(limit=30, p=0.5, border_mode=0, interpolation=1, mask_interpolation=0),
        ])
        pixel_transforms.extend([
            A.OneOf([
                A.GaussianBlur(blur_limit=(3, 7), p=0.5),
                A.RandomBrightnessContrast(p=0.2),
                A.CoarseDropout(num_holes_range=(1, 8), hole_height_range=(0.1, 0.25), hole_width_range=(0.1, 0.25), p=1.0),
                # A.GridDropout(ratio=0.5, unit_size_range=(int(0.1 * IMAGE_HEIGHT), int(0.2 * IMAGE_WIDTH)), p=1.0),
            ], p=0.5),
        ])
    # Post-processing Transforms
    post_transforms = [
        A.Normalize(mean=norm_mean, std=norm_std, max_pixel_value=1.0),
        ToTensorV2()
    ]

    # Spatial+Pixel transforms applied before stacking
    return A.Compose(spatial_transforms + pixel_transforms,
                     additional_targets={'dem': 'image', 'mask': 'mask'}), \
           A.Compose(post_transforms)

# --- Execute Transform Definition ---
try:
    if 'NORM_MEAN' not in locals() or 'NORM_STD' not in locals(): raise NameError("Run Cell 2 first.")
    train_spatial_pixel_transform, train_post_transform = get_transforms(True, NORM_MEAN, NORM_STD)
    val_test_spatial_pixel_transform, val_test_post_transform = get_transforms(False, NORM_MEAN, NORM_STD)
    cl_buffer_spatial_transform = A.Compose([], additional_targets={'dem': 'image', 'mask': 'mask'})
    LOG.info("Transformation pipelines defined.")
except NameError as e:
    LOG.error(f"ERROR defining transforms: {e}.")
    train_spatial_pixel_transform=None; train_post_transform=None
    val_test_spatial_pixel_transform=None
    val_test_post_transform=None
    cl_buffer_spatial_transform=None
except Exception as e:
    LOG.error(f"ERROR defining transforms: {e}")
    traceback.print_exc()
    train_spatial_pixel_transform=None
    train_post_transform=None
    val_test_spatial_pixel_transform=None
    val_test_post_transform=None
    cl_buffer_spatial_transform=None

LOG.info("--- Augmentations Setup Complete ---")

# Cell 4: Dataset Class Definition

class MMFloodDataset(Dataset):
    """Dataset for MMFlood, handles loading, resizing, scaling, augmentation."""
    def __init__(self, sar_paths, dem_paths, mask_paths,
                 spatial_pixel_transform=None, post_transform=None,
                 is_cl_buffer_data=False,
                 # Add norm_percentiles parameter
                 norm_percentiles=None):
        self.sar_paths = sar_paths
        self.dem_paths = dem_paths
        self.mask_paths = mask_paths
        self.spatial_pixel_transform = spatial_pixel_transform
        self.post_transform = post_transform
        self.is_cl_buffer_data = is_cl_buffer_data
        # Store percentiles
        self.norm_percentiles = norm_percentiles
        if norm_percentiles is None:
             LOG.warning("MMFloodDataset initialized without pre-calculated percentiles. Robust scaling will compute at runtime.")

        if not sar_paths: LOG.warning("Warning: Initializing MMFloodDataset with empty SAR paths.")
        assert len(sar_paths) == len(dem_paths) == len(mask_paths), "Mismatched paths!"

    def __len__(self): return len(self.sar_paths)

    def __getitem__(self, idx):
        sar_path, dem_path, mask_path = self.sar_paths[idx], self.dem_paths[idx], self.mask_paths[idx]
        try:
            # Load Data & Resize if necessary
            with rasterio.open(sar_path) as src:
              sar_data = src.read(out_shape=(src.count, IMAGE_HEIGHT, IMAGE_WIDTH), resampling=rasterio.enums.Resampling.bilinear).astype(np.float32) if src.height != IMAGE_HEIGHT or src.width != IMAGE_WIDTH else src.read().astype(np.float32)
              sar_data = np.transpose(sar_data, (1, 2, 0))
              assert src.count==2
            with rasterio.open(dem_path) as src:
              dem_data = src.read(1, out_shape=(IMAGE_HEIGHT, IMAGE_WIDTH), resampling=rasterio.enums.Resampling.bilinear).astype(np.float32) if src.height != IMAGE_HEIGHT or src.width != IMAGE_WIDTH else src.read(1).astype(np.float32)
              dem_data = np.expand_dims(dem_data, axis=-1)
              assert src.count==1
            with rasterio.open(mask_path) as src:
              mask_data = src.read(1, out_shape=(IMAGE_HEIGHT, IMAGE_WIDTH), resampling=rasterio.enums.Resampling.nearest).astype(np.uint8) if src.height != IMAGE_HEIGHT or src.width != IMAGE_WIDTH else src.read(1).astype(np.uint8)
              mask_data = np.expand_dims(mask_data, axis=-1)
              assert src.count==1

            # Pre-process Mask
            mask_data = (mask_data > 0).astype(np.float32) # Binary {0.0, 1.0}

            # Apply Spatial/Pixel Transforms
            if self.spatial_pixel_transform:
                transformed = self.spatial_pixel_transform(image=sar_data, dem=dem_data, mask=mask_data)
                sar_sp, dem_sp, mask_sp = transformed['image'], transformed['dem'], transformed['mask']
            else: sar_sp, dem_sp, mask_sp = sar_data, dem_data, mask_data

            # Scaling to [0, 1] Using pre-calculated percentiles
            vv_p_min, vv_p_max = None, None
            vh_p_min, vh_p_max = None, None
            dem_p_min, dem_p_max = None, None
            if self.norm_percentiles:
                 vv_p_min = self.norm_percentiles.get('vv', {}).get('p_min')
                 vv_p_max = self.norm_percentiles.get('vv', {}).get('p_max')
                 vh_p_min = self.norm_percentiles.get('vh', {}).get('p_min')
                 vh_p_max = self.norm_percentiles.get('vh', {}).get('p_max')
                 dem_p_min = self.norm_percentiles.get('dem', {}).get('p_min')
                 dem_p_max = self.norm_percentiles.get('dem', {}).get('p_max')

            # Apply robust_scale
            sar_sp[..., 0] = robust_scale(sar_sp[..., 0], p_min=vv_p_min, p_max=vv_p_max) # VV (index 0)
            sar_sp[..., 1] = robust_scale(sar_sp[..., 1], p_min=vh_p_min, p_max=vh_p_max) # VH (index 1)
            dem_sp[..., 0] = robust_scale(dem_sp[..., 0], p_min=dem_p_min, p_max=dem_p_max) # DEM (index 0 of dem_sp)

            # Stack SAR and DEM ---
            image_combined = np.concatenate([sar_sp, dem_sp], axis=-1) # -> [H, W, 3]

            # Apply Post Transforms (Normalize, ToTensor) ---
            if self.post_transform:
                final_transformed = self.post_transform(image=image_combined)
                final_image = final_transformed['image'] # -> [C, H, W]
                final_mask = torch.from_numpy(mask_sp.transpose(2, 0, 1)).float() # Mask [H,W,1] -> [1,H,W]
            elif self.is_cl_buffer_data: # Minimal processing for CL buffer
                 final_image = torch.from_numpy(image_combined.transpose(2, 0, 1)).float()
                 final_mask = torch.from_numpy(mask_sp.transpose(2, 0, 1)).float()
            else:
                 LOG.warning(f"Warning: No post_transform/CL flag index {idx}. Raw scaled tensors.")
                 final_image = torch.from_numpy(image_combined.transpose(2, 0, 1)).float()
                 final_mask = torch.from_numpy(mask_sp.transpose(2, 0, 1)).float()

            # Final Shape Check
            exp_img = (INPUT_CHANNELS, IMAGE_HEIGHT, IMAGE_WIDTH); exp_mask = (OUTPUT_CLASSES, IMAGE_HEIGHT, IMAGE_WIDTH)
            if final_image.shape != exp_img or final_mask.shape != exp_mask:
                raise ValueError(f"Final shape mismatch. Img: {final_image.shape} (exp {exp_img}), Mask: {final_mask.shape} (exp {exp_mask})")

            return final_image, final_mask

        except Exception as e:
            LOG.error(f"!!! ERROR in Dataset __getitem__ index {idx} !!!")
            print(f"Paths: SAR={sar_path}, DEM={dem_path}, Mask={mask_path}")
            print(f"Error: {e}")
            traceback.print_exc()
            return None # Filtered by collate_fn

LOG.info("--- Dataset Definition Complete ---")

# Cell 5: Dataset Instantiation, CL Split, Initial Loaders


# --- Instantiate Datasets ---
train_dataset_full = None
val_dataset = None
test_dataset = None
train_dataset_for_cl_split = None
LOG.info("Instantiating Datasets...")

try:
    required = ['train_sar_paths', 'train_spatial_pixel_transform', 'train_post_transform', 'cl_buffer_spatial_transform', 'val_test_spatial_pixel_transform', 'val_test_post_transform', 'NORM_PERCENTILES']
    if not all(k in globals() for k in required):
         missing = [k for k in required if k not in globals()]
         raise NameError(f"Paths, transforms, or NORM_PERCENTILES missing: {missing}. Run previous cells.")

    # Pass NORM_PERCENTILES to Datasets
    if train_sar_paths:
        train_dataset_full = MMFloodDataset(train_sar_paths, train_dem_paths, train_mask_paths, train_spatial_pixel_transform, train_post_transform, norm_percentiles=NORM_PERCENTILES)
        train_dataset_for_cl_split = MMFloodDataset(train_sar_paths, train_dem_paths, train_mask_paths, None, None, is_cl_buffer_data=True, norm_percentiles=None)
    if val_sar_paths:
        val_dataset = MMFloodDataset(val_sar_paths, val_dem_paths, val_mask_paths, val_test_spatial_pixel_transform, val_test_post_transform, norm_percentiles=NORM_PERCENTILES)
    if test_sar_paths:
        test_dataset = MMFloodDataset(test_sar_paths, test_dem_paths, test_mask_paths, val_test_spatial_pixel_transform, val_test_post_transform, norm_percentiles=NORM_PERCENTILES)
    # +++ End Pass Percentiles +++

    LOG.info("Dataset instances created.")
    LOG.info(f"Train size: {len(train_dataset_full) if train_dataset_full else 'N/A'}")
    LOG.info(f"Val size: {len(val_dataset) if val_dataset else 'N/A'}")
    LOG.info(f"Test size: {len(test_dataset) if test_dataset else 'N/A'}")
    LOG.info(f"CL Split size: {len(train_dataset_for_cl_split) if train_dataset_for_cl_split else 'N/A'}")

except NameError as e: LOG.warning(f"ERROR creating Datasets: {e}.")
except Exception as e: LOG.warning(f"ERROR creating Datasets: {e}")

# --- Split Training Data (Random Split - Keep for baseline comparison) ---
task_loaders = []
task_datasets = []
if train_dataset_full and len(train_dataset_full) > 0 and globals().get('NUM_TASKS', 0) > 0:
    num_train_samples = len(train_dataset_full)
    LOG.info(f"Splitting {num_train_samples} train samples randomly for {NUM_TASKS} tasks (for baseline CL)...")
    if num_train_samples < NUM_TASKS:
      LOG.warning(f"NUM_TASKS > num_samples. Setting NUM_TASKS = {num_train_samples}")
      NUM_TASKS = num_train_samples
    if NUM_TASKS > 0:
        samples_per_task = num_train_samples // NUM_TASKS
        task_indices = [list(range(i*samples_per_task, (i+1)*samples_per_task)) for i in range(NUM_TASKS)]
        remainder = num_train_samples % NUM_TASKS
        if remainder > 0:
             if len(task_indices) < NUM_TASKS: task_indices.append([])
             task_indices[-1].extend(range(NUM_TASKS*samples_per_task, num_train_samples))
        task_indices = [inds for inds in task_indices if inds]; LOG.info(f"Resulting in {len(task_indices)} non-empty random tasks.")
        for i, indices in enumerate(task_indices):
          LOG.info(f"Random Task {i+1}: {len(indices)} samples")
        task_datasets = [Subset(train_dataset_full, indices) for indices in task_indices] # Subsets use the main dataset with pre-calc percentiles
else: LOG.info("Skipping initial random CL task splitting.")

# --- Create DataLoader Objects ---
num_workers = min(os.cpu_count() // 2, 4)
pin_memory = True if DEVICE.type == 'cuda' else False
LOG.info(f"Creating DataLoaders: workers={num_workers}, pin_memory={pin_memory}...")

train_loader_baseline = DataLoader(train_dataset_full, batch_size=BATCH_SIZE, shuffle=True, num_workers=num_workers, pin_memory=pin_memory, drop_last=True, collate_fn=collate_fn_skip_error) if train_dataset_full else None
val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers, pin_memory=pin_memory, collate_fn=collate_fn_skip_error) if val_dataset else None
test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers, pin_memory=pin_memory, collate_fn=collate_fn_skip_error) if test_dataset else None
if task_datasets:
  task_loaders = [DataLoader(tds, batch_size=BATCH_SIZE, shuffle=True, num_workers=num_workers, pin_memory=pin_memory, drop_last=True, collate_fn=collate_fn_skip_error) for tds in task_datasets]
  LOG.info(f"Created {len(task_loaders)} random task loaders.")
else:
  LOG.info("No random task datasets, skipping task loader creation.")

LOG.info(f"Loader lengths: Baseline Train={len(train_loader_baseline) if train_loader_baseline else 'N/A'}, Val={len(val_loader) if val_loader else 'N/A'}, Test={len(test_loader) if test_loader else 'N/A'}, Random Tasks={len(task_loaders)}")
LOG.info("--- Datasets and DataLoaders Complete ---")

# Cell 6: Model, Loss, Optimizer Definitions & Test

# --- Helper Function: Build Model ---
def build_model(encoder, weights, in_channels, out_classes):
    """Builds an SMP U-Net model and moves it to DEVICE."""
    LOG.info(f"Building U-Net with encoder: {encoder}, weights: {weights}, input_channels: {in_channels}")
    try:
        model = smp.Unet(encoder_name=encoder, encoder_weights=weights, in_channels=in_channels, classes=out_classes, activation=None)
        # --- Move model to device ---
        model.to(DEVICE)
        LOG.info(f"Model moved to {DEVICE}")
        return model
    except Exception as e: LOG.error(f"ERROR building model: {e}"); traceback.print_exc(); return None

# --- Helper Function: Get Loss ---
def get_loss_function(loss_type, pos_weight_val=None, focal_gamma=FOCAL_GAMMA, tversky_alpha=TVERSKY_ALPHA, tversky_beta=TVERSKY_BETA):
    """Gets the specified loss function and moves it to DEVICE."""
    LOG.info(f"Creating Loss Function: {loss_type}")
    criterion = None
    try:
        if loss_type == 'bce':
            weight_tensor = pos_weight_val if isinstance(pos_weight_val, torch.Tensor) and pos_weight_val.numel() == 1 and pos_weight_val.item() > 0 else None
            if weight_tensor is not None: LOG.info(f"Using BCEWithLogitsLoss with pos_weight = {weight_tensor.item():.4f} (Device: {weight_tensor.device})")
            else: LOG.info(f"Using standard BCEWithLogitsLoss.")
            criterion = nn.BCEWithLogitsLoss(pos_weight=weight_tensor) # Pass tensor directly
        elif loss_type == 'focal':
            criterion = smp.losses.FocalLoss(mode='binary', gamma=focal_gamma, reduction='mean')
            LOG.info(f"Using FocalLoss with gamma = {focal_gamma}")
        elif loss_type == 'tversky':
             criterion = smp.losses.TverskyLoss(mode='binary', alpha=tversky_alpha, beta=tversky_beta, gamma=1.0, from_logits=True, smooth=1e-6)
             LOG.info(f"Using TverskyLoss with alpha = {tversky_alpha}, beta = {tversky_beta}")
        else:
            LOG.warning(f"WARNING: Unknown loss type '{loss_type}'. Defaulting to BCEWithLogitsLoss.")
            criterion = nn.BCEWithLogitsLoss()
        # --- Move criterion to device ---
        if criterion:
             criterion.to(DEVICE)
             LOG.info(f"Loss function moved to {DEVICE}")
    except Exception as e: LOG.error(f"ERROR creating loss function '{loss_type}': {e}"); traceback.print_exc(); criterion = None
    return criterion

# def create_poly_optimizer(model, base_lr=LEARNING_RATE, weight_decay=1e-4): ...
if 'create_poly_optimizer' not in globals(): raise NameError("create_poly_optimizer not defined. Run Cell 2.")
LOG.info("create_poly_optimizer previously defined.")

# --- Build Template Model and Criterion for Testing ---
LOG.info("Building template model and criterion for testing...")
model_template = build_model(ENCODER_NAME, ENCODER_WEIGHTS, INPUT_CHANNELS, OUTPUT_CLASSES)
criterion_template = get_loss_function(LOSS_TYPE, pos_weight_val=pos_weight)

torch.cuda.empty_cache()

# --- Test Instantiation ---
if model_template is not None and criterion_template is not None:
    LOG.info("Performing test for model and loss...")
    try:
        # Create dummy tensors
        dummy_input = torch.randn(BATCH_SIZE, INPUT_CHANNELS, IMAGE_HEIGHT, IMAGE_WIDTH, device=DEVICE)
        dummy_target = torch.randint(0, 2, (BATCH_SIZE, OUTPUT_CLASSES, IMAGE_HEIGHT, IMAGE_WIDTH), device=DEVICE).float()

        # Model, Input -> Output
        output = model_template(dummy_input)
        LOG.info(f"Model test:")
        LOG.info(f"Input shape: {dummy_input.shape}, Device: {dummy_input.device}")
        LOG.info(f"Output shape: {output.shape}, Device: {output.device}")

        LOG.info(f"Criterion test:")
        LOG.info(f"Target shape: {dummy_target.shape}, Device: {dummy_target.device}")
        if isinstance(criterion_template, nn.BCEWithLogitsLoss) and criterion_template.pos_weight is not None:
             print(f"Criterion pos_weight Device: {criterion_template.pos_weight.device}")

        # Output, Target, Criterion
        loss_val = criterion_template(output, dummy_target)
        LOG.info(f"Loss function test successful. Example loss: {loss_val.item()}")

    except Exception as e: LOG.error(f"ERROR during model/loss DEVICE test: {e}"); traceback.print_exc()
    finally: # Clean up template objects
        LOG.info("Cleaning up template objects..."); del model_template, criterion_template
        if 'dummy_input' in locals(): del dummy_input
        if 'output' in locals(): del output
        if 'dummy_target' in locals(): del dummy_target
        if 'loss_val' in locals(): del loss_val
        gc.collect(); torch.cuda.empty_cache() if DEVICE.type == 'cuda' else None
else: LOG.error("ERROR: Template Model or Criterion could not be built. Cannot proceed.")

LOG.info("--- Model/Loss/Optimizer Definitions Complete ---")

# Cell 7: Replay Buffer Class

# --- ReplayBuffer Class Definition ---
class ReplayBuffer:
    """ Manages replay memory (storing paths), provides samples, supports uncertainty scoring. """
    def __init__(self, max_size):
        self.max_size = max_size
        # Store path tuples: (sar_path, dem_path, mask_path)
        self.buffer = deque(maxlen=max_size)
        LOG.info(f"ReplayBuffer initialized with max_size={max_size} (Stores Paths)")

    def add(self, path_tuple):
        """ Adds data point (path tuple) to buffer. Assumes (sar_path, dem_path, mask_path)."""
        try:
            # Check if it's a tuple/list of 3 strings
            if not isinstance(path_tuple, (list, tuple)) or len(path_tuple) != 3 or not all(isinstance(p, str) for p in path_tuple):
                LOG.warning(f"Invalid data_point format in ReplayBuffer.add: {type(path_tuple)}. Expected (sar_path, dem_path, mask_path).")
                return
            # Basic check if paths seem plausible (optional)
            if not all(p.endswith('.tif') for p in path_tuple):
                 LOG.warning(f"Path tuple does not seem to contain .tif files: {path_tuple}")
            # Add the path tuple directly
            self.buffer.append(path_tuple)
        except Exception as e:
            LOG.error(f"General error adding path tuple {path_tuple}: {e}\n{traceback.format_exc()}")

    def __len__(self): return len(self.buffer)

    def sample(self, batch_size, strategy='random', model=None, # Model needed for uncertainty
               # --- Remove device parameter, sampling returns paths ---
               subset_fraction=1.0):
        """ Samples a batch of path tuples. Uncertainty scoring loads data internally. """
        if len(self.buffer) == 0: LOG.debug("Sample on empty buffer."); return None
        buffer_list = list(self.buffer); buffer_size = len(buffer_list); sample_size = min(batch_size, buffer_size)
        sampled_original_indices = []

        try:
            if strategy == 'random':
                sampled_original_indices = random.sample(range(buffer_size), sample_size)
                LOG.debug(f"Random sampling: selected {len(sampled_original_indices)} indices.")
            elif strategy in ['margin', 'least_confidence', 'entropy']:
                if model is None: raise ValueError("Uncertainty sampling requires model.")
                LOG.debug(f"Calculating uncertainty scores (strat:{strategy}, frac:{subset_fraction:.2f})...")
                scores, subset_original_indices_map = self._calculate_uncertainty_scores(
                    model, strategy, subset_fraction=subset_fraction
                )
                LOG.debug("Score calculation finished.")

                if scores is None or subset_original_indices_map is None:
                     LOG.warning(f"Score calc failed for '{strategy}'. Fallback random.")
                     sampled_original_indices = random.sample(range(buffer_size), sample_size)
                else:
                     num_scored=len(scores)
                     select_k_from_subset=min(sample_size, num_scored)
                     if select_k_from_subset < sample_size:
                      LOG.warning(f"Requested {sample_size}, only {num_scored} scored. Selecting {select_k_from_subset}.")
                     if num_scored > 0:
                         sorted_subset_indices=np.argsort(scores)
                         top_subset_indices=sorted_subset_indices[-select_k_from_subset:]
                         sampled_original_indices=[subset_original_indices_map[i] for i in top_subset_indices]
                         LOG.debug(f"Uncertainty: selected top {len(sampled_original_indices)}.")
                     else: LOG.warning("Uncertainty: No scores generated. Fallback random.")
                     sampled_original_indices = random.sample(range(buffer_size), sample_size)
            else: raise ValueError(f"Unknown sampling strategy: {strategy}")

            # Retrieve path tuples
            if sampled_original_indices:
                try:
                    # Return the list of path tuples
                    sampled_path_tuples = [buffer_list[i] for i in sampled_original_indices]
                    LOG.debug(f"Sampled {len(sampled_path_tuples)} path tuples.")
                    return sampled_path_tuples
                except IndexError as ie:
                    LOG.error(f"IndexError retrieving items. Indices:{sampled_original_indices}, Buf size:{buffer_size}. Err:{ie}")
                    return None
            else:
                LOG.warning(f"No indices selected (strategy: {strategy}).")
                return None

        except Exception as e:
            LOG.error(f"General error during ReplayBuffer sampling (strat:{strategy}): {e}\n{traceback.format_exc()}")
            return None

    # Uncertainty Scoring
    def _calculate_uncertainty_scores(self, model, strategy, subset_fraction=1.0):
        """ Calculates uncertainty scores from paths, loads data internally. """
        buffer_list_full = list(self.buffer); buffer_size = len(buffer_list_full)
        if buffer_size == 0: LOG.debug("Attempted to calc scores on empty buffer."); return None, None

        subset_original_indices_map = list(range(buffer_size))
        buffer_subset_paths = buffer_list_full # List of path tuples

        # Subset Selection Logic
        if 0.0 < subset_fraction < 1.0:
             min_subset_size = max(1, BATCH_SIZE // 2)
             target_subset_size = max(min_subset_size, int(buffer_size * subset_fraction))
             actual_subset_size = min(buffer_size, target_subset_size)
             if actual_subset_size < buffer_size:
                 LOG.debug(f"Scoring random subset: size {actual_subset_size} (frac {subset_fraction:.2f}) from buffer {buffer_size}.")
                 subset_original_indices_map = sorted(random.sample(range(buffer_size), actual_subset_size))
                 buffer_subset_paths = [buffer_list_full[i] for i in subset_original_indices_map] # Get subset of paths
             else: LOG.debug(f"Subset size >= buffer size. Scoring full buffer ({buffer_size}).")
        elif subset_fraction >= 1.0:
          LOG.debug(f"subset_fraction >= 1.0. Scoring full buffer ({buffer_size}).")
        else:
          LOG.warning(f"subset_fraction <= 0 ({subset_fraction}). Cannot score.")
          return None, None

        subset_size = len(buffer_subset_paths)
        if subset_size == 0:
          LOG.warning("Buffer subset paths empty post-selection.")
          return None, None

        # --- Data Loading for Scoring ---
        scoring_device = torch.device('cpu') # Perform scoring inference on CPU to avoid interfering with training GPU memory
        LOG.info(f"Creating temporary dataset/loader for scoring {subset_size} items on {scoring_device}...")

        try:
            # Use minimal transforms needed for model input (scaling, normalization, ToTensor)
            # Assume val/test transforms are suitable here. Use pre-calculated percentiles.
            scoring_dataset = MMFloodDataset(
                sar_paths=[p[0] for p in buffer_subset_paths],
                dem_paths=[p[1] for p in buffer_subset_paths],
                mask_paths=[p[2] for p in buffer_subset_paths], # Need masks for dataset init, though not used in scoring
                spatial_pixel_transform=None, # No spatial augs for scoring
                post_transform=val_test_post_transform, # Use validation post-transform (Normalize + ToTensor)
                norm_percentiles=globals().get('NORM_PERCENTILES', None) # Pass global percentiles
            )
            score_batch_size = min(BATCH_SIZE * 2, subset_size)
            score_num_workers = 0 # Using workers within scoring loop can be complex, safer with 0
            temp_loader = DataLoader(scoring_dataset, batch_size=score_batch_size, shuffle=False,
                                     collate_fn=collate_fn_skip_error, num_workers=score_num_workers)
            LOG.debug(f"Temporary DataLoader created for scoring.")
        except Exception as e_load:
             LOG.error(f"Failed to create temporary Dataset/DataLoader for uncertainty scoring: {e_load}")
             return None, None
        # --- End Data Loading Setup ---

        original_mode = model.training; model.eval(); model.to(scoring_device)
        all_scores = []
        is_cuda_eval = scoring_device.type == 'cuda' # Check if scoring device is CUDA

        processed_in_scoring = 0
        with torch.no_grad():
            for batch_idx, data in enumerate(tqdm(temp_loader, desc="Scoring Replay Buffer", leave=False)):
                 if data is None or len(data) < 2 or data[0].nelement() == 0:
                     LOG.warning(f"Skipping empty/invalid batch {batch_idx} in scoring DataLoader.")
                     continue
                 # --- Need only images for scoring ---
                 images, _ = data # Discard masks
                 images = images.to(scoring_device)
                 batch_actual_size = images.size(0)

                 try:
                     # Use autocast if scoring device happens to be CUDA
                     with torch.amp.autocast(device_type=scoring_device.type, enabled=is_cuda_eval):
                      outputs = model(images)
                     probs = torch.sigmoid(outputs.float())

                     # Calculate uncertainty score per image in the batch
                     if strategy == 'least_confidence':
                      image_scores = (0.5 - torch.abs(probs - 0.5)).mean(dim=[1,2,3])
                     elif strategy == 'margin':
                      image_scores = (1.0 - torch.abs(2 * probs - 1.0)).mean(dim=[1,2,3])
                     elif strategy == 'entropy':
                      epsilon = 1e-9; p = probs.clamp(epsilon, 1.0 - epsilon)
                      entropy = - (p*torch.log2(p) + (1-p)*torch.log2(1-p))
                      image_scores = entropy.mean(dim=[1,2,3])
                     else:
                      image_scores = torch.zeros(batch_actual_size, device=scoring_device) # Fallback

                     batch_scores = image_scores.cpu().tolist()
                     all_scores.extend(batch_scores)
                     processed_in_scoring += batch_actual_size
                 except Exception as e:
                     LOG.error(f"Error calculating scores batch {batch_idx}: {e}\n{traceback.format_exc()}")
                     # Append NaN scores for the failed batch
                     all_scores.extend([np.nan] * batch_actual_size)
                     processed_in_scoring += batch_actual_size # Still count as processed

        # Restore model state
        if original_mode: model.train()
        # Move model back to original training device
        model.to(DEVICE)

        LOG.debug(f"Scoring inference done. Processed:{processed_in_scoring}. Scores collected:{len(all_scores)}.")
        final_scores = np.nan_to_num(all_scores, nan=-np.inf); # Replace NaN with low score

        # Check if number of scores matches the number of paths we intended to score
        if len(final_scores) != subset_size:
             LOG.warning(f"Score count mismatch ({len(final_scores)}) vs subset size ({subset_size}). Collate likely skipped some items.")
             return None, None # Fail for now if counts mismatch

        return final_scores, subset_original_indices_map

# --- End Class Definition ---
LOG.info("--- Replay Buffer Complete ---")

# Cell 8: Evaluation Metrics, Function, and EarlyStopping Class


# --- Metric Calculation Functions ---
def get_confusion_matrix_elements(pred_logits, target):
    """Calculates TP, TN, FP, FN for a batch"""
    try:
        pred_prob = torch.sigmoid(pred_logits)
        pred_mask = (pred_prob > 0.5).float() # Thresholding
        pred_flat = pred_mask.view(-1)
        target_flat = target.view(-1).float() # Ensure target is float

        tp = (pred_flat * target_flat).sum()
        fp = pred_flat.sum() - tp
        fn = target_flat.sum() - tp
        tn = target_flat.numel() - tp - fp - fn

        # Ensure counts are non-negative
        tp = torch.clamp(tp, min=0)
        fp = torch.clamp(fp, min=0)
        fn = torch.clamp(fn, min=0)
        tn = torch.clamp(tn, min=0)

        return tp.item(), tn.item(), fp.item(), fn.item()
    except Exception as e:
        LOG.error(f"Error in get_confusion_matrix_elements: {e}\n{traceback.format_exc()}")
        # Return zeros or NaNs? Zeros might be safer for accumulation
        batch_size = pred_logits.shape[0] if pred_logits is not None else 1
        num_pixels_per_item = np.prod(pred_logits.shape[1:]) if pred_logits is not None and pred_logits.ndim > 1 else 1
        total_elements = batch_size * num_pixels_per_item
        # Returning 0 for TP, FP, FN and total for TN might skew results less than all zeros
        return 0.0, float(total_elements), 0.0, 0.0 # TP, TN, FP, FN


def precision_score_from_counts(tp, fp, smooth=1e-6):
    return (tp + smooth) / (tp + fp + smooth)

def recall_score_from_counts(tp, fn, smooth=1e-6):
    return (tp + smooth) / (tp + fn + smooth)

def dice_coefficient_from_counts(tp, fp, fn, smooth=1e-6):
    return (2. * tp + smooth) / (2. * tp + fp + fn + smooth)

def iou_score_from_counts(tp, fp, fn, smooth=1e-6):
    return (tp + smooth) / (tp + fp + fn + smooth)

def f1_score_from_counts(tp, fp, fn, smooth=1e-6):
    # F1 is equivalent to Dice for binary case
    return dice_coefficient_from_counts(tp, fp, fn, smooth)

def matthews_corrcoef_from_counts(tp, tn, fp, fn):
    """Calculates MCC from confusion matrix counts."""
    numerator = (tp * tn) - (fp * fn)
    denominator_sq = (tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)
    if denominator_sq <= 0:
        # If any term in the denominator product is zero, MCC is 0
        # unless the numerator is also non-zero (perfect inverse prediction, result is -1)
        # Handle edge case: If TN+FN=0 (no negatives) or TP+FP=0 (no positives), denominator is 0
        # If perfect prediction (FP=FN=0), result is 1. If perfectly inverse (TP=TN=0), result is -1.
        # If one class is empty and prediction is random, result is 0.
        # Simplified: return 0 if denominator is zero, assuming not perfect inverse case.
         return 0.0
    mcc = numerator / math.sqrt(denominator_sq)
    return mcc


LOG.info("Metric functions defined")

# --- Evaluation Function ---
def evaluate_model(model, dataloader, criterion, device):
    """
    Evaluates model, returns:
    avg_loss, avg_dice, avg_iou, avg_f1, avg_precision, avg_recall, avg_mcc,
    total_tp, total_tn, total_fp, total_fn
    """
    original_mode = model.training; model.eval()
    # Accumulators for averages
    total_loss, total_dice, total_iou, total_f1 = 0.0, 0.0, 0.0, 0.0
    total_precision, total_recall, total_mcc = 0.0, 0.0, 0.0
    # Accumulators for total confusion matrix counts
    accumulated_tp, accumulated_tn, accumulated_fp, accumulated_fn = 0.0, 0.0, 0.0, 0.0
    num_batches = 0
    is_cuda = device.type == 'cuda'

    model.to(device); # Ensure model/criterion on device
    if isinstance(criterion, nn.Module): criterion.to(device)
    if isinstance(criterion, nn.BCEWithLogitsLoss) and criterion.pos_weight is not None:
        if criterion.pos_weight.device != device: criterion.pos_weight = criterion.pos_weight.to(device)

    with torch.no_grad():
        progress_bar = tqdm(dataloader, desc="Evaluating", leave=False, disable=False)
        for images, masks in progress_bar:
            if images.nelement() == 0 or masks.nelement() == 0: continue
            images, masks = images.to(device), masks.to(device)
            try:
                # Use new autocast call
                with torch.amp.autocast(device_type=device.type, enabled=is_cuda):
                    outputs = model(images)
                    # Calculate loss *inside* autocast if criterion supports it,
                    # otherwise calculate outside if needed for stability.
                    loss = criterion(outputs, masks.float())

                # Calculate confusion matrix elements for the batch
                tp, tn, fp, fn = get_confusion_matrix_elements(outputs.float(), masks)

                # Accumulate total counts
                accumulated_tp += tp
                accumulated_tn += tn
                accumulated_fp += fp
                accumulated_fn += fn

                # Calculate per-batch averages here and average them
                batch_precision = precision_score_from_counts(tp, fp)
                batch_recall = recall_score_from_counts(tp, fn)
                batch_dice = dice_coefficient_from_counts(tp, fp, fn)
                batch_iou = iou_score_from_counts(tp, fp, fn)
                batch_f1 = batch_dice # F1 is Dice
                batch_mcc = matthews_corrcoef_from_counts(tp, tn, fp, fn)

                # Accumulate metrics for averaging
                total_loss += loss.item()
                total_precision += batch_precision
                total_recall += batch_recall
                total_dice += batch_dice
                total_iou += batch_iou
                total_f1 += batch_f1
                total_mcc += batch_mcc

                num_batches += 1
                if num_batches % 10 == 0: progress_bar.set_postfix(loss=total_loss/num_batches, f1=total_f1/num_batches)
            except Exception as e:
                LOG.error(f"Error during evaluation batch: {e}\n{traceback.format_exc()}")
                continue # Skip faulty batch

    # Restore original model mode
    if original_mode: model.train()

    if num_batches == 0:
        LOG.warning("Evaluation completed, but no batches were successfully processed.")
        # Return NaNs and Zeros for counts
        return np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, 0, 0, 0, 0 # loss, metrics..., tp, tn, fp, fn
    else:
        # Calculate average metrics
        avg_loss = total_loss / num_batches
        avg_dice = total_dice / num_batches
        avg_iou = total_iou / num_batches
        avg_f1 = total_f1 / num_batches
        avg_precision = total_precision / num_batches
        avg_recall = total_recall / num_batches
        avg_mcc = total_mcc / num_batches

        # Return averages AND total confusion matrix counts
        return avg_loss, avg_dice, avg_iou, avg_f1, avg_precision, avg_recall, avg_mcc, \
               accumulated_tp, accumulated_tn, accumulated_fp, accumulated_fn

# --- EarlyStopping Class Definition ---
class EarlyStopping:
    """Stops training when a monitored metric has stopped improving."""
    def __init__(self, patience=7, verbose=False, delta=0, mode='min', trace_func=LOG.info):
        self.patience = patience
        self.verbose = verbose
        self.counter = 0
        self.best_score = None
        self.early_stop = False
        self.delta = delta
        self.mode = mode
        self.trace_func = trace_func
        if self.mode not in ['min', 'max']:
          raise ValueError("Mode must be 'min' or 'max'")
        self.val_metric_best = np.inf if mode == 'min' else -np.inf

    def __call__(self, val_metric):
        score = val_metric; improvement = False
        if self.best_score is None:
          self.best_score = score
          improvement = True
        elif self.mode == 'max':
          improvement = score > self.best_score + self.delta
        elif self.mode == 'min':
          improvement = score < self.best_score - self.delta
        if improvement:
            if self.verbose:
              self.trace_func(f'Validation metric improved ({self.val_metric_best:.6f} -> {score:.6f}).')
            self.best_score = score
            self.val_metric_best = score
            self.counter = 0
        else:
            self.counter += 1
            if self.verbose: self.trace_func(f'EarlyStopping counter: {self.counter}/{self.patience}')
            if self.counter >= self.patience:
              self.early_stop = True


LOG.info("--- Evaluation & EarlyStopping Setup Complete ---")

# Cell 9: Baseline Training Function Definition

def train_baseline(model, train_loader, val_loader, criterion, optimizer, scheduler,
                   epochs, device, model_save_path='unet_baseline.pth',
                   patience=PATIENCE):
    """Trains baseline model with optional Early Stopping and AMP."""
    use_es = globals().get('USE_EARLY_STOPPING', False)
    is_cuda = device.type == 'cuda' # Check if using CUDA
    LOG.info(f"--- Starting Baseline Training: {is_cuda}) ---")
    LOG.info(f"Target Epochs: {epochs}, Device: {device}, Early Stop: {use_es}")

    history = {'train_loss': [], 'val_loss': [], 'val_dice': [], 'val_iou': [], 'val_f1': [], 'val_precision': [], 'val_recall': []}
    best_val_f1 = -1.0; best_model_state = None; start_time = time.time()

    # +++ Initialize GradScaler +++
    scaler = torch.amp.GradScaler(enabled=is_cuda)

    early_stopper = EarlyStopping(patience=patience, verbose=True, delta=0.001, mode='max', trace_func=LOG.info) if use_es else None
    if early_stopper:
      LOG.info("EarlyStopping initialized.")

    try: # Device placement check
        model.to(device)
        if isinstance(criterion, nn.Module): criterion.to(device)
        if isinstance(criterion, nn.BCEWithLogitsLoss) and criterion.pos_weight is not None:
             if criterion.pos_weight.device != device:
              criterion.pos_weight = criterion.pos_weight.to(device)
        LOG.info(f"Model/Criterion placement on {device} checked.")
    except Exception as e:
        LOG.error(f"Error moving model/criterion to device {device}: {e}")
        return model, history, 0

    LOG.info(f"Starting training loop...")
    actual_epochs_run = 0
    for epoch in range(epochs):
        actual_epochs_run += 1
        model.train(); running_loss = 0.0
        processed_batches = 0
        progress_bar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{epochs}", leave=False)

        for images, masks in progress_bar:
            if images.nelement() == 0 or masks.nelement() == 0:
              continue
            images, masks = images.to(device), masks.to(device)

            optimizer.zero_grad() # Zero gradients first

            try:
                # Autocast context
                with torch.amp.autocast(device_type=device.type, enabled=is_cuda):
                  outputs = model(images)
                  loss = criterion(outputs, masks.float())

                # Scale loss, backward, step, update
                scaler.scale(loss).backward()
                scaler.step(optimizer)
                scaler.update()
                scheduler.step() # Step scheduler after optimizer

                running_loss+=loss.item()
                processed_batches+=1
                if processed_batches>0:
                  progress_bar.set_postfix(loss=running_loss/processed_batches, lr=f"{scheduler.get_last_lr()[0]:.2e}")

            except Exception as e:
                LOG.error(f"Exception during Training Epoch {epoch+1} Batch:\n{traceback.format_exc()}")
                optimizer.zero_grad() # Ensure grads are cleared even if batch failed
                continue # Try to continue epoch

        avg_epoch_loss = running_loss / processed_batches if processed_batches > 0 else 0.0

        # Evaluate
        try:
            # --- Call evaluate_model and unpack ALL 11 results ---
            eval_output_val = evaluate_model(model, val_loader, criterion, device)
            val_loss, val_dice, val_iou, val_f1, val_precision, val_recall, val_mcc, \
            val_tp, val_tn, val_fp, val_fn = eval_output_val # Unpack all values

            # --- Convert metrics safely to CPU floats before using/storing ---
            metrics_cpu = []
            for metric_val in eval_output_val[:7]: # Process the 7 metric values
                if isinstance(metric_val, torch.Tensor):
                     metrics_cpu.append(metric_val.item())
                elif metric_val is None:
                     metrics_cpu.append(np.nan)
                else:
                     try: metrics_cpu.append(float(metric_val))
                     except (ValueError, TypeError): metrics_cpu.append(np.nan)
            vl_f, vd_f, vi_f, vf1_f, vp_f, vr_f, vmcc_f = metrics_cpu # Unpack floats

            # --- Store History ---
            history['train_loss'].append(avg_epoch_loss)
            history['val_loss'].append(vl_f)
            history['val_dice'].append(vd_f)
            history['val_iou'].append(vi_f)
            history['val_f1'].append(vf1_f)
            history['val_precision'].append(vp_f)
            history['val_recall'].append(vr_f)
            history.setdefault('mcc', []).append(vmcc_f)

            # --- Log Validation Metrics ---
            LOG.info(f"Epoch {epoch+1} finished. Train Loss: {avg_epoch_loss:.4f} | Val Loss: {vl_f:.4f} | Val F1: {vf1_f:.4f} | Val MCC: {vmcc_f:.4f}")
            # Store the float F1 for early stopping checks
            current_val_f1_for_stopping = vf1_f

        except Exception as e:
             LOG.error(f"Exception during Validation Epoch {epoch+1}:\n{traceback.format_exc()}")
             # Append NaNs for all metrics if eval failed
             history['train_loss'].append(avg_epoch_loss) # Still store train loss
             history['val_loss'].append(np.nan)
             history['val_dice'].append(np.nan)
             history['val_iou'].append(np.nan)
             history['val_f1'].append(np.nan)
             history['val_precision'].append(np.nan)
             history['val_recall'].append(np.nan)
             history.setdefault('mcc', []).append(np.nan)
             current_val_f1_for_stopping = -1.0 # Use -1.0 if eval failed

        if val_f1 > best_val_f1:
            best_val_f1 = val_f1
            best_model_state = {k: v.cpu() for k, v in model.state_dict().items()}
            LOG.info(f">>> Validation F1 improved to {best_val_f1:.4f}. Saving model state. <<<")

        if use_es and early_stopper:
            early_stopper(val_f1)
            if early_stopper.early_stop:
              LOG.info("Early stopping triggered.")
              break

    end_time = time.time(); training_time = end_time - start_time
    LOG.info(f"--- Baseline Training Finished ({actual_epochs_run} epochs run) ---")
    LOG.info(f"Total Training Time: {training_time:.2f} seconds")
    LOG.info(f"Best Validation F1 Score achieved: {best_val_f1:.4f}")
    try:
        if best_model_state:
          torch.save(best_model_state, model_save_path)
          LOG.info(f"Best model state saved to {model_save_path}")
        elif actual_epochs_run > 0:
          last_state = {k:v.cpu() for k,v in model.state_dict().items()}
          torch.save(last_state, model_save_path.replace('.pth','_last.pth'))
          LOG.info(f"Saved last epoch model state to {model_save_path.replace('.pth','_last.pth')}.")
    except Exception as e: LOG.error(f"Error saving model: {e}\n{traceback.format_exc()}")

    return model, history, training_time

LOG.info("--- Baseline Training Function Definition Complete ---")

# Cell 10: Baseline Execution Setup

# --- Instantiate Global Criterion (on DEVICE) ---
LOG.info("Instantiating global criterion...")
criterion = None
try:
    # Ensure necessary variables exist
    if not all(k in globals() and globals()[k] is not None for k in ['LOSS_TYPE', 'pos_weight', 'FOCAL_GAMMA', 'TVERSKY_ALPHA', 'TVERSKY_BETA', 'DEVICE']):
        raise NameError("Required parameters (LOSS_TYPE, pos_weight, DEVICE, etc.) not found.")
    # get_loss_function now moves criterion to DEVICE
    criterion = get_loss_function(LOSS_TYPE, pos_weight_val=pos_weight)
    if criterion is None: raise ValueError("get_loss_function returned None.")
    LOG.info(f"Global criterion created: {criterion.__class__.__name__} on {next(criterion.parameters()).device if list(criterion.parameters()) else DEVICE}") # Check device
except Exception as e:
  LOG.error(f"ERROR creating global criterion: {e}")
  traceback.print_exc()

# --- Global Dictionaries ---
results = {}
histories = {}
saved_model_paths = {}
LOG.info("Global result dictionaries initialized.")

# --- Precondition Checks ---
LOG.info("Performing precondition checks...")
can_proceed = True
required_vars_exist = True
global_vars_needed = [
    'ENCODER_NAME', 'LOSS_TYPE', 'INPUT_CHANNELS', 'OUTPUT_CLASSES', 'criterion',
    'train_loader_baseline', 'val_loader', 'test_loader', 'task_loaders',
    'EPOCHS_BASELINE', 'EPOCHS_PER_TASK', 'LEARNING_RATE', 'DEVICE',
    'train_dataset_for_cl_split', 'SAVED_MODELS_DIR'
]
for var_name in global_vars_needed:
    if var_name not in globals() or globals()[var_name] is None:
        LOG.error(f"ERROR: Prerequisite '{var_name}' not defined or is None.")
        required_vars_exist = False
        can_proceed = False

if required_vars_exist: # Only check loaders if basic vars exist
    LOG.info("Checking DataLoaders...")
    loaders_to_check = {'Train Baseline': train_loader_baseline, 'Validation': val_loader, 'Test': test_loader}
    if isinstance(task_loaders, list) and task_loaders: loaders_to_check['Task 0'] = task_loaders[0]
    for name, loader in loaders_to_check.items():
        if loader is None:
          LOG.error(f"ERROR: {name} DataLoader is None.")
          can_proceed = False
        elif len(loader) == 0:
          LOG.warning(f"WARNING: {name} DataLoader has length 0.") # Decide if fatal
          if name != 'Task 0': LOG.info(f"ERROR: Essential DataLoader '{name}' is empty.")
          can_proceed = False

# Check model build
if can_proceed:
    LOG.info("Testing model building...")
    _test_model = build_model(ENCODER_NAME, ENCODER_WEIGHTS, INPUT_CHANNELS, OUTPUT_CLASSES) # Builds on DEVICE
    if _test_model is None:
      LOG.error(f"ERROR: Failed build '{ENCODER_NAME}'.")
      can_proceed = False
    else:
      del _test_model
      gc.collect()
      torch.cuda.empty_cache() if DEVICE.type == 'cuda' else None
      LOG.info("Model build test successful.")

if can_proceed: LOG.info("--- Preconditions met, Criterion Instantiated ---")
else: LOG.warning("--- Preconditions FAILED. Subsequent training may be skipped. ---")

cl_strategies = ['random', 'margin', 'least_confidence', 'entropy']

# Cell 11: Baseline Model Training and Evaluation


# --- Safely get params ---
encoder_name = globals().get('ENCODER_NAME', 'unknown_encoder')
encoder_weights = str(globals().get('ENCODER_WEIGHTS', 'unknown'))
loss_type = globals().get('LOSS_TYPE', 'unknown_loss')
epochs_baseline_str = str(globals().get('EPOCHS_BASELINE', 'unknown'))

# --- Define Experiment Details ---
experiment_details_dict = globals().setdefault('experiment_details_dict', {})
experiment_details_dict.update({ # Update with current baseline info
    "encoder": encoder_name, "encoder_weights": encoder_weights, "loss_type": loss_type,
    "baseline_epochs": epochs_baseline_str,
     # Keep other relevant details if already present
})

# --- Define Unique Run Name & Model Path ---
run_name_baseline = f"baseline_{encoder_name}W{encoder_weights}_{loss_type}_e{epochs_baseline_str}_amp"
LOG.info(f"Attempting Baseline Run: {run_name_baseline}")
baseline_save_path = os.path.join(SAVED_MODELS_DIR, f"{run_name_baseline}.pth")
baseline_last_save_path = baseline_save_path.replace('.pth', '_last.pth') # Path for _last model

# --- Define UNIQUE JSON Filename for THIS RUN ---
json_run_filename = os.path.join(SAVED_MODELS_DIR, f"{run_name_baseline}_results.json")
LOG.info(f"Checking for existing results file: {os.path.basename(json_run_filename)}")

# --- Skip Logic (Prioritize loading valid JSON results) ---
skip_training_baseline = False
baseline_eval_needed = True # Assume needed unless valid JSON loaded
# Default metrics placeholder
_def_metrics = {'loss': np.nan, 'dice': np.nan, 'iou': np.nan, 'f1': np.nan, 'precision': np.nan, 'recall': np.nan, 'mcc': np.nan, 'time': 0, 'mem_mb': 0}
results.setdefault(run_name_baseline, _def_metrics.copy()) # Ensure entry exists
histories.setdefault(run_name_baseline, {}) # Ensure entry exists

if os.path.exists(json_run_filename):
    LOG.warning(f"Results JSON exists: {os.path.basename(json_run_filename)}. Attempting to load.")
    try:
         with open(json_run_filename, 'r') as f:
              previous_data = json.load(f)
         prev_metrics = previous_data.get("final_test_metrics", {})
         # --- Check for a key metric (like f1) to validate ---
         if prev_metrics and 'f1' in prev_metrics and not math.isnan(float(prev_metrics['f1'])): # Check if F1 is present and not NaN
             LOG.info(f"Valid previous results loaded from JSON for '{run_name_baseline}'. Skipping training and evaluation.")
             skip_training_baseline = True
             baseline_eval_needed = False
             # --- Load data into memory dictionaries ---
             results[run_name_baseline] = prev_metrics
             histories[run_name_baseline] = previous_data.get("validation_history", {})
             # Try to find corresponding model path and store it
             if os.path.exists(baseline_save_path):
                  saved_model_paths[run_name_baseline] = baseline_save_path
             elif os.path.exists(baseline_last_save_path):
                  saved_model_paths[run_name_baseline] = baseline_last_save_path
             else:
                  LOG.warning(f"Loaded results from JSON, but corresponding model file (.pth or _last.pth) not found for {run_name_baseline}")
         else:
              LOG.warning("Previous JSON found but contained invalid/incomplete metrics. Will proceed to check for model file.")
    except Exception as json_e:
         LOG.error(f"Error loading or parsing previous JSON {os.path.basename(json_run_filename)}: {json_e}. Will proceed to check for model file.")

# Check model file only if JSON didn't lead to skipping everything
if not skip_training_baseline and not baseline_eval_needed: # Should only happen if JSON loaded successfully
     pass # Already decided to skip both
elif os.path.exists(baseline_save_path):
    LOG.warning(f"Model file exists: {os.path.basename(baseline_save_path)}. JSON not found/invalid. Training skipped, evaluation needed.")
    skip_training_baseline = True
    baseline_eval_needed = True # Ensure eval is needed
    saved_model_paths[run_name_baseline] = baseline_save_path
elif os.path.exists(baseline_last_save_path): # Check for _last only if .pth not found
     LOG.warning(f"Last model file exists: {os.path.basename(baseline_last_save_path)}. JSON not found/invalid. Training skipped, evaluation needed.")
     skip_training_baseline = True
     baseline_eval_needed = True # Ensure eval is needed
     saved_model_paths[run_name_baseline] = baseline_last_save_path
else:
    LOG.info("No existing model file or valid results JSON found. Training & Evaluation required.")
    skip_training_baseline = False
    baseline_eval_needed = True


# --- Execute Training (if needed) ---
model_out_base = None; time_baseline=0; peak_mem_baseline=0 # Init vars
if can_proceed and not skip_training_baseline:
    # ... (Training logic remains the same) ...
    # It calculates model_out_base, history_baseline, time_baseline, peak_mem_baseline
    LOG.info(f"=== Training Baseline Model ({run_name_baseline}) ===")
    baseline_model = build_model(ENCODER_NAME, ENCODER_WEIGHTS, INPUT_CHANNELS, OUTPUT_CLASSES)
    if baseline_model:
        optimizer_baseline = create_poly_optimizer(baseline_model, base_lr=LEARNING_RATE)
        if optimizer_baseline:
            scheduler_baseline = None
            try:
              num_batches_epoch = len(train_loader_baseline)
              num_training_steps = EPOCHS_BASELINE * num_batches_epoch
              scheduler_baseline = optim.lr_scheduler.PolynomialLR(optimizer_baseline, total_iters=num_training_steps, power=0.9)
              LOG.info(f"Baseline scheduler iters={num_training_steps}")
            except Exception as e_sch:
              LOG.error(f"ERROR Baseline scheduler: {e_sch}")
            if scheduler_baseline:
                if DEVICE.type == 'cuda':
                  torch.cuda.reset_peak_memory_stats(DEVICE)
                model_out_base, history_baseline, time_baseline = train_baseline(baseline_model, train_loader_baseline, val_loader, criterion, optimizer_baseline, scheduler_baseline, EPOCHS_BASELINE, DEVICE, model_save_path=baseline_save_path, patience=PATIENCE)
                histories[run_name_baseline] = history_baseline
                peak_mem_baseline = torch.cuda.max_memory_allocated(DEVICE)/(1024**2) if DEVICE.type == 'cuda' else 0.0
                # Update saved path map after training
                if os.path.exists(baseline_save_path):
                  saved_model_paths[run_name_baseline] = baseline_save_path
                elif os.path.exists(baseline_last_save_path):
                  saved_model_paths[run_name_baseline] = baseline_last_save_path
            else:
              LOG.error("Baseline skip: scheduler fail.")
              baseline_eval_needed=False
              results[run_name_baseline]=_def_metrics.copy()
        else:
          LOG.error("Baseline skip: optimizer fail.")
          baseline_eval_needed=False
          results[run_name_baseline]=_def_metrics.copy()
    else:
      LOG.error("Baseline skip: model build fail.")
      baseline_eval_needed=False
      results[run_name_baseline]=_def_metrics.copy()
elif not can_proceed:
    LOG.error(f"Cannot run baseline '{run_name_baseline}': Preconditions failed.")
    baseline_eval_needed=False # Can't eval if preconditions fail
elif skip_training_baseline and baseline_eval_needed:
    LOG.info(f"Baseline training skipped for {run_name_baseline}, proceeding to evaluation.")
    # Get time/mem from results dict if previously loaded, otherwise keep 0
    time_baseline = results[run_name_baseline].get('time', 0)
    peak_mem_baseline = results[run_name_baseline].get('mem_mb', 0)
elif skip_training_baseline and not baseline_eval_needed:
     LOG.info(f"Baseline training AND evaluation skipped for {run_name_baseline} based on loaded JSON.")


# --- Execute Evaluation (if needed and possible) ---
evaluation_performed_this_run = False # Flag to check if eval was actually done
if baseline_eval_needed and can_proceed and test_loader is not None:
    LOG.info(f"--- Evaluating Baseline ({run_name_baseline}) on TEST set ---")
    eval_model = None
    try:
        # Load model (either from training or file)
        if model_out_base: # Use model trained in this session if available
            eval_model = model_out_base
            LOG.info("Evaluating model trained this session.")
        # Otherwise, load from saved path if it exists in our map
        elif saved_model_paths.get(run_name_baseline) and os.path.exists(saved_model_paths[run_name_baseline]):
             model_file_to_load = saved_model_paths[run_name_baseline]
             LOG.info(f"Loading existing baseline model from: {os.path.basename(model_file_to_load)}")
             eval_model = build_model(ENCODER_NAME, ENCODER_WEIGHTS, INPUT_CHANNELS, OUTPUT_CLASSES)
             if eval_model:
                  # Use weights_only=True for security if loading untrusted files
                  # For your own files, False is okay but generates a warning
                  state_dict = torch.load(model_file_to_load, map_location=DEVICE, weights_only=False) # Set weights_only
                  if 'model_state_dict' in state_dict: state_dict = state_dict['model_state_dict'] # Handle checkpoint dicts
                  eval_model.load_state_dict(state_dict)
                  eval_model.to(DEVICE)
                  LOG.info("Loaded weights.")
             else:
              LOG.error("Failed to build model for evaluation.")
        else:
          LOG.error("Cannot evaluate baseline: No model from training and no valid saved path found/exists.")

        # Perform evaluation if model is ready
        if eval_model:
            eval_model.eval() # Ensure eval mode
            eval_output = evaluate_model(eval_model, test_loader, criterion, DEVICE)
            loss_base_test, dice_base_test, iou_base_test, f1_base_test, precision_base_test, recall_base_test, mcc_base_test, \
            tp_base, tn_base, fp_base, fn_base = eval_output

            # --- Store results in memory dictionary ---
            current_run_results = {
                'loss': loss_base_test, 'dice': dice_base_test, 'iou': iou_base_test, 'f1': f1_base_test,
                'precision': precision_base_test, 'recall': recall_base_test, 'mcc': mcc_base_test,
                'tp': tp_base, 'tn': tn_base, 'fp': fp_base, 'fn': fn_base,
                # Make sure time/mem reflect actual run (0 if only eval)
                'time': time_baseline if not skip_training_baseline else 0, # Time is training time
                'mem_mb': peak_mem_baseline if not skip_training_baseline else 0 # Mem is peak during training
            }
            results[run_name_baseline] = current_run_results # Update global dict

            # --- Log results ---
            LOG.info("--------------------------------------------------------------------------------------")
            LOG.info(f"Final Baseline TEST -> L:{loss_base_test:.4f}, F1:{f1_base_test:.4f}, IoU:{iou_base_test:.4f}, MCC:{mcc_base_test:.4f}")
            LOG.info(f"P:{precision_base_test:.4f}, R:{recall_base_test:.4f}, PeakMem:{current_run_results['mem_mb']:.2f} MB, Time:{current_run_results['time']:.2f} s") # Use stored results
            LOG.info(f"Confusion Matrix (Test): TP={tp_base:.0f}, TN={tn_base:.0f}, FP={fp_base:.0f}, FN={fn_base:.0f}")
            LOG.info("--------------------------------------------------------------------------------------")
            evaluation_performed_this_run = True # Mark that eval happened
        else:
             LOG.warning("Skipping evaluation as model could not be loaded or trained.")
             # Ensure results dict has defaults if eval skipped
             results[run_name_baseline].update(_def_metrics.copy())

    except Exception as e_eval:
        LOG.error(f"ERROR Baseline TEST evaluation: {e_eval}", exc_info=True)
        results[run_name_baseline].update(_def_metrics.copy())

    # --- Cleanup loaded evaluation model ---
    if eval_model and not model_out_base: del eval_model


# --- Save results to JSON if evaluation was performed or training happened ---
if evaluation_performed_this_run or not skip_training_baseline:
    experiment_details_dict["last_update"] = datetime.now().isoformat()
    save_metrics_to_json(
         filename=json_run_filename, # Use the unique filename
         run_name=run_name_baseline,
         run_result_summary=results.get(run_name_baseline),
         run_cl_detail_matrix=None,
         run_history=histories.get(run_name_baseline),
         experiment_details=experiment_details_dict
    )
else:
     LOG.info(f"Skipping JSON save for {run_name_baseline} as neither training nor evaluation was performed in this session.")


# --- Final Cleanup after Baseline ---
# ... (cleanup remains the same) ...
if 'model_out_base' in locals() and model_out_base: del model_out_base;
if 'baseline_model' in locals() and baseline_model: del baseline_model;
if 'optimizer_baseline' in locals() and optimizer_baseline: del optimizer_baseline;
if 'scheduler_baseline' in locals() and scheduler_baseline: del scheduler_baseline;
gc.collect(); torch.cuda.empty_cache() if DEVICE.type == 'cuda' else None

LOG.info(f"===== Baseline Processing Complete for {run_name_baseline} =====")

# Cell 12: Load Metadata and Create Mappings


# --- Load Activations Metadata ---
emsr_id_to_info = {}
if os.path.exists(ACTIVATIONS_JSON_PATH):
    try:
        with open(ACTIVATIONS_JSON_PATH, 'r') as f:
            activations_data = json.load(f)

        # Populate mapping: EMSR ID -> {'start_dt': datetime, 'subset': str}
        for emsr_id, info in activations_data.items():
            start_dt = parse_datetime(info.get('start'))
            subset = info.get('subset')
            if start_dt and subset:
                emsr_id_to_info[emsr_id] = {'start_dt': start_dt, 'subset': subset}
            else:
                 LOG.warning(f"Warning: Missing start date or subset for {emsr_id}. Skipping.")
        LOG.info(f"Loaded metadata for {len(emsr_id_to_info)} EMSR events from {ACTIVATIONS_JSON_PATH}")
    except Exception as e:
        LOG.error(f"ERROR loading or parsing {ACTIVATIONS_JSON_PATH}: {e}")
        activations_data = {} # Ensure it exists but is empty on error
else:
    LOG.error(f"ERROR: {ACTIVATIONS_JSON_PATH} not found. Cannot proceed with time-based tasks.")
    activations_data = {} # Ensure it exists but is empty

# --- Build Path-to-EMSR ID Mappings ---
# Combine all path lists for easier mapping (assuming they are defined globally)
all_sar_paths = (train_sar_paths or []) + (val_sar_paths or []) + (test_sar_paths or [])
all_dem_paths = (train_dem_paths or []) + (val_dem_paths or []) + (test_dem_paths or [])
all_mask_paths = (train_mask_paths or []) + (val_mask_paths or []) + (test_mask_paths or [])

chip_path_to_emsr_id = {}
unmapped_count = 0
for path_list in [all_sar_paths, all_dem_paths, all_mask_paths]:
    if not path_list: continue # Skip if a list is empty
    for path in path_list:
        emsr_id = get_emsr_id_from_path(path)
        if emsr_id and emsr_id in emsr_id_to_info: # Only map if ID is found and in metadata
            chip_path_to_emsr_id[path] = emsr_id
        else:
            unmapped_count += 1

if unmapped_count > 0:
     LOG.warning(f"Warning: {unmapped_count} file paths could not be mapped to a valid EMSR ID found in metadata.")
LOG.info(f"Mapped {len(chip_path_to_emsr_id)} chip paths to EMSR IDs.")

# Check if mappings worked for training data
train_mapped_count = sum(1 for p in train_sar_paths if p in chip_path_to_emsr_id)
LOG.info(f"Successfully mapped {train_mapped_count} out of {len(train_sar_paths)} training SAR paths.")
if train_mapped_count == 0 and len(train_sar_paths) > 0:
     LOG.info("CRITICAL ERROR: No training paths were mapped. Cannot create time-based tasks.")

LOG.info("--- Metadata Loading Complete ---")

# Cell 13: Define Tasks by Year Ranges & Create Task-Specific DataLoaders (Corrected Arg Name & Percentiles)

# --- Variables for storing task-specific loaders ---
time_task_train_loaders = []
time_task_test_loaders = []
task_definitions = [] # Stores tuples: {'id': int, 'years': str, 'train_loader': DataLoader, 'test_loader': DataLoader}

# --- Define Year Ranges ---
year_ranges = [(2014, 2017), (2018, 2019), (2020, 2021)] # 3 ranges
# --- Update NUM_TASKS based on year_ranges ---
original_num_tasks = globals().get('NUM_TASKS', 0)
NUM_TASKS = len(year_ranges) # Set NUM_TASKS based on the defined ranges
if original_num_tasks != NUM_TASKS: LOG.warning(f"Adjusting NUM_TASKS from {original_num_tasks} to {NUM_TASKS} based on year_ranges.")
# --------------------------------------------

# --- Create Loaders for each Year Range Task ---
# <<< Check if NORM_PERCENTILES exists globally (it should after Cell 2 runs) >>>
if 'NORM_PERCENTILES' not in globals():
     LOG.error("CRITICAL: NORM_PERCENTILES not found globally. Run Cell 2 first!")
     # Set to None to prevent NameError below, but warnings will persist in Dataset init
     NORM_PERCENTILES = None

if 'chip_path_to_emsr_id' in locals() and chip_path_to_emsr_id and \
   'emsr_id_to_info' in locals() and emsr_id_to_info and \
   'train_sar_paths' in locals() and train_sar_paths and \
   'test_sar_paths' in locals() and test_sar_paths and NUM_TASKS > 0:

    LOG.info(f"Defining {NUM_TASKS} tasks based on year ranges...")
    num_workers = min(os.cpu_count() // 2, 4)
    pin_memory = True if DEVICE.type == 'cuda' else False

    for i, (start_year, end_year) in enumerate(year_ranges):
        task_id = i + 1
        LOG.info(f"Processing Task {task_id} ({start_year}-{end_year})...")
        task_train_paths_sar, task_train_paths_dem, task_train_paths_mask = [], [], []
        task_test_paths_sar, task_test_paths_dem, task_test_paths_mask = [], [], []

        # Gather Training Paths
        for path_idx, sar_path in enumerate(train_sar_paths):
            emsr_id = chip_path_to_emsr_id.get(sar_path); event_info = emsr_id_to_info.get(emsr_id)
            if event_info and start_year <= event_info['start_dt'].year <= end_year:
                 task_train_paths_sar.append(sar_path); task_train_paths_dem.append(train_dem_paths[path_idx]); task_train_paths_mask.append(train_mask_paths[path_idx])

        # Gather Test Paths
        for path_idx, sar_path in enumerate(test_sar_paths):
             emsr_id = chip_path_to_emsr_id.get(sar_path); event_info = emsr_id_to_info.get(emsr_id)
             if event_info and start_year <= event_info['start_dt'].year <= end_year:
                  task_test_paths_sar.append(sar_path); task_test_paths_dem.append(test_dem_paths[path_idx]); task_test_paths_mask.append(test_mask_paths[path_idx])

        LOG.info(f"Found {len(task_train_paths_sar)} train samples and {len(task_test_paths_sar)} test samples.")

        # Create Train Loader
        train_loader = None
        if task_train_paths_sar:
            if 'train_spatial_pixel_transform' not in globals() or 'train_post_transform' not in globals(): LOG.error("ERROR: Train transforms missing.");
            else:
                task_train_dataset = MMFloodDataset(
                    task_train_paths_sar, task_train_paths_dem, task_train_paths_mask,
                    spatial_pixel_transform=train_spatial_pixel_transform,
                    post_transform=train_post_transform,
                    # +++ ADD norm_percentiles ARGUMENT HERE +++
                    norm_percentiles=NORM_PERCENTILES
                )
                train_loader = DataLoader(task_train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=num_workers, pin_memory=pin_memory, drop_last=True, collate_fn=collate_fn_skip_error)
                if train_loader: LOG.info(f"Created train loader with {len(train_loader)} batches.")
        else: LOG.warning(f"WARNING: No training samples for Task {task_id}.")
        time_task_train_loaders.append(train_loader)

        # Create Test Loader
        test_loader = None
        if task_test_paths_sar:
            if 'val_test_spatial_pixel_transform' not in globals() or 'val_test_post_transform' not in globals(): LOG.error("ERROR: Val/Test transforms missing.");
            else:
                task_test_dataset = MMFloodDataset(
                    task_test_paths_sar, task_test_paths_dem, task_test_paths_mask,
                    spatial_pixel_transform=val_test_spatial_pixel_transform,
                    post_transform=val_test_post_transform,
                    # norm_percentiles arg
                    norm_percentiles=NORM_PERCENTILES
                )
                test_loader = DataLoader(task_test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers, pin_memory=pin_memory, collate_fn=collate_fn_skip_error)
                if test_loader: LOG.info(f"Created test loader with {len(test_loader)} batches.")
        else: LOG.warning(f"WARNING: No test samples for Task {task_id}.")
        time_task_test_loaders.append(test_loader)

        task_definitions.append({'id': task_id, 'years': f"{start_year}-{end_year}", 'train_loader': train_loader, 'test_loader': test_loader})

    valid_train_loaders = [ldr for ldr in time_task_train_loaders if ldr is not None and len(ldr) > 0]
    valid_test_loaders = [ldr for ldr in time_task_test_loaders if ldr is not None and len(ldr) > 0]
    LOG.info(f"Created {len(valid_train_loaders)} non-empty training loaders and {len(valid_test_loaders)} non-empty test loaders.")
    if len(valid_train_loaders) < NUM_TASKS or len(valid_test_loaders) < NUM_TASKS: LOG.warning("WARNING: One or more tasks lack valid train or test loaders.")

else: LOG.warning("Skipping year-based task definition: Missing prerequisites."); time_task_train_loaders = []; time_task_test_loaders = []; task_definitions = []

LOG.info("--- Year-Based Task Definition Complete ---")

# Cell 14: Define CL Training & Evaluation Function


def train_cl_detailed_eval(
    model, time_task_train_loaders, time_task_test_loaders, val_loader,
    criterion, optimizer, scheduler, epochs_per_task, replay_buffer,
    replay_batch_size, sampling_strategy,
    original_train_paths, # Expects a tuple: (train_sar_paths, train_dem_paths, train_mask_paths)
    device, model_save_path='unet_cl_detailed_eval.pth', num_tasks = NUM_TASKS,
    augment_replay=True, patience=PATIENCE, checkpoint_path=None):
    """ Trains CL with AMP, combined batches, path buffer, and checkpointing. """

    strategy_name=sampling_strategy
    use_es=globals().get('USE_EARLY_STOPPING', False)
    is_cuda = device.type == 'cuda'
    LOG.info(f"--- Starting Continual Learning Training: {is_cuda} ---")
    LOG.info(f"Strategy: {strategy_name} sampling | Buffer: Path Storage | Combined Batches: True")
    # ... other log messages ...

    scaler = torch.amp.GradScaler(enabled=is_cuda)

    # Define replay_post_transform internally (needed for processing loaded paths)
    replay_post_transform = None
    try:
        if not isinstance(NORM_MEAN, list) or len(NORM_MEAN) != INPUT_CHANNELS or not isinstance(NORM_STD, list) or len(NORM_STD) != INPUT_CHANNELS: raise ValueError("NORM_MEAN/STD invalid.")
        replay_post_transform = A.Compose([A.Normalize(mean=NORM_MEAN, std=NORM_STD, max_pixel_value=1.0), ToTensorV2()])
        LOG.info("Replay post-transform defined.")
    except Exception as e_tf: LOG.critical(f"CRITICAL ERROR defining replay_post_transform: {e_tf}"); return model, {}, 0, [[]]

    # --- Checkpoint Loading/Init ---
    start_k = 0; start_epoch = 0; global_step = 0
    val_history={'val_loss':[],'val_dice':[],'val_iou':[],'val_f1':[],'val_precision':[],'val_recall':[], 'mcc': []}
    best_overall_val_f1 = -1.0
    best_overall_val_f1 = -1.0
    _def_metrics={'loss':np.nan,'f1':np.nan,'prec':np.nan,'rec':np.nan,'dice':np.nan,'iou':np.nan}
    eval_results_matrix=[[_def_metrics.copy() for _ in range(num_tasks)] for _ in range(num_tasks)]
    overall_start_time=time.time(); current_task_training_times=[]
    best_model_state=None
    cl_early_stopper=EarlyStopping(patience=patience, verbose=True, delta=0.001, mode='max', trace_func=LOG.info) if use_es else None
    if cl_early_stopper: LOG.info("EarlyStopping initialized.")
    if not time_task_train_loaders or len(time_task_train_loaders)!=num_tasks: LOG.error("Invalid train loaders."); return model, val_history, 0, eval_results_matrix

    # Initial Device Placement Check
    try: # Only need to handle pos_weight potentially
        if isinstance(criterion, nn.BCEWithLogitsLoss) and criterion.pos_weight is not None:
             if criterion.pos_weight.device != device: criterion.pos_weight = criterion.pos_weight.to(device)
        LOG.info(f"Initial Criterion placement check on {device} successful.")
    except Exception as e: LOG.error(f"Error placing criterion pos_weight: {e}"); return model, val_history, 0, eval_results_matrix

    # Checkpoint Loading
    if checkpoint_path and os.path.exists(checkpoint_path):
        LOG.warning(f"Attempting load: {os.path.basename(checkpoint_path)}")
        try:
            checkpoint = torch.load(checkpoint_path, map_location='cpu')
            model.load_state_dict(checkpoint['model_state_dict']); model.to(device)
            optimizer.load_state_dict(checkpoint['optimizer_state_dict']); scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
            if 'scaler_state_dict' in checkpoint and is_cuda: scaler.load_state_dict(checkpoint['scaler_state_dict']); LOG.info("GradScaler state loaded.")
            # Load path buffer
            replay_buffer.buffer = deque(checkpoint['replay_buffer'], maxlen=replay_buffer.max_size)
            LOG.info(f"Path Replay buffer restored: {len(replay_buffer)} items.")
            # End Load path buffer
            global_step = checkpoint.get('global_step', 0); val_history = checkpoint.get('val_history', val_history); best_overall_val_f1 = checkpoint.get('best_overall_val_f1', -1.0)
            loaded_task_k = checkpoint['task_k']; loaded_epoch = checkpoint['epoch']
            if loaded_epoch == epochs_per_task - 1: start_k = loaded_task_k + 1; start_epoch = 0
            else: start_k = loaded_task_k; start_epoch = loaded_epoch + 1
            LOG.warning(f"Checkpoint loaded. Resume Task {start_k + 1}, Epoch {start_epoch + 1}.")
            del checkpoint; gc.collect()
        except Exception as load_e: LOG.error(f"ERROR loading checkpoint: {load_e}"); LOG.warning("Training from scratch."); start_k=0; start_epoch=0; global_step=0; replay_buffer.buffer.clear(); val_history={'val_loss':[],'f1':[]}; best_overall_val_f1 = -1.0
    else: LOG.info("No checkpoint. Starting training from scratch.")

    # --- Get original train paths for buffer update ---
    if original_train_paths is None or len(original_train_paths) != 3:
        LOG.error("original_train_paths tuple (sar, dem, mask) not provided correctly. Cannot update path buffer."); return model, val_history, 0, eval_results_matrix
    orig_train_sar_paths, orig_train_dem_paths, orig_train_mask_paths = original_train_paths
    num_orig_train_samples = len(orig_train_sar_paths)
    if num_orig_train_samples == 0:
         LOG.warning("Original training paths are empty. Replay buffer cannot be filled.")


    LOG.info(f"Starting CL training loop from Task {start_k+1}, Epoch {start_epoch+1}...")
    actual_tasks_run=0
    # --- Task Loop ---
    for k in range(start_k, num_tasks):
        actual_tasks_run+=1; LOG.info(f"--- Processing Task {k+1}/{num_tasks} ---"); task_start_time=time.time()
        current_task_start_epoch = start_epoch if k == start_k else 0
        if current_task_start_epoch >= epochs_per_task: LOG.info(f"Skipping Task {k+1} (completed)."); continue

        current_task_loader = time_task_train_loaders[k]
        if current_task_loader is None or len(current_task_loader)==0: LOG.warning(f"Task {k+1} loader empty. Skip train epochs.")
        else:
            # --- Epoch Loop ---
            for epoch in range(current_task_start_epoch, epochs_per_task):
                model.train(); running_loss=0.0; processed_batches=0; progress_bar=tqdm(current_task_loader, desc=f"Task {k+1}, Ep {epoch+1}", leave=False)
                # --- Batch Loop ---
                for current_images, current_masks in progress_bar:
                    if current_images.nelement()==0 or current_masks.nelement()==0: continue
                    current_images, current_masks = current_images.to(device), current_masks.to(device)

                    # --- Combined Batch Logic with Path Loading ---
                    replay_images_processed, replay_masks_processed = None, None
                    if len(replay_buffer) > 0:
                        # 1. Sample Path Tuples
                        sampled_path_tuples = replay_buffer.sample(replay_batch_size, strategy=sampling_strategy, model=model, subset_fraction=UNCERTAINTY_SUBSET_FRACTION)

                        if sampled_path_tuples:
                            # 2. Load and Process Paths
                            proc_rep_img_list, proc_rep_mask_list = [], []
                            LOG.debug(f"Loading/Processing {len(sampled_path_tuples)} sampled replay paths...")
                            for sar_p, dem_p, mask_p in sampled_path_tuples:
                                try:
                                    # --- Replicate MMFloodDataset loading logic ---
                                    with rasterio.open(sar_p) as src: sar_data = src.read(out_shape=(2, IMAGE_HEIGHT, IMAGE_WIDTH), resampling=Resampling.bilinear).astype(np.float32); sar_data = np.transpose(sar_data, (1, 2, 0))
                                    with rasterio.open(dem_p) as src: dem_data = src.read(1, out_shape=(IMAGE_HEIGHT, IMAGE_WIDTH), resampling=Resampling.bilinear).astype(np.float32); dem_data = np.expand_dims(dem_data, axis=-1)
                                    with rasterio.open(mask_p) as src: mask_data = src.read(1, out_shape=(IMAGE_HEIGHT, IMAGE_WIDTH), resampling=Resampling.nearest).astype(np.uint8); mask_data = np.expand_dims(mask_data, axis=-1)
                                    mask_data = (mask_data > 0).astype(np.float32)
                                    # Augment if needed
                                    if augment_replay: tfd = train_spatial_pixel_transform(image=sar_data, dem=dem_data, mask=mask_data); i_s, i_d, m_s = tfd['image'], tfd['dem'], tfd['mask']
                                    else: i_s, i_d, m_s = sar_data, dem_data, mask_data
                                    # Scale (using pre-calculated percentiles)
                                    vv_p_min, vv_p_max = NORM_PERCENTILES.get('vv', {}).get('p_min'), NORM_PERCENTILES.get('vv', {}).get('p_max')
                                    vh_p_min, vh_p_max = NORM_PERCENTILES.get('vh', {}).get('p_min'), NORM_PERCENTILES.get('vh', {}).get('p_max')
                                    dem_p_min, dem_p_max = NORM_PERCENTILES.get('dem', {}).get('p_min'), NORM_PERCENTILES.get('dem', {}).get('p_max')
                                    i_s[..., 0] = robust_scale(i_s[..., 0], p_min=vv_p_min, p_max=vv_p_max)
                                    i_s[..., 1] = robust_scale(i_s[..., 1], p_min=vh_p_min, p_max=vh_p_max)
                                    i_d[..., 0] = robust_scale(i_d[..., 0], p_min=dem_p_min, p_max=dem_p_max)
                                    # Combine and Post-Transform (Normalize + ToTensor)
                                    img_comb = np.concatenate([i_s, i_d], axis=-1)
                                    fpost = replay_post_transform(image=img_comb)
                                    i_norm_t = fpost['image']
                                    m_fin_t = torch.from_numpy(m_s.transpose(2, 0, 1)).float()
                                    proc_rep_img_list.append(i_norm_t); proc_rep_mask_list.append(m_fin_t)
                                except Exception as load_err: LOG.warning(f"Error loading/processing replay path {os.path.basename(sar_p)}: {load_err}")
                                    # --- End Replicate ---

                            # 3. Stack processed replay data if any succeeded
                            if proc_rep_img_list:
                                try:
                                    replay_images_processed=torch.stack(proc_rep_img_list).to(device)
                                    replay_masks_processed=torch.stack(proc_rep_mask_list).to(device)
                                    LOG.debug(f"Successfully processed and stacked {len(proc_rep_img_list)} replay samples.")
                                except Exception as stack_err: LOG.error(f"Error stacking processed replay: {stack_err}")
                            else: LOG.warning("No replay paths successfully processed for this batch.")

                    # 4. Concatenate current and replay batches
                    if replay_images_processed is not None and replay_masks_processed is not None:
                        combined_images = torch.cat((current_images, replay_images_processed), dim=0)
                        combined_masks = torch.cat((current_masks, replay_masks_processed), dim=0)
                    else: # Use only current batch if replay failed
                        combined_images = current_images; combined_masks = current_masks
                    # --- End Combined Batch Logic ---

                    # --- Single Forward/Backward Pass with AMP ---
                    optimizer.zero_grad()
                    try:
                        with torch.amp.autocast(device_type=device.type, enabled=is_cuda):
                          outputs = model(combined_images)
                          loss = criterion(outputs, combined_masks.float()) # Loss on combined batch
                        scaler.scale(loss).backward()
                        scaler.step(optimizer)
                        scaler.update()
                        scheduler.step()
                        global_step += 1
                        running_loss += loss.item(); processed_batches += 1
                        if processed_batches > 0: progress_bar.set_postfix(loss=running_loss/processed_batches, lr=f"{scheduler.get_last_lr()[0]:.2e}")
                    except Exception as e: LOG.error(f"Exception Train Task {k+1} Ep {epoch+1} Batch:\n{traceback.format_exc()}"); optimizer.zero_grad(); continue
                # --- End Batch Loop ---

                avg_epoch_loss=running_loss/processed_batches if processed_batches > 0 else 0.0; base_log_message = f"T{k+1}, E{epoch+1}/{epochs_per_task} TrainLoss:{avg_epoch_loss:.4f}"
                if processed_batches == 0: LOG.warning(f"No batches processed T{k+1}, E{epoch+1}")

                # --- Epoch Checkpointing (Save path buffer) ---
                checkpoint_saved_successfully = False; saved_checkpoint_basename = None
                if SAVE_EPOCH_CHECKPOINTS:
                    try:
                        checkpoint_data = { 'task_k': k, 'epoch': epoch, 'model_state_dict': {key: val.cpu() for key, val in model.state_dict().items()}, 'optimizer_state_dict': optimizer.state_dict(), 'scheduler_state_dict': scheduler.state_dict(),
                                           # Save path buffer
                                           'replay_buffer': list(replay_buffer.buffer),
                                           'global_step': global_step, 'val_history': val_history, 'best_overall_val_f1': best_overall_val_f1, 'scaler_state_dict': scaler.state_dict() if is_cuda else None }
                        base_name = os.path.splitext(model_save_path)[0]; checkpoint_filename = f"{base_name}_task{k+1}_epoch{epoch+1}_ckpt.pth"
                        torch.save(checkpoint_data, checkpoint_filename); saved_checkpoint_basename = os.path.basename(checkpoint_filename); checkpoint_saved_successfully = True
                    except Exception as ckpt_e: LOG.error(f"ERROR saving epoch checkpoint: {ckpt_e}")
                if checkpoint_saved_successfully and saved_checkpoint_basename: LOG.info(f"{base_log_message} | Ckpt: {saved_checkpoint_basename}")
                else: LOG.info(base_log_message);
            # --- End Epoch Loop ---

        # --- After Task Training ---
        if current_task_start_epoch < epochs_per_task:
            task_train_end_time=time.time(); task_training_time=task_train_end_time-task_start_time; current_task_training_times.append(task_training_time); LOG.info(f"Task {k+1} training complete. Time: {task_training_time:.2f}s")
            # --- Buffer Update (Add Paths) ---
            LOG.info(f"Updating path replay buffer..."); num_to_add=replay_buffer.max_size//num_tasks+1 if num_tasks>0 else 50;
            if num_orig_train_samples > 0 :
                added_count=0
                # Sample indices from the original full training dataset
                indices_to_add = random.sample(range(num_orig_train_samples), min(num_to_add, num_orig_train_samples))
                for idx in indices_to_add:
                    try:
                        # Retrieve the path tuple for the sampled index
                        path_tuple = (orig_train_sar_paths[idx], orig_train_dem_paths[idx], orig_train_mask_paths[idx])
                        replay_buffer.add(path_tuple) # Add the path tuple
                        added_count+=1
                    except IndexError:
                         LOG.error(f"IndexError retrieving paths for index {idx} during buffer update.")
                    except Exception as e_add:
                         LOG.error(f"Error adding path tuple for index {idx} to buffer: {e_add}")
                LOG.info(f"Added {added_count} path tuples. Buffer size: {len(replay_buffer)}/{replay_buffer.max_size}")
            else: LOG.warning("Original train paths empty, cannot update buffer.")
            # --- End Buffer Update ---

            # Detailed Evaluation
            LOG.info(f"Performing evaluation after Task {k+1}...");
            for j in range(k+1):
                 test_loader_j=time_task_test_loaders[j] if j < len(time_task_test_loaders) else None
                 if test_loader_j is None or len(test_loader_j)==0: LOG.warning(f"Skip eval Task {j+1}."); continue
                 LOG.info(f"Evaluating on Task {j+1} test set...");
                 try:
                     # --- Call evaluate_model and unpack ALL 11 results ---
                     eval_output_task = evaluate_model(model, test_loader_j, criterion, device)
                     t_loss, t_dice, t_iou, t_f1, t_prec, t_rec, t_mcc, \
                     t_tp, t_tn, t_fp, t_fn = eval_output_task

                     # --- Store all results in the matrix ---
                     eval_results_matrix[k][j] = {
                         'loss': t_loss, 'dice': t_dice, 'iou': t_iou, 'f1': t_f1,
                         'precision': t_prec, 'recall': t_rec, 'mcc': t_mcc,
                         'tp': t_tp, 'tn': t_tn, 'fp': t_fp, 'fn': t_fn
                     }
                     LOG.info(f"  -> T{j+1} Test: L:{t_loss:.4f}|F1:{t_f1:.4f}|P:{t_prec:.4f}|R:{t_rec:.4f}|MCC:{t_mcc:.4f}")

                 except Exception as e:
                      LOG.error(f"Exc Detail Eval Task {j+1}:\n{traceback.format_exc()}")
                      # Store placeholder on error, ensure all keys exist if possible
                      eval_results_matrix[k][j] = {
                          'loss': np.nan, 'dice': np.nan, 'iou': np.nan, 'f1': np.nan,
                          'precision': np.nan, 'recall': np.nan, 'mcc': np.nan,
                          'tp': 0, 'tn': 0, 'fp': 0, 'fn': 0
                      }
            # Mixed Validation
            mixed_val_f1=np.nan
            if val_loader is not None:
                LOG.info(f"Eval mixed val set...")
                try:
                    # --- Evaluate model ---
                    eval_output_val = evaluate_model(model, val_loader, criterion, device)
                    vl, vd, vi, vf1, vp, vr, vmcc, v_tp, v_tn, v_fp, v_fn = eval_output_val

                    # --- Convert all metrics safely to CPU floats ---
                    metrics_cpu = []
                    for metric_val in eval_output_val[:7]:
                        if isinstance(metric_val, torch.Tensor):
                             # If it's a tensor (unexpectedly), get scalar via .item()
                             metrics_cpu.append(metric_val.item())
                             # LOG.warning(f"evaluate_model returned a tensor ({metric_val.device}), converting via .item()")
                        elif metric_val is None:
                             metrics_cpu.append(np.nan) # Handle None case
                        else:
                             try:
                                 # Ensure it's a float, handles ints etc.
                                 metrics_cpu.append(float(metric_val))
                             except (ValueError, TypeError):
                                 LOG.warning(f"Could not convert metric value {metric_val} to float, using NaN.")
                                 metrics_cpu.append(np.nan)

                    vl_f, vd_f, vi_f, vf1_f, vp_f, vr_f, vmcc_f = metrics_cpu # Unpack CPU floats
                    # --- End Safe Conversion ---

                    # Add safe floats to history
                    val_history['val_loss'].append(vl_f);
                    val_history['val_dice'].append(vd_f);
                    val_history['val_iou'].append(vi_f);
                    val_history['val_f1'].append(vf1_f);
                    val_history['val_precision'].append(vp_f);
                    val_history['val_recall'].append(vr_f);
                    val_history['mcc'].append(vmcc_f)

                    LOG.info(f"Mixed Val >>> L: {vl_f:.4f} | F1: {vf1_f:.4f}")
                    mixed_val_f1 = vf1_f # Use the guaranteed float

                    # --- Now use the guaranteed CPU float with np.isnan and comparison ---
                    if not np.isnan(vf1_f) and vf1_f > best_overall_val_f1:
                         best_overall_val_f1 = vf1_f # Use the float value
                         best_model_state = {ky: v.cpu() for ky, v in model.state_dict().items()}
                         LOG.info(f">> Val F1 improved:{vf1_f:.4f}. Saving state.")

                except Exception as e:
                    LOG.error(f"Exc Mixed Val Task {k+1}:\n{traceback.format_exc()}")
                    # Append NaN to history if evaluation failed
                    val_history['val_loss'].append(np.nan); val_history['val_dice'].append(np.nan); val_history['val_iou'].append(np.nan); val_history['val_f1'].append(np.nan); val_history['val_precision'].append(np.nan); val_history['val_recall'].append(np.nan)
                    mixed_val_f1 = np.nan # Ensure mixed_val_f1 is NaN if eval failed
            else:
                LOG.info("Skip mixed val eval.")
    # --- End Task Loop ---

    # Final logging and saving
    overall_end_time=time.time(); total_training_time=overall_end_time-overall_start_time
    LOG.info(f"--- Continual Learning Training Finished ---"); LOG.info(f"Tasks Run: {actual_tasks_run} | Time: {total_training_time:.2f}s | Best Val F1: {best_overall_val_f1:.4f}")
    try:
        if best_model_state: torch.save(best_model_state, model_save_path); LOG.info(f"Best model saved: {os.path.basename(model_save_path)}")
        elif actual_tasks_run>0: last_state={ky:v.cpu() for ky,v in model.state_dict().items()}; last_path = model_save_path.replace('.pth','_last.pth'); torch.save(last_state, last_path); LOG.info(f"Saved last task model: {os.path.basename(last_path)}")
        else: LOG.warning("No tasks run, nothing saved as 'last'.")
    except Exception as e: LOG.error(f"Error saving CL model:{e}")

    return model, val_history, total_training_time, eval_results_matrix

LOG.info("--- CL Training Function Defined ---")

"""## CL Model Training with Buffer Size = 100"""

# Cell 15: Execute CL Training (REPLAY_BUFFER_SIZE = 100)


# --- Init dictionaries if they don't exist ---
results = globals().setdefault('results', {}); histories = globals().setdefault('histories', {}); saved_model_paths = globals().setdefault('saved_model_paths', {}); cl_detailed_results = globals().setdefault('cl_detailed_results', {})
if not results: LOG.info("Initialized results dict.")
if not histories: LOG.info("Initialized histories dict.")
if not saved_model_paths: LOG.info("Initialized saved_model_paths dict.")
if not cl_detailed_results: LOG.info("Initialized cl_detailed_results dict.")

# --- Safely get params ---
encoder_name = globals().get('ENCODER_NAME', 'unknown_encoder')
encoder_weights = str(globals().get('ENCODER_WEIGHTS', 'unknown'))
loss_type = globals().get('LOSS_TYPE', 'unknown_loss')

# --- Define Experiment Details Dictionary ---
# Using setdefault to avoid re-definition if already exists
experiment_details_dict = globals().setdefault('experiment_details_dict', {})
# Update/set details for this run
experiment_details_dict.update({
    "encoder": encoder_name, "encoder_weights": encoder_weights, "loss_type": loss_type,
    "num_tasks": globals().get('NUM_TASKS', '?'),
    "epochs_per_task": globals().get('EPOCHS_PER_TASK', '?'),
    "epochs_baseline": globals().get('EPOCHS_BASELINE', '?'), # Keep baseline info if available
    "buffer_size": globals().get('REPLAY_BUFFER_SIZE', '?'),
    "replay_batch_size": globals().get('REPLAY_BATCH_SIZE', '?'),
    "approx_score_fraction": globals().get('UNCERTAINTY_SUBSET_FRACTION', 'N/A'),
    "save_epoch_checkpoints": globals().get('SAVE_EPOCH_CHECKPOINTS', '?'),
    "use_early_stopping": globals().get('USE_EARLY_STOPPING', '?'),
    "last_update": datetime.now().isoformat(),
    # Add flags indicating which optimizations are active
    "amp_enabled": True,
    "combined_batches": True,
    "buffer_stores_paths": True,
    "percentile_scaling": True if globals().get('NORM_PERCENTILES') else False
})


# --- Check Prerequisites ---
can_run_cl = False # Assume false initially
required_paths = ['train_sar_paths', 'train_dem_paths', 'train_mask_paths'] # Need these originals
if 'can_proceed' in globals() and can_proceed:
    if 'time_task_train_loaders' not in globals() or not time_task_train_loaders: LOG.error("CL Fail: Time-based Train loaders missing.")
    elif 'time_task_test_loaders' not in globals() or not time_task_test_loaders: LOG.error("CL Fail: Time-based Test loaders missing.")
    elif not all(p in globals() and globals()[p] is not None for p in required_paths):
        missing_paths = [p for p in required_paths if p not in globals() or globals()[p] is None]
        LOG.error(f"CL Fail: Original training paths missing: {missing_paths}. Cannot fill path buffer.")
    elif 'NUM_TASKS' not in globals() or NUM_TASKS <= 0: LOG.error("CL Fail: NUM_TASKS invalid.")
    elif len(time_task_train_loaders) != NUM_TASKS: LOG.error(f"CL Fail: Loader count ({len(time_task_train_loaders)}) != current NUM_TASKS ({NUM_TASKS}). Check Cell 13.")
    else: LOG.info(f"Preconditions for CL met (NUM_TASKS={NUM_TASKS})."); can_run_cl = True
else: LOG.error("CL Fail: Base preconditions not met.")

APPLY_REPLAY_AUG = True; LOG.info(f"Replay augmentation set to: {APPLY_REPLAY_AUG}")

# --- Loop through CL Strategies ---
if 'cl_strategies' not in globals(): cl_strategies = ['random'] # Default if not defined
for strategy in cl_strategies:
    # --- Define names and paths ---
    run_name_cl = f"cl_{strategy}_{encoder_name}W{encoder_weights}_{loss_type}_t{NUM_TASKS}e{EPOCHS_PER_TASK}_b{REPLAY_BUFFER_SIZE}"
    LOG.info(f"<<<<<<<<<<<< Attempting CL Run: {run_name_cl} >>>>>>>>>>>>")
    LOG.info("---------------------------------------------------------------------------------------------")
    cl_save_path = os.path.join(SAVED_MODELS_DIR, f"{run_name_cl}_detailed.pth")
    cl_last_save_path = cl_save_path.replace('.pth', '_last.pth')
    json_run_filename = os.path.join(SAVED_MODELS_DIR, f"{run_name_cl}_results.json") # Unique JSON path
    LOG.info(f"Checking for existing results file: {os.path.basename(json_run_filename)}")

    # --- Skip Logic (Prioritize loading valid JSON results) ---
    skip_this_run = False # Flag to skip both train and eval for this strategy
    skip_training_cl = False # Flag to skip only training
    # Default metrics placeholder
    _def_metrics = {'loss':np.nan,'f1':np.nan,'precision':np.nan,'recall':np.nan, 'mcc':np.nan, 'tp':0, 'tn':0, 'fp':0, 'fn':0, 'time': 0, 'mem_mb': 0}
    results.setdefault(run_name_cl, _def_metrics.copy()) # Ensure entry exists
    histories.setdefault(run_name_cl, {}) # Ensure entry exists
    cl_detailed_results.setdefault(run_name_cl, [[{} for _ in range(NUM_TASKS)] for _ in range(NUM_TASKS)]) # Placeholder

    if os.path.exists(json_run_filename):
        LOG.warning(f"Results JSON exists: {os.path.basename(json_run_filename)}. Attempting to load.")
        try:
             with open(json_run_filename, 'r') as f: previous_data = json.load(f)
             prev_metrics = previous_data.get("final_test_metrics", {})
             # --- Check for a key metric to validate ---
             if prev_metrics and 'f1' in prev_metrics and prev_metrics['f1'] is not None: # Check if F1 is present and not None (JSON null)
                 LOG.info(f"Valid previous results loaded from JSON for '{run_name_cl}'. Skipping run.")
                 skip_this_run = True # Skip both train and eval
                 # --- Load data into memory dictionaries ---
                 results[run_name_cl] = prev_metrics
                 histories[run_name_cl] = previous_data.get("validation_history", {})
                 cl_detailed_results[run_name_cl] = previous_data.get("detailed_cl_matrix", [[{} for _ in range(NUM_TASKS)] for _ in range(NUM_TASKS)])
                 # Try to find corresponding model path and store it
                 model_path_found = None
                 if os.path.exists(cl_save_path): model_path_found = cl_save_path
                 elif os.path.exists(cl_last_save_path): model_path_found = cl_last_save_path
                 if model_path_found:
                     saved_model_paths[run_name_cl] = model_path_found
                 else:
                     LOG.warning(f"Loaded results from JSON, but corresponding model file not found for {run_name_cl}")
             else:
                 LOG.warning("Previous JSON found but contained invalid/incomplete metrics.")
        except Exception as json_e:
             LOG.error(f"Error loading or parsing previous JSON {os.path.basename(json_run_filename)}: {json_e}.")

    # If we decided to skip everything based on JSON, continue to next strategy
    if skip_this_run:
         LOG.info(f"Skipping strategy '{strategy}' entirely due to loaded results.")
         continue # Go to the next iteration of the for strategy loop

    # Check for model file only if not skipping run based on JSON
    path_found_for_skip_training = None
    if os.path.exists(cl_save_path): path_found_for_skip_training = cl_save_path
    elif os.path.exists(cl_last_save_path): path_found_for_skip_training = cl_last_save_path

    if path_found_for_skip_training:
        LOG.warning(f"Model file exists ({os.path.basename(path_found_for_skip_training)}). JSON not found/invalid. Training skipped, evaluation needed.")
        skip_training_cl = True # Only skip training
        saved_model_paths[run_name_cl] = path_found_for_skip_training # Store path for eval
    else:
        LOG.info(f"No completed model file or valid results JSON found for '{strategy}'. Proceeding with training & evaluation.")
        skip_training_cl = False


    # --- Execute Training (if not skipped) ---
    model_out = None # Initialize model_out for the evaluation block
    # Get time/mem from results dict if loaded from JSON, otherwise use 0
    time_cl = results[run_name_cl].get('time', 0)
    peak_mem_cl = results[run_name_cl].get('mem_mb', 0)

    if can_run_cl and not skip_training_cl:
        LOG.info(f"=== Preparing CL Model ({run_name_cl}) ===")
        if not time_task_train_loaders or len(time_task_train_loaders) != NUM_TASKS: LOG.error(f"ERROR: Invalid loaders for {run_name_cl}. Skipping."); continue

        latest_epoch_checkpoint_path = find_latest_checkpoint(cl_save_path)
        if latest_epoch_checkpoint_path: LOG.warning(f"Found epoch ckpt: {os.path.basename(latest_epoch_checkpoint_path)}. Will resume.")
        else: LOG.info("No epoch ckpt found.")

        # Build fresh components for each strategy run
        cl_model = build_model(ENCODER_NAME, ENCODER_WEIGHTS, INPUT_CHANNELS, OUTPUT_CLASSES)
        if cl_model:
            optimizer_cl = create_poly_optimizer(cl_model, base_lr=LEARNING_RATE)
            replay_buffer = ReplayBuffer(max_size=REPLAY_BUFFER_SIZE) # Fresh Path buffer
            if optimizer_cl:
                scheduler_cl = None;
                try: total_batches_all_tasks = sum(EPOCHS_PER_TASK * len(loader) for loader in time_task_train_loaders if loader and len(loader) > 0); assert total_batches_all_tasks > 0; scheduler_cl = optim.lr_scheduler.PolynomialLR(optimizer_cl, total_iters=total_batches_all_tasks, power=0.9); LOG.info(f"CL scheduler iters={total_batches_all_tasks}")
                except Exception as e: LOG.error(f"ERROR CL scheduler: {e}");
                if scheduler_cl:
                    # --- Fallback Weight Loading (with Debug Print) ---
                    if latest_epoch_checkpoint_path is None:
                        LOG.info("Checking fallback weights..."); weights_path_to_load = None
                        if os.path.exists(cl_save_path): weights_path_to_load = cl_save_path
                        elif os.path.exists(cl_last_save_path): weights_path_to_load = cl_last_save_path

                        if weights_path_to_load: # Check if a path was found
                            LOG.warning(f"Attempting FALLBACK load: {os.path.basename(weights_path_to_load)}")
                            try:
                                LOG.info(f"DEBUG: Inside try block. Value of weights_path_to_load = {repr(weights_path_to_load)}")
                                if weights_path_to_load is None: LOG.error("CRITICAL DEBUG: weights_path_to_load is None JUST BEFORE torch.load!")
                                # Use weights_only=True
                                state_dict = torch.load(weights_path_to_load, map_location='cpu', weights_only=False)
                                if 'model_state_dict' in state_dict: state_dict = state_dict['model_state_dict']
                                cl_model.load_state_dict(state_dict, strict=False); LOG.info("Fallback weights loaded.")
                                cl_model.to(DEVICE)
                            except Exception as fb_err:
                                LOG.error(f"--- DETAILED FALLBACK LOAD TRACEBACK ---")
                                LOG.error(traceback.format_exc())
                                LOG.error(f"--- END TRACEBACK ---")
                                LOG.error(f"Fallback load error (logged fb_err object): {fb_err}")
                                cl_model.to(DEVICE) # Ensure model is on device even if load failed
                        else:
                            LOG.info("No fallback model found.")
                            cl_model.to(DEVICE) # Ensure on device if no fallback found
                    else:
                         cl_model.to(DEVICE) # Ensure on device if resuming from epoch ckpt

                    # Reset memory stats before training starts/resumes
                    if DEVICE.type == 'cuda':
                         try: torch.cuda.reset_peak_memory_stats(DEVICE)
                         except Exception as e_mem: LOG.warning(f"Could not reset cuda mem stats: {e_mem}")

                    LOG.info(f"Calling train_cl_detailed_eval for {run_name_cl}...")
                    # Ensure the original path lists are available globally here
                    if not all(p in globals() for p in required_paths):
                         LOG.error(f"Cannot start training for {run_name_cl}: Original train paths {required_paths} not found globally.")
                         # Clean up potentially created model/optimizer here before continuing
                         del cl_model, optimizer_cl, replay_buffer, scheduler_cl; gc.collect(); torch.cuda.empty_cache() if DEVICE.type=='cuda' else None;
                         continue # Skip this strategy

                    # --- Call Training Function ---
                    # This function performs the actual training epochs and tasks
                    model_out, val_history_cl, time_cl, eval_matrix_dict = train_cl_detailed_eval(
                        cl_model, time_task_train_loaders, time_task_test_loaders, val_loader, criterion,
                        optimizer_cl, scheduler_cl, EPOCHS_PER_TASK, replay_buffer, REPLAY_BATCH_SIZE,
                        strategy,
                        original_train_paths=(train_sar_paths, train_dem_paths, train_mask_paths), # Pass paths
                        device=DEVICE, model_save_path=cl_save_path,
                        num_tasks=NUM_TASKS, augment_replay=APPLY_REPLAY_AUG, patience=PATIENCE,
                        checkpoint_path=latest_epoch_checkpoint_path
                    )

                    # --- Store results from training run in memory ---
                    cl_detailed_results[run_name_cl] = eval_matrix_dict
                    histories[run_name_cl] = val_history_cl
                    peak_mem_cl = torch.cuda.max_memory_allocated(DEVICE)/(1024**2) if DEVICE.type == 'cuda' else 0.0
                    # Update saved model path map based on final state
                    current_save_path = None
                    if os.path.exists(cl_save_path): current_save_path = cl_save_path
                    elif os.path.exists(cl_last_save_path): current_save_path = cl_last_save_path
                    if current_save_path: saved_model_paths[run_name_cl] = current_save_path

                # --- Handle scheduler failure ---
                else:
                     LOG.error(f"Skip CL '{strategy}': scheduler fail.")
                     model_out = None # Ensure model_out is None if training failed

            # --- Handle optimizer failure ---
            else:
                 LOG.error(f"Skip CL '{strategy}': optimizer fail.")
                 model_out = None

        # --- Handle model build failure ---
        else:
             LOG.error(f"Skip CL '{strategy}': model fail.")
             model_out = None

    # --- Handle preconditions fail ---
    elif not can_run_cl:
        LOG.error(f"Skip CL ({strategy}): preconditions fail.")
        continue # Skip to next strategy

    # --- Execute Evaluation (if training wasn't skipped, OR if only training was skipped but model file exists) ---
    evaluation_performed_this_run = False
    # Determine if we need to evaluate:
    # Need model output from training OR a path to a saved model for this run.
    model_available_for_eval = (model_out is not None) or (saved_model_paths.get(run_name_cl) and os.path.exists(saved_model_paths[run_name_cl]))
    should_evaluate = model_available_for_eval and can_run_cl and test_loader is not None

    if should_evaluate:
        LOG.info(f"--- Evaluating final ({strategy}) on main test set ---")
        eval_model_cl = None
        try:
            # Get the model: either the output of training or load from file
            if model_out: # Use model from training if it exists
                 eval_model_cl = model_out
                 LOG.info("Evaluating model trained in this session.")
            elif saved_model_paths.get(run_name_cl) and os.path.exists(saved_model_paths[run_name_cl]):
                 model_file_to_load = saved_model_paths[run_name_cl]
                 LOG.info(f"Loading existing CL model from: {os.path.basename(model_file_to_load)}")
                 eval_model_cl = build_model(ENCODER_NAME, ENCODER_WEIGHTS, INPUT_CHANNELS, OUTPUT_CLASSES)
                 if eval_model_cl:
                      state_dict = torch.load(model_file_to_load, map_location=DEVICE, weights_only=False) # Set weights_only
                      if 'model_state_dict' in state_dict: state_dict = state_dict['model_state_dict']
                      eval_model_cl.load_state_dict(state_dict); eval_model_cl.to(DEVICE); LOG.info("Loaded weights.")
                 else: LOG.error(f"Failed to build model for evaluation: {run_name_cl}")
            else:
                 LOG.error(f"Cannot evaluate CL model: No model from training and no valid saved path found for {run_name_cl}.")

            # Perform evaluation if model is ready
            if eval_model_cl:
                 eval_model_cl.eval() # Set to eval mode
                 eval_output_final = evaluate_model(eval_model_cl, test_loader, criterion, DEVICE)
                 loss, dice, iou, f1, prec, rec, mcc, tp_final, tn_final, fp_final, fn_final = eval_output_final
                 # --- Update results dictionary for this run ---
                 current_run_summary = {
                     'loss': loss, 'dice': dice, 'iou': iou, 'f1': f1, 'precision': prec, 'recall': rec, 'mcc': mcc,
                     'tp': tp_final, 'tn': tn_final, 'fp': fp_final, 'fn': fn_final,
                     'time': time_cl, 'mem_mb': peak_mem_cl # Use time/mem calculated during training run
                 }
                 results[run_name_cl] = current_run_summary # Store/update in global dict
                 # --- Log results ---
                 LOG.info("---------------------------------------------------------------------------------------------")
                 LOG.info(f"Final CL ({strategy}) Main TEST >> L:{loss:.4f}, F1:{f1:.4f}, P:{prec:.4f}, R:{rec:.4f}, MCC:{mcc:.4f}")
                 LOG.info(f"Confusion Matrix (Final Test): TP={tp_final:.0f}, TN={tn_final:.0f}, FP={fp_final:.0f}, FN={fn_final:.0f}")
                 LOG.info("---------------------------------------------------------------------------------------------")
                 evaluation_performed_this_run = True
            else:
                 LOG.warning(f"Skipping evaluation for {run_name_cl} as model could not be loaded or trained.")
                 # results dict already has placeholders from setdefault earlier

        except Exception as e_cl_eval:
             LOG.error(f"ERROR CL Main TEST eval for {run_name_cl}: {e_cl_eval}", exc_info=True)
             # Update results dict with defaults on error, but keep time/mem if available
             results[run_name_cl].update({k:v for k,v in _def_metrics.items() if k not in ['time', 'mem_mb']})

        # Cleanup loaded eval model if it wasn't from training output
        if eval_model_cl and not model_out: del eval_model_cl

    elif not model_available_for_eval:
         LOG.warning(f"Cannot evaluate {run_name_cl}: No trained model output and no saved model path found.")
    elif not test_loader:
         LOG.warning(f"Skipping final main test eval for {run_name_cl}: test_loader None.")
    elif not can_run_cl:
         LOG.warning(f"Skipping final main test eval for {run_name_cl}: Preconditions not met.")


    # --- Save results ---
    if not skip_this_run:
         experiment_details_dict["last_update"] = datetime.now().isoformat()
         save_metrics_to_json(
              filename=json_run_filename, # Unique filename for this run
              run_name=run_name_cl,
              run_result_summary=results.get(run_name_cl),
              run_cl_detail_matrix=cl_detailed_results.get(run_name_cl),
              run_history=histories.get(run_name_cl),
              experiment_details=experiment_details_dict
         )
    else:
         LOG.info(f"Skipping JSON save for {run_name_cl} as run was skipped based on existing JSON.")

    # --- Cleanup potentially large objects from training/evaluation ---
    if 'model_out' in locals() and model_out: del model_out
    if 'cl_model' in locals() and cl_model: del cl_model
    if 'optimizer_cl' in locals() and optimizer_cl: del optimizer_cl
    if 'scheduler_cl' in locals() and scheduler_cl: del scheduler_cl
    if 'replay_buffer' in locals() and replay_buffer: del replay_buffer
    if 'val_history_cl' in locals() and val_history_cl: del val_history_cl
    if 'eval_matrix_dict' in locals() and eval_matrix_dict: del eval_matrix_dict
    # Explicitly delete eval_model_cl if it exists from loading
    if 'eval_model_cl' in locals() and eval_model_cl: del eval_model_cl
    gc.collect()
    if DEVICE.type == 'cuda': torch.cuda.empty_cache();
    LOG.info(f"{strategy} resources released.")

    LOG.info(f"Finished processing run for '{run_name_cl}'")
    LOG.info("---------------------------------------------------------------------------------------------")
# --- End Strategy Loop ---

LOG.info("--- CL Execution Loop Complete ---")

# Cell 15: Execute CL Training (REPLAY_BUFFER_SIZE = 100 ...CONTINUED FROM ABOVE)
# LOADED CHECKPOINT FOR CL_LEAST_CONFIDENCE

# --- Init dictionaries if they don't exist ---
results = globals().setdefault('results', {}); histories = globals().setdefault('histories', {}); saved_model_paths = globals().setdefault('saved_model_paths', {}); cl_detailed_results = globals().setdefault('cl_detailed_results', {})
if not results: LOG.info("Initialized results dict.")
if not histories: LOG.info("Initialized histories dict.")
if not saved_model_paths: LOG.info("Initialized saved_model_paths dict.")
if not cl_detailed_results: LOG.info("Initialized cl_detailed_results dict.")

# --- Safely get params ---
encoder_name = globals().get('ENCODER_NAME', 'unknown_encoder')
encoder_weights = str(globals().get('ENCODER_WEIGHTS', 'unknown'))
loss_type = globals().get('LOSS_TYPE', 'unknown_loss')

# --- Define Experiment Details Dictionary ---
# Using setdefault to avoid re-definition if already exists
experiment_details_dict = globals().setdefault('experiment_details_dict', {})
# Update/set details for this run
experiment_details_dict.update({
    "encoder": encoder_name, "encoder_weights": encoder_weights, "loss_type": loss_type,
    "num_tasks": globals().get('NUM_TASKS', '?'),
    "epochs_per_task": globals().get('EPOCHS_PER_TASK', '?'),
    "epochs_baseline": globals().get('EPOCHS_BASELINE', '?'), # Keep baseline info if available
    "buffer_size": globals().get('REPLAY_BUFFER_SIZE', '?'),
    "replay_batch_size": globals().get('REPLAY_BATCH_SIZE', '?'),
    "approx_score_fraction": globals().get('UNCERTAINTY_SUBSET_FRACTION', 'N/A'),
    "save_epoch_checkpoints": globals().get('SAVE_EPOCH_CHECKPOINTS', '?'),
    "use_early_stopping": globals().get('USE_EARLY_STOPPING', '?'),
    "last_update": datetime.now().isoformat(),
    # Add flags indicating which optimizations are active
    "amp_enabled": True,
    "combined_batches": True,
    "buffer_stores_paths": True,
    "percentile_scaling": True if globals().get('NORM_PERCENTILES') else False
})


# --- Check Prerequisites ---
can_run_cl = False # Assume false initially
required_paths = ['train_sar_paths', 'train_dem_paths', 'train_mask_paths'] # Need these originals
if 'can_proceed' in globals() and can_proceed:
    if 'time_task_train_loaders' not in globals() or not time_task_train_loaders: LOG.error("CL Fail: Time-based Train loaders missing.")
    elif 'time_task_test_loaders' not in globals() or not time_task_test_loaders: LOG.error("CL Fail: Time-based Test loaders missing.")
    elif not all(p in globals() and globals()[p] is not None for p in required_paths):
        missing_paths = [p for p in required_paths if p not in globals() or globals()[p] is None]
        LOG.error(f"CL Fail: Original training paths missing: {missing_paths}. Cannot fill path buffer.")
    elif 'NUM_TASKS' not in globals() or NUM_TASKS <= 0: LOG.error("CL Fail: NUM_TASKS invalid.")
    elif len(time_task_train_loaders) != NUM_TASKS: LOG.error(f"CL Fail: Loader count ({len(time_task_train_loaders)}) != current NUM_TASKS ({NUM_TASKS}). Check Cell 13.")
    else: LOG.info(f"Preconditions for CL met (NUM_TASKS={NUM_TASKS})."); can_run_cl = True
else: LOG.error("CL Fail: Base preconditions not met.")

APPLY_REPLAY_AUG = True; LOG.info(f"Replay augmentation set to: {APPLY_REPLAY_AUG}")

# --- Loop through CL Strategies ---
if 'cl_strategies' not in globals(): cl_strategies = ['random'] # Default if not defined
for strategy in cl_strategies:
    # --- Define names and paths ---
    run_name_cl = f"cl_{strategy}_{encoder_name}W{encoder_weights}_{loss_type}_t{NUM_TASKS}e{EPOCHS_PER_TASK}_b{REPLAY_BUFFER_SIZE}"
    LOG.info(f"<<<<<<<<<<<< Attempting CL Run: {run_name_cl} >>>>>>>>>>>>")
    LOG.info("---------------------------------------------------------------------------------------------")
    cl_save_path = os.path.join(SAVED_MODELS_DIR, f"{run_name_cl}_detailed.pth")
    cl_last_save_path = cl_save_path.replace('.pth', '_last.pth')
    json_run_filename = os.path.join(SAVED_MODELS_DIR, f"{run_name_cl}_results.json") # Unique JSON path
    LOG.info(f"Checking for existing results file: {os.path.basename(json_run_filename)}")

    # --- Skip Logic (Prioritize loading valid JSON results) ---
    skip_this_run = False # Flag to skip both train and eval for this strategy
    skip_training_cl = False # Flag to skip only training
    # Default metrics placeholder
    _def_metrics = {'loss':np.nan,'f1':np.nan,'precision':np.nan,'recall':np.nan, 'mcc':np.nan, 'tp':0, 'tn':0, 'fp':0, 'fn':0, 'time': 0, 'mem_mb': 0}
    results.setdefault(run_name_cl, _def_metrics.copy()) # Ensure entry exists
    histories.setdefault(run_name_cl, {}) # Ensure entry exists
    cl_detailed_results.setdefault(run_name_cl, [[{} for _ in range(NUM_TASKS)] for _ in range(NUM_TASKS)]) # Placeholder

    if os.path.exists(json_run_filename):
        LOG.warning(f"Results JSON exists: {os.path.basename(json_run_filename)}. Attempting to load.")
        try:
             with open(json_run_filename, 'r') as f: previous_data = json.load(f)
             prev_metrics = previous_data.get("final_test_metrics", {})
             # --- Check for a key metric to validate ---
             if prev_metrics and 'f1' in prev_metrics and prev_metrics['f1'] is not None: # Check if F1 is present and not None (JSON null)
                 LOG.info(f"Valid previous results loaded from JSON for '{run_name_cl}'. Skipping run.")
                 skip_this_run = True # Skip both train and eval
                 # --- Load data into memory dictionaries ---
                 results[run_name_cl] = prev_metrics
                 histories[run_name_cl] = previous_data.get("validation_history", {})
                 cl_detailed_results[run_name_cl] = previous_data.get("detailed_cl_matrix", [[{} for _ in range(NUM_TASKS)] for _ in range(NUM_TASKS)])
                 # Try to find corresponding model path and store it
                 model_path_found = None
                 if os.path.exists(cl_save_path): model_path_found = cl_save_path
                 elif os.path.exists(cl_last_save_path): model_path_found = cl_last_save_path
                 if model_path_found:
                     saved_model_paths[run_name_cl] = model_path_found
                 else:
                     LOG.warning(f"Loaded results from JSON, but corresponding model file not found for {run_name_cl}")
             else:
                 LOG.warning("Previous JSON found but contained invalid/incomplete metrics.")
        except Exception as json_e:
             LOG.error(f"Error loading or parsing previous JSON {os.path.basename(json_run_filename)}: {json_e}.")

    # If we decided to skip everything based on JSON, continue to next strategy
    if skip_this_run:
         LOG.info(f"Skipping strategy '{strategy}' entirely due to loaded results.")
         continue # Go to the next iteration of the for strategy loop

    # Check for model file only if not skipping run based on JSON
    path_found_for_skip_training = None
    if os.path.exists(cl_save_path): path_found_for_skip_training = cl_save_path
    elif os.path.exists(cl_last_save_path): path_found_for_skip_training = cl_last_save_path

    if path_found_for_skip_training:
        LOG.warning(f"Model file exists ({os.path.basename(path_found_for_skip_training)}). JSON not found/invalid. Training skipped, evaluation needed.")
        skip_training_cl = True # Only skip training
        saved_model_paths[run_name_cl] = path_found_for_skip_training # Store path for eval
    else:
        LOG.info(f"No completed model file or valid results JSON found for '{strategy}'. Proceeding with training & evaluation.")
        skip_training_cl = False


    # --- Execute Training (if not skipped) ---
    model_out = None # Initialize model_out for the evaluation block
    # Get time/mem from results dict if loaded from JSON, otherwise use 0
    time_cl = results[run_name_cl].get('time', 0)
    peak_mem_cl = results[run_name_cl].get('mem_mb', 0)

    if can_run_cl and not skip_training_cl:
        LOG.info(f"=== Preparing CL Model ({run_name_cl}) ===")
        if not time_task_train_loaders or len(time_task_train_loaders) != NUM_TASKS: LOG.error(f"ERROR: Invalid loaders for {run_name_cl}. Skipping."); continue

        latest_epoch_checkpoint_path = find_latest_checkpoint(cl_save_path)
        if latest_epoch_checkpoint_path: LOG.warning(f"Found epoch ckpt: {os.path.basename(latest_epoch_checkpoint_path)}. Will resume.")
        else: LOG.info("No epoch ckpt found.")

        # Build fresh components for each strategy run
        cl_model = build_model(ENCODER_NAME, ENCODER_WEIGHTS, INPUT_CHANNELS, OUTPUT_CLASSES)
        if cl_model:
            optimizer_cl = create_poly_optimizer(cl_model, base_lr=LEARNING_RATE)
            replay_buffer = ReplayBuffer(max_size=REPLAY_BUFFER_SIZE) # Fresh Path buffer
            if optimizer_cl:
                scheduler_cl = None;
                try: total_batches_all_tasks = sum(EPOCHS_PER_TASK * len(loader) for loader in time_task_train_loaders if loader and len(loader) > 0); assert total_batches_all_tasks > 0; scheduler_cl = optim.lr_scheduler.PolynomialLR(optimizer_cl, total_iters=total_batches_all_tasks, power=0.9); LOG.info(f"CL scheduler iters={total_batches_all_tasks}")
                except Exception as e: LOG.error(f"ERROR CL scheduler: {e}");
                if scheduler_cl:
                    # --- Fallback Weight Loading (with Debug Print) ---
                    if latest_epoch_checkpoint_path is None:
                        LOG.info("Checking fallback weights..."); weights_path_to_load = None
                        if os.path.exists(cl_save_path): weights_path_to_load = cl_save_path
                        elif os.path.exists(cl_last_save_path): weights_path_to_load = cl_last_save_path

                        if weights_path_to_load: # Check if a path was found
                            LOG.warning(f"Attempting FALLBACK load: {os.path.basename(weights_path_to_load)}")
                            try:
                                LOG.info(f"DEBUG: Inside try block. Value of weights_path_to_load = {repr(weights_path_to_load)}")
                                if weights_path_to_load is None: LOG.error("CRITICAL DEBUG: weights_path_to_load is None JUST BEFORE torch.load!")
                                # Use weights_only=True
                                state_dict = torch.load(weights_path_to_load, map_location='cpu', weights_only=False)
                                if 'model_state_dict' in state_dict: state_dict = state_dict['model_state_dict']
                                cl_model.load_state_dict(state_dict, strict=False); LOG.info("Fallback weights loaded.")
                                cl_model.to(DEVICE)
                            except Exception as fb_err:
                                LOG.error(f"--- DETAILED FALLBACK LOAD TRACEBACK ---")
                                LOG.error(traceback.format_exc())
                                LOG.error(f"--- END TRACEBACK ---")
                                LOG.error(f"Fallback load error (logged fb_err object): {fb_err}")
                                cl_model.to(DEVICE) # Ensure model is on device even if load failed
                        else:
                            LOG.info("No fallback model found.")
                            cl_model.to(DEVICE) # Ensure on device if no fallback found
                    else:
                         cl_model.to(DEVICE) # Ensure on device if resuming from epoch ckpt

                    # Reset memory stats before training starts/resumes
                    if DEVICE.type == 'cuda':
                         try: torch.cuda.reset_peak_memory_stats(DEVICE)
                         except Exception as e_mem: LOG.warning(f"Could not reset cuda mem stats: {e_mem}")

                    LOG.info(f"Calling train_cl_detailed_eval for {run_name_cl}...")
                    # Ensure the original path lists are available globally here
                    if not all(p in globals() for p in required_paths):
                         LOG.error(f"Cannot start training for {run_name_cl}: Original train paths {required_paths} not found globally.")
                         # Clean up potentially created model/optimizer here before continuing
                         del cl_model, optimizer_cl, replay_buffer, scheduler_cl; gc.collect(); torch.cuda.empty_cache() if DEVICE.type=='cuda' else None;
                         continue # Skip this strategy

                    # --- Call Training Function ---
                    # This function performs the actual training epochs and tasks
                    model_out, val_history_cl, time_cl, eval_matrix_dict = train_cl_detailed_eval(
                        cl_model, time_task_train_loaders, time_task_test_loaders, val_loader, criterion,
                        optimizer_cl, scheduler_cl, EPOCHS_PER_TASK, replay_buffer, REPLAY_BATCH_SIZE,
                        strategy,
                        original_train_paths=(train_sar_paths, train_dem_paths, train_mask_paths), # Pass paths
                        device=DEVICE, model_save_path=cl_save_path,
                        num_tasks=NUM_TASKS, augment_replay=APPLY_REPLAY_AUG, patience=PATIENCE,
                        checkpoint_path=latest_epoch_checkpoint_path
                    )

                    # --- Store results from training run in memory ---
                    cl_detailed_results[run_name_cl] = eval_matrix_dict
                    histories[run_name_cl] = val_history_cl
                    peak_mem_cl = torch.cuda.max_memory_allocated(DEVICE)/(1024**2) if DEVICE.type == 'cuda' else 0.0
                    # Update saved model path map based on final state
                    current_save_path = None
                    if os.path.exists(cl_save_path): current_save_path = cl_save_path
                    elif os.path.exists(cl_last_save_path): current_save_path = cl_last_save_path
                    if current_save_path: saved_model_paths[run_name_cl] = current_save_path

                # --- Handle scheduler failure ---
                else:
                     LOG.error(f"Skip CL '{strategy}': scheduler fail.")
                     model_out = None # Ensure model_out is None if training failed

            # --- Handle optimizer failure ---
            else:
                 LOG.error(f"Skip CL '{strategy}': optimizer fail.")
                 model_out = None

        # --- Handle model build failure ---
        else:
             LOG.error(f"Skip CL '{strategy}': model fail.")
             model_out = None

    # --- Handle preconditions fail ---
    elif not can_run_cl:
        LOG.error(f"Skip CL ({strategy}): preconditions fail.")
        continue # Skip to next strategy

    # --- Execute Evaluation (if training wasn't skipped, OR if only training was skipped but model file exists) ---
    evaluation_performed_this_run = False
    # Determine if we need to evaluate:
    # Need model output from training OR a path to a saved model for this run.
    model_available_for_eval = (model_out is not None) or (saved_model_paths.get(run_name_cl) and os.path.exists(saved_model_paths[run_name_cl]))
    should_evaluate = model_available_for_eval and can_run_cl and test_loader is not None

    if should_evaluate:
        LOG.info(f"--- Evaluating final ({strategy}) on main test set ---")
        eval_model_cl = None
        try:
            # Get the model: either the output of training or load from file
            if model_out: # Use model from training if it exists
                 eval_model_cl = model_out
                 LOG.info("Evaluating model trained in this session.")
            elif saved_model_paths.get(run_name_cl) and os.path.exists(saved_model_paths[run_name_cl]):
                 model_file_to_load = saved_model_paths[run_name_cl]
                 LOG.info(f"Loading existing CL model from: {os.path.basename(model_file_to_load)}")
                 eval_model_cl = build_model(ENCODER_NAME, ENCODER_WEIGHTS, INPUT_CHANNELS, OUTPUT_CLASSES)
                 if eval_model_cl:
                      state_dict = torch.load(model_file_to_load, map_location=DEVICE, weights_only=False) # Set weights_only
                      if 'model_state_dict' in state_dict: state_dict = state_dict['model_state_dict']
                      eval_model_cl.load_state_dict(state_dict); eval_model_cl.to(DEVICE); LOG.info("Loaded weights.")
                 else: LOG.error(f"Failed to build model for evaluation: {run_name_cl}")
            else:
                 LOG.error(f"Cannot evaluate CL model: No model from training and no valid saved path found for {run_name_cl}.")

            # Perform evaluation if model is ready
            if eval_model_cl:
                 eval_model_cl.eval() # Set to eval mode
                 eval_output_final = evaluate_model(eval_model_cl, test_loader, criterion, DEVICE)
                 loss, dice, iou, f1, prec, rec, mcc, tp_final, tn_final, fp_final, fn_final = eval_output_final
                 # --- Update results dictionary for this run ---
                 current_run_summary = {
                     'loss': loss, 'dice': dice, 'iou': iou, 'f1': f1, 'precision': prec, 'recall': rec, 'mcc': mcc,
                     'tp': tp_final, 'tn': tn_final, 'fp': fp_final, 'fn': fn_final,
                     'time': time_cl, 'mem_mb': peak_mem_cl # Use time/mem calculated during training run
                 }
                 results[run_name_cl] = current_run_summary # Store/update in global dict
                 # --- Log results ---
                 LOG.info("---------------------------------------------------------------------------------------------")
                 LOG.info(f"Final CL ({strategy}) Main TEST >> L:{loss:.4f}, F1:{f1:.4f}, P:{prec:.4f}, R:{rec:.4f}, MCC:{mcc:.4f}")
                 LOG.info(f"Confusion Matrix (Final Test): TP={tp_final:.0f}, TN={tn_final:.0f}, FP={fp_final:.0f}, FN={fn_final:.0f}")
                 LOG.info("---------------------------------------------------------------------------------------------")
                 evaluation_performed_this_run = True
            else:
                 LOG.warning(f"Skipping evaluation for {run_name_cl} as model could not be loaded or trained.")
                 # results dict already has placeholders from setdefault earlier

        except Exception as e_cl_eval:
             LOG.error(f"ERROR CL Main TEST eval for {run_name_cl}: {e_cl_eval}", exc_info=True)
             # Update results dict with defaults on error, but keep time/mem if available
             results[run_name_cl].update({k:v for k,v in _def_metrics.items() if k not in ['time', 'mem_mb']})

        # Cleanup loaded eval model if it wasn't from training output
        if eval_model_cl and not model_out: del eval_model_cl

    elif not model_available_for_eval:
         LOG.warning(f"Cannot evaluate {run_name_cl}: No trained model output and no saved model path found.")
    elif not test_loader:
         LOG.warning(f"Skipping final main test eval for {run_name_cl}: test_loader None.")
    elif not can_run_cl:
         LOG.warning(f"Skipping final main test eval for {run_name_cl}: Preconditions not met.")


    # --- Save results ---
    if not skip_this_run:
         experiment_details_dict["last_update"] = datetime.now().isoformat()
         save_metrics_to_json(
              filename=json_run_filename, # Unique filename for this run
              run_name=run_name_cl,
              run_result_summary=results.get(run_name_cl),
              run_cl_detail_matrix=cl_detailed_results.get(run_name_cl),
              run_history=histories.get(run_name_cl),
              experiment_details=experiment_details_dict
         )
    else:
         LOG.info(f"Skipping JSON save for {run_name_cl} as run was skipped based on existing JSON.")

    # --- Cleanup potentially large objects from training/evaluation ---
    if 'model_out' in locals() and model_out: del model_out
    if 'cl_model' in locals() and cl_model: del cl_model
    if 'optimizer_cl' in locals() and optimizer_cl: del optimizer_cl
    if 'scheduler_cl' in locals() and scheduler_cl: del scheduler_cl
    if 'replay_buffer' in locals() and replay_buffer: del replay_buffer
    if 'val_history_cl' in locals() and val_history_cl: del val_history_cl
    if 'eval_matrix_dict' in locals() and eval_matrix_dict: del eval_matrix_dict
    # Explicitly delete eval_model_cl if it exists from loading
    if 'eval_model_cl' in locals() and eval_model_cl: del eval_model_cl
    gc.collect()
    if DEVICE.type == 'cuda': torch.cuda.empty_cache();
    LOG.info(f"{strategy} resources released.")

    LOG.info(f"Finished processing run for '{run_name_cl}'")
    LOG.info("---------------------------------------------------------------------------------------------")
# --- End Strategy Loop ---

LOG.info("--- CL Execution Loop Complete ---")

"""## CL Model Training with Buffer Size = 50"""

# Cell 15: Execute CL Training (REPLAY_BUFFER_SIZE = 50)


# --- Init dictionaries if they don't exist ---
results = globals().setdefault('results', {}); histories = globals().setdefault('histories', {}); saved_model_paths = globals().setdefault('saved_model_paths', {}); cl_detailed_results = globals().setdefault('cl_detailed_results', {})
if not results: LOG.info("Initialized results dict.")
if not histories: LOG.info("Initialized histories dict.")
if not saved_model_paths: LOG.info("Initialized saved_model_paths dict.")
if not cl_detailed_results: LOG.info("Initialized cl_detailed_results dict.")

# --- Safely get params ---
encoder_name = globals().get('ENCODER_NAME', 'unknown_encoder')
encoder_weights = str(globals().get('ENCODER_WEIGHTS', 'unknown'))
loss_type = globals().get('LOSS_TYPE', 'unknown_loss')

# --- Define Experiment Details Dictionary ---
# Using setdefault to avoid re-definition if already exists
experiment_details_dict = globals().setdefault('experiment_details_dict', {})
# Update/set details for this run
experiment_details_dict.update({
    "encoder": encoder_name, "encoder_weights": encoder_weights, "loss_type": loss_type,
    "num_tasks": globals().get('NUM_TASKS', '?'),
    "epochs_per_task": globals().get('EPOCHS_PER_TASK', '?'),
    "epochs_baseline": globals().get('EPOCHS_BASELINE', '?'), # Keep baseline info if available
    "buffer_size": globals().get('REPLAY_BUFFER_SIZE', '?'),
    "replay_batch_size": globals().get('REPLAY_BATCH_SIZE', '?'),
    "approx_score_fraction": globals().get('UNCERTAINTY_SUBSET_FRACTION', 'N/A'),
    "save_epoch_checkpoints": globals().get('SAVE_EPOCH_CHECKPOINTS', '?'),
    "use_early_stopping": globals().get('USE_EARLY_STOPPING', '?'),
    "last_update": datetime.now().isoformat(),
    # Add flags indicating which optimizations are active
    "amp_enabled": True,
    "combined_batches": True,
    "buffer_stores_paths": True,
    "percentile_scaling": True if globals().get('NORM_PERCENTILES') else False
})


# --- Check Prerequisites ---
can_run_cl = False # Assume false initially
required_paths = ['train_sar_paths', 'train_dem_paths', 'train_mask_paths'] # Need these originals
if 'can_proceed' in globals() and can_proceed:
    if 'time_task_train_loaders' not in globals() or not time_task_train_loaders: LOG.error("CL Fail: Time-based Train loaders missing.")
    elif 'time_task_test_loaders' not in globals() or not time_task_test_loaders: LOG.error("CL Fail: Time-based Test loaders missing.")
    elif not all(p in globals() and globals()[p] is not None for p in required_paths):
        missing_paths = [p for p in required_paths if p not in globals() or globals()[p] is None]
        LOG.error(f"CL Fail: Original training paths missing: {missing_paths}. Cannot fill path buffer.")
    elif 'NUM_TASKS' not in globals() or NUM_TASKS <= 0: LOG.error("CL Fail: NUM_TASKS invalid.")
    elif len(time_task_train_loaders) != NUM_TASKS: LOG.error(f"CL Fail: Loader count ({len(time_task_train_loaders)}) != current NUM_TASKS ({NUM_TASKS}). Check Cell 13.")
    else: LOG.info(f"Preconditions for CL met (NUM_TASKS={NUM_TASKS})."); can_run_cl = True
else: LOG.error("CL Fail: Base preconditions not met.")

APPLY_REPLAY_AUG = True; LOG.info(f"Replay augmentation set to: {APPLY_REPLAY_AUG}")

# --- Loop through CL Strategies ---
if 'cl_strategies' not in globals(): cl_strategies = ['random'] # Default if not defined
for strategy in cl_strategies:
    # --- Define names and paths ---
    run_name_cl = f"cl_{strategy}_{encoder_name}W{encoder_weights}_{loss_type}_t{NUM_TASKS}e{EPOCHS_PER_TASK}_b{REPLAY_BUFFER_SIZE}"
    LOG.info(f"<<<<<<<<<<<< Attempting CL Run: {run_name_cl} >>>>>>>>>>>>")
    LOG.info("---------------------------------------------------------------------------------------------")
    cl_save_path = os.path.join(SAVED_MODELS_DIR, f"{run_name_cl}_detailed.pth")
    cl_last_save_path = cl_save_path.replace('.pth', '_last.pth')
    json_run_filename = os.path.join(SAVED_MODELS_DIR, f"{run_name_cl}_results.json") # Unique JSON path
    LOG.info(f"Checking for existing results file: {os.path.basename(json_run_filename)}")

    # --- Skip Logic (Prioritize loading valid JSON results) ---
    skip_this_run = False # Flag to skip both train and eval for this strategy
    skip_training_cl = False # Flag to skip only training
    # Default metrics placeholder
    _def_metrics = {'loss':np.nan,'f1':np.nan,'precision':np.nan,'recall':np.nan, 'mcc':np.nan, 'tp':0, 'tn':0, 'fp':0, 'fn':0, 'time': 0, 'mem_mb': 0}
    results.setdefault(run_name_cl, _def_metrics.copy()) # Ensure entry exists
    histories.setdefault(run_name_cl, {}) # Ensure entry exists
    cl_detailed_results.setdefault(run_name_cl, [[{} for _ in range(NUM_TASKS)] for _ in range(NUM_TASKS)]) # Placeholder

    if os.path.exists(json_run_filename):
        LOG.warning(f"Results JSON exists: {os.path.basename(json_run_filename)}. Attempting to load.")
        try:
             with open(json_run_filename, 'r') as f: previous_data = json.load(f)
             prev_metrics = previous_data.get("final_test_metrics", {})
             # --- Check for a key metric to validate ---
             if prev_metrics and 'f1' in prev_metrics and prev_metrics['f1'] is not None: # Check if F1 is present and not None (JSON null)
                 LOG.info(f"Valid previous results loaded from JSON for '{run_name_cl}'. Skipping run.")
                 skip_this_run = True # Skip both train and eval
                 # --- Load data into memory dictionaries ---
                 results[run_name_cl] = prev_metrics
                 histories[run_name_cl] = previous_data.get("validation_history", {})
                 cl_detailed_results[run_name_cl] = previous_data.get("detailed_cl_matrix", [[{} for _ in range(NUM_TASKS)] for _ in range(NUM_TASKS)])
                 # Try to find corresponding model path and store it
                 model_path_found = None
                 if os.path.exists(cl_save_path): model_path_found = cl_save_path
                 elif os.path.exists(cl_last_save_path): model_path_found = cl_last_save_path
                 if model_path_found:
                     saved_model_paths[run_name_cl] = model_path_found
                 else:
                     LOG.warning(f"Loaded results from JSON, but corresponding model file not found for {run_name_cl}")
             else:
                 LOG.warning("Previous JSON found but contained invalid/incomplete metrics.")
        except Exception as json_e:
             LOG.error(f"Error loading or parsing previous JSON {os.path.basename(json_run_filename)}: {json_e}.")

    # If we decided to skip everything based on JSON, continue to next strategy
    if skip_this_run:
         LOG.info(f"Skipping strategy '{strategy}' entirely due to loaded results.")
         continue # Go to the next iteration of the for strategy loop

    # Check for model file only if not skipping run based on JSON
    path_found_for_skip_training = None
    if os.path.exists(cl_save_path): path_found_for_skip_training = cl_save_path
    elif os.path.exists(cl_last_save_path): path_found_for_skip_training = cl_last_save_path

    if path_found_for_skip_training:
        LOG.warning(f"Model file exists ({os.path.basename(path_found_for_skip_training)}). JSON not found/invalid. Training skipped, evaluation needed.")
        skip_training_cl = True # Only skip training
        saved_model_paths[run_name_cl] = path_found_for_skip_training # Store path for eval
    else:
        LOG.info(f"No completed model file or valid results JSON found for '{strategy}'. Proceeding with training & evaluation.")
        skip_training_cl = False


    # --- Execute Training (if not skipped) ---
    model_out = None # Initialize model_out for the evaluation block
    # Get time/mem from results dict if loaded from JSON, otherwise use 0
    time_cl = results[run_name_cl].get('time', 0)
    peak_mem_cl = results[run_name_cl].get('mem_mb', 0)

    if can_run_cl and not skip_training_cl:
        LOG.info(f"=== Preparing CL Model ({run_name_cl}) ===")
        if not time_task_train_loaders or len(time_task_train_loaders) != NUM_TASKS: LOG.error(f"ERROR: Invalid loaders for {run_name_cl}. Skipping."); continue

        latest_epoch_checkpoint_path = find_latest_checkpoint(cl_save_path)
        if latest_epoch_checkpoint_path: LOG.warning(f"Found epoch ckpt: {os.path.basename(latest_epoch_checkpoint_path)}. Will resume.")
        else: LOG.info("No epoch ckpt found.")

        # Build fresh components for each strategy run
        cl_model = build_model(ENCODER_NAME, ENCODER_WEIGHTS, INPUT_CHANNELS, OUTPUT_CLASSES)
        if cl_model:
            optimizer_cl = create_poly_optimizer(cl_model, base_lr=LEARNING_RATE)
            replay_buffer = ReplayBuffer(max_size=REPLAY_BUFFER_SIZE) # Fresh Path buffer
            if optimizer_cl:
                scheduler_cl = None;
                try: total_batches_all_tasks = sum(EPOCHS_PER_TASK * len(loader) for loader in time_task_train_loaders if loader and len(loader) > 0); assert total_batches_all_tasks > 0; scheduler_cl = optim.lr_scheduler.PolynomialLR(optimizer_cl, total_iters=total_batches_all_tasks, power=0.9); LOG.info(f"CL scheduler iters={total_batches_all_tasks}")
                except Exception as e: LOG.error(f"ERROR CL scheduler: {e}");
                if scheduler_cl:
                    # --- Fallback Weight Loading (with Debug Print) ---
                    if latest_epoch_checkpoint_path is None:
                        LOG.info("Checking fallback weights..."); weights_path_to_load = None
                        if os.path.exists(cl_save_path): weights_path_to_load = cl_save_path
                        elif os.path.exists(cl_last_save_path): weights_path_to_load = cl_last_save_path

                        if weights_path_to_load: # Check if a path was found
                            LOG.warning(f"Attempting FALLBACK load: {os.path.basename(weights_path_to_load)}")
                            try:
                                LOG.info(f"DEBUG: Inside try block. Value of weights_path_to_load = {repr(weights_path_to_load)}")
                                if weights_path_to_load is None: LOG.error("CRITICAL DEBUG: weights_path_to_load is None JUST BEFORE torch.load!")
                                # Use weights_only=True
                                state_dict = torch.load(weights_path_to_load, map_location='cpu', weights_only=False)
                                if 'model_state_dict' in state_dict: state_dict = state_dict['model_state_dict']
                                cl_model.load_state_dict(state_dict, strict=False); LOG.info("Fallback weights loaded.")
                                cl_model.to(DEVICE)
                            except Exception as fb_err:
                                LOG.error(f"--- DETAILED FALLBACK LOAD TRACEBACK ---")
                                LOG.error(traceback.format_exc())
                                LOG.error(f"--- END TRACEBACK ---")
                                LOG.error(f"Fallback load error (logged fb_err object): {fb_err}")
                                cl_model.to(DEVICE) # Ensure model is on device even if load failed
                        else:
                            LOG.info("No fallback model found.")
                            cl_model.to(DEVICE) # Ensure on device if no fallback found
                    else:
                         cl_model.to(DEVICE) # Ensure on device if resuming from epoch ckpt

                    # Reset memory stats before training starts/resumes
                    if DEVICE.type == 'cuda':
                         try: torch.cuda.reset_peak_memory_stats(DEVICE)
                         except Exception as e_mem: LOG.warning(f"Could not reset cuda mem stats: {e_mem}")

                    LOG.info(f"Calling train_cl_detailed_eval for {run_name_cl}...")
                    # Ensure the original path lists are available globally here
                    if not all(p in globals() for p in required_paths):
                         LOG.error(f"Cannot start training for {run_name_cl}: Original train paths {required_paths} not found globally.")
                         # Clean up potentially created model/optimizer here before continuing
                         del cl_model, optimizer_cl, replay_buffer, scheduler_cl; gc.collect(); torch.cuda.empty_cache() if DEVICE.type=='cuda' else None;
                         continue # Skip this strategy

                    # --- Call Training Function ---
                    # This function performs the actual training epochs and tasks
                    model_out, val_history_cl, time_cl, eval_matrix_dict = train_cl_detailed_eval(
                        cl_model, time_task_train_loaders, time_task_test_loaders, val_loader, criterion,
                        optimizer_cl, scheduler_cl, EPOCHS_PER_TASK, replay_buffer, REPLAY_BATCH_SIZE,
                        strategy,
                        original_train_paths=(train_sar_paths, train_dem_paths, train_mask_paths), # Pass paths
                        device=DEVICE, model_save_path=cl_save_path,
                        num_tasks=NUM_TASKS, augment_replay=APPLY_REPLAY_AUG, patience=PATIENCE,
                        checkpoint_path=latest_epoch_checkpoint_path
                    )

                    # --- Store results from training run in memory ---
                    cl_detailed_results[run_name_cl] = eval_matrix_dict
                    histories[run_name_cl] = val_history_cl
                    peak_mem_cl = torch.cuda.max_memory_allocated(DEVICE)/(1024**2) if DEVICE.type == 'cuda' else 0.0
                    # Update saved model path map based on final state
                    current_save_path = None
                    if os.path.exists(cl_save_path): current_save_path = cl_save_path
                    elif os.path.exists(cl_last_save_path): current_save_path = cl_last_save_path
                    if current_save_path: saved_model_paths[run_name_cl] = current_save_path

                # --- Handle scheduler failure ---
                else:
                     LOG.error(f"Skip CL '{strategy}': scheduler fail.")
                     model_out = None # Ensure model_out is None if training failed

            # --- Handle optimizer failure ---
            else:
                 LOG.error(f"Skip CL '{strategy}': optimizer fail.")
                 model_out = None

        # --- Handle model build failure ---
        else:
             LOG.error(f"Skip CL '{strategy}': model fail.")
             model_out = None

    # --- Handle preconditions fail ---
    elif not can_run_cl:
        LOG.error(f"Skip CL ({strategy}): preconditions fail.")
        continue # Skip to next strategy

    # --- Execute Evaluation (if training wasn't skipped, OR if only training was skipped but model file exists) ---
    evaluation_performed_this_run = False
    # Determine if we need to evaluate:
    # Need model output from training OR a path to a saved model for this run.
    model_available_for_eval = (model_out is not None) or (saved_model_paths.get(run_name_cl) and os.path.exists(saved_model_paths[run_name_cl]))
    should_evaluate = model_available_for_eval and can_run_cl and test_loader is not None

    if should_evaluate:
        LOG.info(f"--- Evaluating final ({strategy}) on main test set ---")
        eval_model_cl = None
        try:
            # Get the model: either the output of training or load from file
            if model_out: # Use model from training if it exists
                 eval_model_cl = model_out
                 LOG.info("Evaluating model trained in this session.")
            elif saved_model_paths.get(run_name_cl) and os.path.exists(saved_model_paths[run_name_cl]):
                 model_file_to_load = saved_model_paths[run_name_cl]
                 LOG.info(f"Loading existing CL model from: {os.path.basename(model_file_to_load)}")
                 eval_model_cl = build_model(ENCODER_NAME, ENCODER_WEIGHTS, INPUT_CHANNELS, OUTPUT_CLASSES)
                 if eval_model_cl:
                      state_dict = torch.load(model_file_to_load, map_location=DEVICE, weights_only=False) # Set weights_only
                      if 'model_state_dict' in state_dict: state_dict = state_dict['model_state_dict']
                      eval_model_cl.load_state_dict(state_dict); eval_model_cl.to(DEVICE); LOG.info("Loaded weights.")
                 else: LOG.error(f"Failed to build model for evaluation: {run_name_cl}")
            else:
                 LOG.error(f"Cannot evaluate CL model: No model from training and no valid saved path found for {run_name_cl}.")

            # Perform evaluation if model is ready
            if eval_model_cl:
                 eval_model_cl.eval() # Set to eval mode
                 eval_output_final = evaluate_model(eval_model_cl, test_loader, criterion, DEVICE)
                 loss, dice, iou, f1, prec, rec, mcc, tp_final, tn_final, fp_final, fn_final = eval_output_final
                 # --- Update results dictionary for this run ---
                 current_run_summary = {
                     'loss': loss, 'dice': dice, 'iou': iou, 'f1': f1, 'precision': prec, 'recall': rec, 'mcc': mcc,
                     'tp': tp_final, 'tn': tn_final, 'fp': fp_final, 'fn': fn_final,
                     'time': time_cl, 'mem_mb': peak_mem_cl # Use time/mem calculated during training run
                 }
                 results[run_name_cl] = current_run_summary # Store/update in global dict
                 # --- Log results ---
                 LOG.info("---------------------------------------------------------------------------------------------")
                 LOG.info(f"Final CL ({strategy}) Main TEST >> L:{loss:.4f}, F1:{f1:.4f}, P:{prec:.4f}, R:{rec:.4f}, MCC:{mcc:.4f}")
                 LOG.info(f"Confusion Matrix (Final Test): TP={tp_final:.0f}, TN={tn_final:.0f}, FP={fp_final:.0f}, FN={fn_final:.0f}")
                 LOG.info("---------------------------------------------------------------------------------------------")
                 evaluation_performed_this_run = True
            else:
                 LOG.warning(f"Skipping evaluation for {run_name_cl} as model could not be loaded or trained.")
                 # results dict already has placeholders from setdefault earlier

        except Exception as e_cl_eval:
             LOG.error(f"ERROR CL Main TEST eval for {run_name_cl}: {e_cl_eval}", exc_info=True)
             # Update results dict with defaults on error, but keep time/mem if available
             results[run_name_cl].update({k:v for k,v in _def_metrics.items() if k not in ['time', 'mem_mb']})

        # Cleanup loaded eval model if it wasn't from training output
        if eval_model_cl and not model_out: del eval_model_cl

    elif not model_available_for_eval:
         LOG.warning(f"Cannot evaluate {run_name_cl}: No trained model output and no saved model path found.")
    elif not test_loader:
         LOG.warning(f"Skipping final main test eval for {run_name_cl}: test_loader None.")
    elif not can_run_cl:
         LOG.warning(f"Skipping final main test eval for {run_name_cl}: Preconditions not met.")


    # --- Save results ---
    if not skip_this_run:
         experiment_details_dict["last_update"] = datetime.now().isoformat()
         save_metrics_to_json(
              filename=json_run_filename, # Unique filename for this run
              run_name=run_name_cl,
              run_result_summary=results.get(run_name_cl),
              run_cl_detail_matrix=cl_detailed_results.get(run_name_cl),
              run_history=histories.get(run_name_cl),
              experiment_details=experiment_details_dict
         )
    else:
         LOG.info(f"Skipping JSON save for {run_name_cl} as run was skipped based on existing JSON.")

    # --- Cleanup potentially large objects from training/evaluation ---
    if 'model_out' in locals() and model_out: del model_out
    if 'cl_model' in locals() and cl_model: del cl_model
    if 'optimizer_cl' in locals() and optimizer_cl: del optimizer_cl
    if 'scheduler_cl' in locals() and scheduler_cl: del scheduler_cl
    if 'replay_buffer' in locals() and replay_buffer: del replay_buffer
    if 'val_history_cl' in locals() and val_history_cl: del val_history_cl
    if 'eval_matrix_dict' in locals() and eval_matrix_dict: del eval_matrix_dict
    # Explicitly delete eval_model_cl if it exists from loading
    if 'eval_model_cl' in locals() and eval_model_cl: del eval_model_cl
    gc.collect()
    if DEVICE.type == 'cuda': torch.cuda.empty_cache();
    LOG.info(f"{strategy} resources released.")

    LOG.info(f"Finished processing run for '{run_name_cl}'")
    LOG.info("---------------------------------------------------------------------------------------------")
# --- End Strategy Loop ---

LOG.info("--- CL Execution Loop Complete ---")

"""## CL Model Training with Buffer Size = 150"""

# Cell 15: Execute CL Training (REPLAY_BUFFER_SIZE = 150)


# --- Init dictionaries if they don't exist ---
results = globals().setdefault('results', {}); histories = globals().setdefault('histories', {}); saved_model_paths = globals().setdefault('saved_model_paths', {}); cl_detailed_results = globals().setdefault('cl_detailed_results', {})
if not results: LOG.info("Initialized results dict.")
if not histories: LOG.info("Initialized histories dict.")
if not saved_model_paths: LOG.info("Initialized saved_model_paths dict.")
if not cl_detailed_results: LOG.info("Initialized cl_detailed_results dict.")

# --- Safely get params ---
encoder_name = globals().get('ENCODER_NAME', 'unknown_encoder')
encoder_weights = str(globals().get('ENCODER_WEIGHTS', 'unknown'))
loss_type = globals().get('LOSS_TYPE', 'unknown_loss')

# --- Define Experiment Details Dictionary ---
# Using setdefault to avoid re-definition if already exists
experiment_details_dict = globals().setdefault('experiment_details_dict', {})
# Update/set details for this run
experiment_details_dict.update({
    "encoder": encoder_name, "encoder_weights": encoder_weights, "loss_type": loss_type,
    "num_tasks": globals().get('NUM_TASKS', '?'),
    "epochs_per_task": globals().get('EPOCHS_PER_TASK', '?'),
    "epochs_baseline": globals().get('EPOCHS_BASELINE', '?'), # Keep baseline info if available
    "buffer_size": globals().get('REPLAY_BUFFER_SIZE', '?'),
    "replay_batch_size": globals().get('REPLAY_BATCH_SIZE', '?'),
    "approx_score_fraction": globals().get('UNCERTAINTY_SUBSET_FRACTION', 'N/A'),
    "save_epoch_checkpoints": globals().get('SAVE_EPOCH_CHECKPOINTS', '?'),
    "use_early_stopping": globals().get('USE_EARLY_STOPPING', '?'),
    "last_update": datetime.now().isoformat(),
    # Add flags indicating which optimizations are active
    "amp_enabled": True,
    "combined_batches": True,
    "buffer_stores_paths": True,
    "percentile_scaling": True if globals().get('NORM_PERCENTILES') else False
})


# --- Check Prerequisites ---
can_run_cl = False # Assume false initially
required_paths = ['train_sar_paths', 'train_dem_paths', 'train_mask_paths'] # Need these originals
if 'can_proceed' in globals() and can_proceed:
    if 'time_task_train_loaders' not in globals() or not time_task_train_loaders: LOG.error("CL Fail: Time-based Train loaders missing.")
    elif 'time_task_test_loaders' not in globals() or not time_task_test_loaders: LOG.error("CL Fail: Time-based Test loaders missing.")
    elif not all(p in globals() and globals()[p] is not None for p in required_paths):
        missing_paths = [p for p in required_paths if p not in globals() or globals()[p] is None]
        LOG.error(f"CL Fail: Original training paths missing: {missing_paths}. Cannot fill path buffer.")
    elif 'NUM_TASKS' not in globals() or NUM_TASKS <= 0: LOG.error("CL Fail: NUM_TASKS invalid.")
    elif len(time_task_train_loaders) != NUM_TASKS: LOG.error(f"CL Fail: Loader count ({len(time_task_train_loaders)}) != current NUM_TASKS ({NUM_TASKS}). Check Cell 13.")
    else: LOG.info(f"Preconditions for CL met (NUM_TASKS={NUM_TASKS})."); can_run_cl = True
else: LOG.error("CL Fail: Base preconditions not met.")

APPLY_REPLAY_AUG = True; LOG.info(f"Replay augmentation set to: {APPLY_REPLAY_AUG}")

# --- Loop through CL Strategies ---
if 'cl_strategies' not in globals(): cl_strategies = ['random'] # Default if not defined
for strategy in cl_strategies:
    # --- Define names and paths ---
    run_name_cl = f"cl_{strategy}_{encoder_name}W{encoder_weights}_{loss_type}_t{NUM_TASKS}e{EPOCHS_PER_TASK}_b{REPLAY_BUFFER_SIZE}"
    LOG.info(f"<<<<<<<<<<<< Attempting CL Run: {run_name_cl} >>>>>>>>>>>>")
    LOG.info("---------------------------------------------------------------------------------------------")
    cl_save_path = os.path.join(SAVED_MODELS_DIR, f"{run_name_cl}_detailed.pth")
    cl_last_save_path = cl_save_path.replace('.pth', '_last.pth')
    json_run_filename = os.path.join(SAVED_MODELS_DIR, f"{run_name_cl}_results.json") # Unique JSON path
    LOG.info(f"Checking for existing results file: {os.path.basename(json_run_filename)}")

    # --- Skip Logic (Prioritize loading valid JSON results) ---
    skip_this_run = False # Flag to skip both train and eval for this strategy
    skip_training_cl = False # Flag to skip only training
    # Default metrics placeholder
    _def_metrics = {'loss':np.nan,'f1':np.nan,'precision':np.nan,'recall':np.nan, 'mcc':np.nan, 'tp':0, 'tn':0, 'fp':0, 'fn':0, 'time': 0, 'mem_mb': 0}
    results.setdefault(run_name_cl, _def_metrics.copy()) # Ensure entry exists
    histories.setdefault(run_name_cl, {}) # Ensure entry exists
    cl_detailed_results.setdefault(run_name_cl, [[{} for _ in range(NUM_TASKS)] for _ in range(NUM_TASKS)]) # Placeholder

    if os.path.exists(json_run_filename):
        LOG.warning(f"Results JSON exists: {os.path.basename(json_run_filename)}. Attempting to load.")
        try:
             with open(json_run_filename, 'r') as f: previous_data = json.load(f)
             prev_metrics = previous_data.get("final_test_metrics", {})
             # --- Check for a key metric to validate ---
             if prev_metrics and 'f1' in prev_metrics and prev_metrics['f1'] is not None: # Check if F1 is present and not None (JSON null)
                 LOG.info(f"Valid previous results loaded from JSON for '{run_name_cl}'. Skipping run.")
                 skip_this_run = True # Skip both train and eval
                 # --- Load data into memory dictionaries ---
                 results[run_name_cl] = prev_metrics
                 histories[run_name_cl] = previous_data.get("validation_history", {})
                 cl_detailed_results[run_name_cl] = previous_data.get("detailed_cl_matrix", [[{} for _ in range(NUM_TASKS)] for _ in range(NUM_TASKS)])
                 # Try to find corresponding model path and store it
                 model_path_found = None
                 if os.path.exists(cl_save_path): model_path_found = cl_save_path
                 elif os.path.exists(cl_last_save_path): model_path_found = cl_last_save_path
                 if model_path_found:
                     saved_model_paths[run_name_cl] = model_path_found
                 else:
                     LOG.warning(f"Loaded results from JSON, but corresponding model file not found for {run_name_cl}")
             else:
                 LOG.warning("Previous JSON found but contained invalid/incomplete metrics.")
        except Exception as json_e:
             LOG.error(f"Error loading or parsing previous JSON {os.path.basename(json_run_filename)}: {json_e}.")

    # If we decided to skip everything based on JSON, continue to next strategy
    if skip_this_run:
         LOG.info(f"Skipping strategy '{strategy}' entirely due to loaded results.")
         continue # Go to the next iteration of the for strategy loop

    # Check for model file only if not skipping run based on JSON
    path_found_for_skip_training = None
    if os.path.exists(cl_save_path): path_found_for_skip_training = cl_save_path
    elif os.path.exists(cl_last_save_path): path_found_for_skip_training = cl_last_save_path

    if path_found_for_skip_training:
        LOG.warning(f"Model file exists ({os.path.basename(path_found_for_skip_training)}). JSON not found/invalid. Training skipped, evaluation needed.")
        skip_training_cl = True # Only skip training
        saved_model_paths[run_name_cl] = path_found_for_skip_training # Store path for eval
    else:
        LOG.info(f"No completed model file or valid results JSON found for '{strategy}'. Proceeding with training & evaluation.")
        skip_training_cl = False


    # --- Execute Training (if not skipped) ---
    model_out = None # Initialize model_out for the evaluation block
    # Get time/mem from results dict if loaded from JSON, otherwise use 0
    time_cl = results[run_name_cl].get('time', 0)
    peak_mem_cl = results[run_name_cl].get('mem_mb', 0)

    if can_run_cl and not skip_training_cl:
        LOG.info(f"=== Preparing CL Model ({run_name_cl}) ===")
        if not time_task_train_loaders or len(time_task_train_loaders) != NUM_TASKS: LOG.error(f"ERROR: Invalid loaders for {run_name_cl}. Skipping."); continue

        latest_epoch_checkpoint_path = find_latest_checkpoint(cl_save_path)
        if latest_epoch_checkpoint_path: LOG.warning(f"Found epoch ckpt: {os.path.basename(latest_epoch_checkpoint_path)}. Will resume.")
        else: LOG.info("No epoch ckpt found.")

        # Build fresh components for each strategy run
        cl_model = build_model(ENCODER_NAME, ENCODER_WEIGHTS, INPUT_CHANNELS, OUTPUT_CLASSES)
        if cl_model:
            optimizer_cl = create_poly_optimizer(cl_model, base_lr=LEARNING_RATE)
            replay_buffer = ReplayBuffer(max_size=REPLAY_BUFFER_SIZE) # Fresh Path buffer
            if optimizer_cl:
                scheduler_cl = None;
                try: total_batches_all_tasks = sum(EPOCHS_PER_TASK * len(loader) for loader in time_task_train_loaders if loader and len(loader) > 0); assert total_batches_all_tasks > 0; scheduler_cl = optim.lr_scheduler.PolynomialLR(optimizer_cl, total_iters=total_batches_all_tasks, power=0.9); LOG.info(f"CL scheduler iters={total_batches_all_tasks}")
                except Exception as e: LOG.error(f"ERROR CL scheduler: {e}");
                if scheduler_cl:
                    # --- Fallback Weight Loading (with Debug Print) ---
                    if latest_epoch_checkpoint_path is None:
                        LOG.info("Checking fallback weights..."); weights_path_to_load = None
                        if os.path.exists(cl_save_path): weights_path_to_load = cl_save_path
                        elif os.path.exists(cl_last_save_path): weights_path_to_load = cl_last_save_path

                        if weights_path_to_load: # Check if a path was found
                            LOG.warning(f"Attempting FALLBACK load: {os.path.basename(weights_path_to_load)}")
                            try:
                                LOG.info(f"DEBUG: Inside try block. Value of weights_path_to_load = {repr(weights_path_to_load)}")
                                if weights_path_to_load is None: LOG.error("CRITICAL DEBUG: weights_path_to_load is None JUST BEFORE torch.load!")
                                # Use weights_only=True
                                state_dict = torch.load(weights_path_to_load, map_location='cpu', weights_only=False)
                                if 'model_state_dict' in state_dict: state_dict = state_dict['model_state_dict']
                                cl_model.load_state_dict(state_dict, strict=False); LOG.info("Fallback weights loaded.")
                                cl_model.to(DEVICE)
                            except Exception as fb_err:
                                LOG.error(f"--- DETAILED FALLBACK LOAD TRACEBACK ---")
                                LOG.error(traceback.format_exc())
                                LOG.error(f"--- END TRACEBACK ---")
                                LOG.error(f"Fallback load error (logged fb_err object): {fb_err}")
                                cl_model.to(DEVICE) # Ensure model is on device even if load failed
                        else:
                            LOG.info("No fallback model found.")
                            cl_model.to(DEVICE) # Ensure on device if no fallback found
                    else:
                         cl_model.to(DEVICE) # Ensure on device if resuming from epoch ckpt

                    # Reset memory stats before training starts/resumes
                    if DEVICE.type == 'cuda':
                         try: torch.cuda.reset_peak_memory_stats(DEVICE)
                         except Exception as e_mem: LOG.warning(f"Could not reset cuda mem stats: {e_mem}")

                    LOG.info(f"Calling train_cl_detailed_eval for {run_name_cl}...")
                    # Ensure the original path lists are available globally here
                    if not all(p in globals() for p in required_paths):
                         LOG.error(f"Cannot start training for {run_name_cl}: Original train paths {required_paths} not found globally.")
                         # Clean up potentially created model/optimizer here before continuing
                         del cl_model, optimizer_cl, replay_buffer, scheduler_cl; gc.collect(); torch.cuda.empty_cache() if DEVICE.type=='cuda' else None;
                         continue # Skip this strategy

                    # --- Call Training Function ---
                    # This function performs the actual training epochs and tasks
                    model_out, val_history_cl, time_cl, eval_matrix_dict = train_cl_detailed_eval(
                        cl_model, time_task_train_loaders, time_task_test_loaders, val_loader, criterion,
                        optimizer_cl, scheduler_cl, EPOCHS_PER_TASK, replay_buffer, REPLAY_BATCH_SIZE,
                        strategy,
                        original_train_paths=(train_sar_paths, train_dem_paths, train_mask_paths), # Pass paths
                        device=DEVICE, model_save_path=cl_save_path,
                        num_tasks=NUM_TASKS, augment_replay=APPLY_REPLAY_AUG, patience=PATIENCE,
                        checkpoint_path=latest_epoch_checkpoint_path
                    )

                    # --- Store results from training run in memory ---
                    cl_detailed_results[run_name_cl] = eval_matrix_dict
                    histories[run_name_cl] = val_history_cl
                    peak_mem_cl = torch.cuda.max_memory_allocated(DEVICE)/(1024**2) if DEVICE.type == 'cuda' else 0.0
                    # Update saved model path map based on final state
                    current_save_path = None
                    if os.path.exists(cl_save_path): current_save_path = cl_save_path
                    elif os.path.exists(cl_last_save_path): current_save_path = cl_last_save_path
                    if current_save_path: saved_model_paths[run_name_cl] = current_save_path

                # --- Handle scheduler failure ---
                else:
                     LOG.error(f"Skip CL '{strategy}': scheduler fail.")
                     model_out = None # Ensure model_out is None if training failed

            # --- Handle optimizer failure ---
            else:
                 LOG.error(f"Skip CL '{strategy}': optimizer fail.")
                 model_out = None

        # --- Handle model build failure ---
        else:
             LOG.error(f"Skip CL '{strategy}': model fail.")
             model_out = None

    # --- Handle preconditions fail ---
    elif not can_run_cl:
        LOG.error(f"Skip CL ({strategy}): preconditions fail.")
        continue # Skip to next strategy

    # --- Execute Evaluation (if training wasn't skipped, OR if only training was skipped but model file exists) ---
    evaluation_performed_this_run = False
    # Determine if we need to evaluate:
    # Need model output from training OR a path to a saved model for this run.
    model_available_for_eval = (model_out is not None) or (saved_model_paths.get(run_name_cl) and os.path.exists(saved_model_paths[run_name_cl]))
    should_evaluate = model_available_for_eval and can_run_cl and test_loader is not None

    if should_evaluate:
        LOG.info(f"--- Evaluating final ({strategy}) on main test set ---")
        eval_model_cl = None
        try:
            # Get the model: either the output of training or load from file
            if model_out: # Use model from training if it exists
                 eval_model_cl = model_out
                 LOG.info("Evaluating model trained in this session.")
            elif saved_model_paths.get(run_name_cl) and os.path.exists(saved_model_paths[run_name_cl]):
                 model_file_to_load = saved_model_paths[run_name_cl]
                 LOG.info(f"Loading existing CL model from: {os.path.basename(model_file_to_load)}")
                 eval_model_cl = build_model(ENCODER_NAME, ENCODER_WEIGHTS, INPUT_CHANNELS, OUTPUT_CLASSES)
                 if eval_model_cl:
                      state_dict = torch.load(model_file_to_load, map_location=DEVICE, weights_only=False) # Set weights_only
                      if 'model_state_dict' in state_dict: state_dict = state_dict['model_state_dict']
                      eval_model_cl.load_state_dict(state_dict); eval_model_cl.to(DEVICE); LOG.info("Loaded weights.")
                 else: LOG.error(f"Failed to build model for evaluation: {run_name_cl}")
            else:
                 LOG.error(f"Cannot evaluate CL model: No model from training and no valid saved path found for {run_name_cl}.")

            # Perform evaluation if model is ready
            if eval_model_cl:
                 eval_model_cl.eval() # Set to eval mode
                 eval_output_final = evaluate_model(eval_model_cl, test_loader, criterion, DEVICE)
                 loss, dice, iou, f1, prec, rec, mcc, tp_final, tn_final, fp_final, fn_final = eval_output_final
                 # --- Update results dictionary for this run ---
                 current_run_summary = {
                     'loss': loss, 'dice': dice, 'iou': iou, 'f1': f1, 'precision': prec, 'recall': rec, 'mcc': mcc,
                     'tp': tp_final, 'tn': tn_final, 'fp': fp_final, 'fn': fn_final,
                     'time': time_cl, 'mem_mb': peak_mem_cl # Use time/mem calculated during training run
                 }
                 results[run_name_cl] = current_run_summary # Store/update in global dict
                 # --- Log results ---
                 LOG.info("---------------------------------------------------------------------------------------------")
                 LOG.info(f"Final CL ({strategy}) Main TEST >> L:{loss:.4f}, F1:{f1:.4f}, P:{prec:.4f}, R:{rec:.4f}, MCC:{mcc:.4f}")
                 LOG.info(f"Confusion Matrix (Final Test): TP={tp_final:.0f}, TN={tn_final:.0f}, FP={fp_final:.0f}, FN={fn_final:.0f}")
                 LOG.info("---------------------------------------------------------------------------------------------")
                 evaluation_performed_this_run = True
            else:
                 LOG.warning(f"Skipping evaluation for {run_name_cl} as model could not be loaded or trained.")
                 # results dict already has placeholders from setdefault earlier

        except Exception as e_cl_eval:
             LOG.error(f"ERROR CL Main TEST eval for {run_name_cl}: {e_cl_eval}", exc_info=True)
             # Update results dict with defaults on error, but keep time/mem if available
             results[run_name_cl].update({k:v for k,v in _def_metrics.items() if k not in ['time', 'mem_mb']})

        # Cleanup loaded eval model if it wasn't from training output
        if eval_model_cl and not model_out: del eval_model_cl

    elif not model_available_for_eval:
         LOG.warning(f"Cannot evaluate {run_name_cl}: No trained model output and no saved model path found.")
    elif not test_loader:
         LOG.warning(f"Skipping final main test eval for {run_name_cl}: test_loader None.")
    elif not can_run_cl:
         LOG.warning(f"Skipping final main test eval for {run_name_cl}: Preconditions not met.")


    # --- Save results ---
    if not skip_this_run:
         experiment_details_dict["last_update"] = datetime.now().isoformat()
         save_metrics_to_json(
              filename=json_run_filename, # Unique filename for this run
              run_name=run_name_cl,
              run_result_summary=results.get(run_name_cl),
              run_cl_detail_matrix=cl_detailed_results.get(run_name_cl),
              run_history=histories.get(run_name_cl),
              experiment_details=experiment_details_dict
         )
    else:
         LOG.info(f"Skipping JSON save for {run_name_cl} as run was skipped based on existing JSON.")

    # --- Cleanup potentially large objects from training/evaluation ---
    if 'model_out' in locals() and model_out: del model_out
    if 'cl_model' in locals() and cl_model: del cl_model
    if 'optimizer_cl' in locals() and optimizer_cl: del optimizer_cl
    if 'scheduler_cl' in locals() and scheduler_cl: del scheduler_cl
    if 'replay_buffer' in locals() and replay_buffer: del replay_buffer
    if 'val_history_cl' in locals() and val_history_cl: del val_history_cl
    if 'eval_matrix_dict' in locals() and eval_matrix_dict: del eval_matrix_dict
    # Explicitly delete eval_model_cl if it exists from loading
    if 'eval_model_cl' in locals() and eval_model_cl: del eval_model_cl
    gc.collect()
    if DEVICE.type == 'cuda': torch.cuda.empty_cache();
    LOG.info(f"{strategy} resources released.")

    LOG.info(f"Finished processing run for '{run_name_cl}'")
    LOG.info("---------------------------------------------------------------------------------------------")
# --- End Strategy Loop ---

LOG.info("--- CL Execution Loop Complete ---")

"""# PART 3: ANALYSIS"""

# Cell 16: Helper Functions for Analysis


def load_and_parse_metrics_v1(filepath, logger, default_cl_tasks=3, default_baseline_tasks=10):
    """
    Loads and parses metrics from a JSON file with the 'v1' structure:
    """
    # Initialize return values
    summary_results = {}
    cl_detailed_matrix = {}
    validation_history = {}
    baseline_num_tasks = default_baseline_tasks # Use default initially

    LOG.info(f"Attempting to load metrics from: {os.path.basename(filepath)}")

    if os.path.exists(filepath):
        try:
            with open(filepath, 'r') as f:
                 # Handle potential NaN, Infinity in JSON if needed
                 try:
                     loaded_data = json.load(f)
                 except json.JSONDecodeError as json_err:
                     if 'NaN' in str(json_err) or 'Infinity' in str(json_err):
                          LOG.warning(f"JSON file contains non-standard 'NaN' or 'Infinity'. Attempting reload with replacements.")
                          f.seek(0)
                          file_content = f.read().replace(': NaN', ': null').replace(': Infinity', ': null').replace(': -Infinity', ': null')
                          loaded_data = json.loads(file_content)
                     else:
                          raise json_err

            # Extract data using expected keys for this format
            summary_results = loaded_data.get("summary_results", {})
            cl_detailed_matrix = loaded_data.get("detailed_cl_matrix", {})
            validation_history = loaded_data.get("validation_history", {})

            # Get baseline number of tasks from experiment details
            exp_details = loaded_data.get("experiment_details", {})
            try:
                baseline_num_tasks_from_file = exp_details.get("num_tasks")
                if baseline_num_tasks_from_file is not None:
                     baseline_num_tasks = int(baseline_num_tasks_from_file)
                # Handle string '30' case seen in data
                elif isinstance(exp_details.get("epochs_baseline"), str):
                     try:
                         # Assuming num_tasks relates to epochs_baseline if num_tasks missing
                         baseline_num_tasks = int(exp_details["epochs_baseline"])
                         LOG.warning(f"Parsed num_tasks from string 'epochs_baseline': {baseline_num_tasks}")
                     except (ValueError, KeyError): pass # Ignore if conversion fails or key missing
            except (ValueError, TypeError):
                 LOG.warning(f"Could not parse num_tasks from experiment_details. Using default: {baseline_num_tasks}")
                 # Keep the default value

            LOG.info(f"Successfully loaded metrics from JSON (Baseline Expected Tasks: {baseline_num_tasks}, CL Expected Tasks: {default_cl_tasks}).")

        except Exception as e:
            LOG.error(f"ERROR loading or parsing {os.path.basename(filepath)}: {e}\n{traceback.format_exc()}")
            LOG.warning("Returning empty metrics dictionaries.")
            summary_results, cl_detailed_matrix, validation_history = {}, {}, {}
            baseline_num_tasks = default_baseline_tasks

    else:
        LOG.warning(f"JSON file not found: {filepath}. Returning empty metrics.")
        summary_results, cl_detailed_matrix, validation_history = {}, {}, {}
        baseline_num_tasks = default_baseline_tasks

    return summary_results, cl_detailed_matrix, validation_history, baseline_num_tasks

LOG.info("Defined load_and_parse_metrics")



def calculate_iou_from_dice(dice_score):
    """Calculates IoU from Dice score, handling None/NaN/Invalid."""
    if dice_score is None: return np.nan
    if isinstance(dice_score, str) and dice_score.lower() == 'nan': return np.nan
    if isinstance(dice_score, (float, np.number)) and np.isnan(dice_score): return np.nan
    try:
        dice_float = float(dice_score)
    except (ValueError, TypeError):
        return np.nan
    if not (0 <= dice_float <= 1): return np.nan
    return 0.0 if dice_float == 0.0 else dice_float / (2.0 - dice_float)
LOG.info("Defined calculate_iou_from_dice")

def format_metric(value, precision=4):
    """Formats a metric value, handling None and NaN."""
    if value is None: return "N/A"
    if isinstance(value, str) and value.lower() == 'nan': return "N/A"
    if isinstance(value, (float, np.number)) and np.isnan(value): return "N/A"
    try:
        float_val = float(value)
        if precision > 0 and np.isclose(float_val, 0):
             return f"{0.0:.{precision}f}"
        return f"{float_val:.{precision}f}"
    except (ValueError, TypeError):
        return str(value)
LOG.info("Defined format_metric")

def format_metric_list(metric_list, precision=4):
    """Formats a list of metrics, handling non-numeric types and NaN."""
    formatted_list = []
    for m in metric_list:
         formatted_list.append(format_metric(m, precision))
    return "[" + ", ".join(formatted_list) + "]"
LOG.info("Defined format_metric_list")


LOG.info("Analysis Helper Functions Defined")

# Cell 17: Metric Analysis (Optimized)

# --- Configuration ---
encoder_name = globals().get('ENCODER_NAME', 'resnet34')
encoder_weights = str(globals().get('ENCODER_WEIGHTS', 'imagenet')) # Ensure string
loss_type = globals().get('LOSS_TYPE', 'tversky')
num_tasks_loaded = globals().get('NUM_TASKS', 3)

# --- Find and Load Individual JSON Result Files ---
file_pattern_suffix = "_results.json"
search_pattern = os.path.join(SAVED_MODELS_DIR, f"*_{encoder_name}W{encoder_weights}_{loss_type}_*{file_pattern_suffix}")
LOG.info(f"Searching for result files matching: {search_pattern}")

json_files_found = glob.glob(search_pattern)
LOG.info(f"Found {len(json_files_found)} matching result files.")
LOG.warning('Reload cell manually if "Error Loading or parsing file..."')


# Load files in parallel
with concurrent.futures.ThreadPoolExecutor() as executor:
    loaded_files = list(executor.map(load_json_file, json_files_found))

results = {}
cl_results_matrices = {}

for loaded in loaded_files:
    if loaded["error"]:
        LOG.error(f"Error loading or parsing file {os.path.basename(loaded['f_path'])}: {loaded['error']}")
        continue

    run_name = loaded["run_name"]
    data = loaded["data"]

    if not run_name:
        run_name = os.path.basename(loaded['f_path']).replace(file_pattern_suffix, "")
        LOG.warning(f"File {os.path.basename(loaded['f_path'])} missing 'run_name' key. Using filename: {run_name}")

    LOG.info(f"Loading data for run: {run_name}")
    results[run_name] = data.get("final_test_metrics", {})

    # Calculate IoU if needed
    if results[run_name].get('iou') is None:
        dice_val_summary = results[run_name].get('dice', results[run_name].get('f1'))
        if dice_val_summary is not None:
            results[run_name]['iou'] = calculate_iou_from_dice(dice_val_summary)

    if "detailed_cl_matrix" in data and data["detailed_cl_matrix"]:
        cl_results_matrices[run_name] = data["detailed_cl_matrix"]

if not results:
    LOG.warning("No results were successfully loaded from JSON files.")

# --- Analysis Section ---
LOG.info("=======================================================================================")
LOG.info(f" Metrics Analysis Report ({encoder_name}W{encoder_weights} / {loss_type})")
LOG.info("=======================================================================================")

if not cl_results_matrices:
    LOG.info("No CL matrices (cl_results_matrices dict) available to analyze for CL metrics.")
else:
    LOG.info(f"Analyzing CL matrices for {len(cl_results_matrices)} strategies...")
    primary_metric_for_cl = 'f1'

    # Create shallow copy
    analysis_cl_matrices = {k: v[:] for k, v in cl_results_matrices.items()}

    for run_name, results_matrix_list in analysis_cl_matrices.items():
        if 'baseline' in run_name.lower() or '_t' not in run_name: # Skip baseline or non-task runs for CL metrics
            if run_name in results:
                results[run_name][f'avg_plasticity_{primary_metric_for_cl}'] = np.nan
                results[run_name][f'avg_forgetting_{primary_metric_for_cl}'] = np.nan
            continue

        LOG.info("")
        LOG.info(f"Metrics for CL Run: {run_name}")
        LOG.info("")
        run_summary = results.get(run_name, {})
        LOG.info(f">>> Final Test Metrics <<<")
        summary_metrics_to_show = ['loss', 'f1', 'iou', 'mcc', 'precision', 'recall', 'dice']
        for metric_key in summary_metrics_to_show:
            val = run_summary.get(metric_key)
            LOG.info(f"{metric_key.upper()}: {format_metric(val)}")
        LOG.info(f"TP: {run_summary.get('tp', 'N/A')}, TN: {run_summary.get('tn', 'N/A')}, FP: {run_summary.get('fp', 'N/A')}, FN: {run_summary.get('fn', 'N/A')}")

        # Calculate IoU if needed within the detailed matrix
        iou_calculated_count = 0
        try:
            for k_task_idx, task_row in enumerate(results_matrix_list):
                if not isinstance(task_row, list):
                    continue
                for j_eval_task_idx, metrics_dict in enumerate(task_row):
                    if isinstance(metrics_dict, dict) and metrics_dict.get('iou') is None:
                        dice_val = metrics_dict.get('dice', metrics_dict.get('f1', np.nan))
                        iou_calculated = calculate_iou_from_dice(dice_val)
                        metrics_dict['iou'] = iou_calculated
                        if not np.isnan(iou_calculated):
                            iou_calculated_count += 1
        except Exception as iou_err:
            LOG.error(f"Error during detailed IoU calculation for {run_name}: {iou_err}")
        if iou_calculated_count > 0:
            LOG.debug(f"Calculated 'iou' for {iou_calculated_count} detailed metric sets in {run_name}.")

        # --- Extract primary metric matrix for CL calculations ---
        results_matrix_primary_np = None
        try:
            # First determine matrix dimensions
            actual_rows = len(results_matrix_list)
            if actual_rows == 0:
                raise ValueError("Matrix data is empty")

            # Find maximum number of columns (tasks)
            actual_cols = max(len(row) for row in results_matrix_list) if actual_rows > 0 else 0
            target_cols = min(actual_cols, num_tasks_loaded)

            # Initialize matrix with NaNs
            results_matrix_primary_np = np.full((actual_rows, target_cols), np.nan, dtype=float)

            # Fill matrix with metric values
            for k in range(actual_rows):
                if not isinstance(results_matrix_list[k], list):
                    continue

                for j in range(min(len(results_matrix_list[k]), target_cols)):
                    metrics_dict = results_matrix_list[k][j]
                    if not isinstance(metrics_dict, dict):
                        continue

                    metric_val = metrics_dict.get(primary_metric_for_cl)

                    # Handle None values gracefully
                    if metric_val is None:
                        continue

                    try:
                        results_matrix_primary_np[k, j] = float(metric_val)
                    except (ValueError, TypeError) as conv_err:
                        LOG.debug(f"Could not convert metric value {metric_val} to float at position ({k},{j})")
                        continue

        except Exception as extract_err:
            LOG.error(f"Error extracting primary metric matrix for {run_name}: {extract_err}. Skipping CL calculations.")
            if run_name in results:
                results[run_name][f'avg_plasticity_{primary_metric_for_cl}'] = np.nan
                results[run_name][f'avg_forgetting_{primary_metric_for_cl}'] = np.nan
            continue

        # --- Perform CL Calculations (Plasticity, Forgetting) ---
        avg_plasticity, avg_forgetting = np.nan, np.nan
        if results_matrix_primary_np is not None:
            try:
                matrix_rows, matrix_cols = results_matrix_primary_np.shape
                if matrix_rows > 0 and matrix_cols > 0:
                    # Calculate plasticity (diagonal mean)
                    diag_len = min(matrix_rows, matrix_cols)
                    diagonal = np.diag(results_matrix_primary_np[:diag_len, :diag_len])
                    valid_diagonal = diagonal[~np.isnan(diagonal)]
                    if valid_diagonal.size > 0:
                        avg_plasticity = np.mean(valid_diagonal)

                    # Calculate forgetting
                    if matrix_rows > 1:
                        final_row_idx = matrix_rows - 1
                        tasks_to_compare = min(final_row_idx, diag_len)
                        if tasks_to_compare > 0:
                            initial_metrics = np.diag(results_matrix_primary_np[:tasks_to_compare, :tasks_to_compare])
                            final_metrics_on_prev = results_matrix_primary_np[final_row_idx, :tasks_to_compare]
                            if initial_metrics.size == final_metrics_on_prev.size:
                                metric_drops = initial_metrics - final_metrics_on_prev
                                valid_drops = metric_drops[~np.isnan(metric_drops)]
                                if valid_drops.size > 0:
                                    avg_forgetting = np.mean(valid_drops)
                else:
                    LOG.warning(f"Extracted primary metric matrix for {run_name} is empty.")
            except Exception as calc_err:
                LOG.error(f"Error calculating CL metrics for {run_name}: {calc_err}", exc_info=True)
                avg_plasticity, avg_forgetting = np.nan, np.nan

        # Store results (unchanged)
        if run_name in results:
            results[run_name][f'avg_plasticity_{primary_metric_for_cl}'] = avg_plasticity
            results[run_name][f'avg_forgetting_{primary_metric_for_cl}'] = avg_forgetting
        else:
            LOG.error(f"Run {run_name} not in main results dict. CL metrics not stored.")

        LOG.info(f"PLASTICITY: {format_metric(avg_plasticity)}")
        LOG.info(f"FORGETTING: {format_metric(avg_forgetting)}")

LOG.info("---------------------------------------------------------------------------------------")
LOG.info("Cell 17: Analysis and CL Metric Storage Complete")
LOG.info("---------------------------------------------------------------------------------------")

# Cell 18: Consolidated Results Analysis, Tables, and Plots (Using pre-calculated metrics from Cell 17)

import pandas as pd
import seaborn as sns

pd.set_option('display.max_columns', None)
pd.set_option('display.width', 1000)

all_run_data = []

# Ensure 'results' dictionary from Cell 17 exists
if 'results' not in globals() or not isinstance(results, dict) or not results:
    LOG.error("'results' dictionary not found, not a dict, or empty. Cell 17 must run successfully and populate 'results'.")
    # df_all will be empty, plots/tables will show no data or raise errors.
    results = {} # Avoid crashing, proceed with empty data
else:
    LOG.info(f"Processing {len(results)} runs from 'results' dictionary.")

cl_metric_key_suffix = 'f1'

# Sort results to process baseline first, then by run_name for consistent order
sorted_results_items = sorted(results.items(), key=lambda item: (0 if 'baseline' in item[0].lower() else 1, item[0]))


for run_name, metrics_data in sorted_results_items:
    #LOG.info(f"Compiling data for run: {run_name}")
    processed_run = {'run_name': run_name}

    # Extract metrics directly from metrics_data
    processed_run.update({
        'F1': metrics_data.get('f1'),
        'IoU': metrics_data.get('iou'),
        'Recall': metrics_data.get('recall'),
        'Precision': metrics_data.get('precision'),
        'MCC': metrics_data.get('mcc'),
        'Time (s)': metrics_data.get('time'),
        'Peak Mem (MB)': metrics_data.get('mem_mb')
    })

    # For CL metrics, get the pre-calculated values
    avg_forgetting_val = metrics_data.get(f'avg_forgetting_{cl_metric_key_suffix}', np.nan)
    avg_plasticity_val = metrics_data.get(f'avg_plasticity_{cl_metric_key_suffix}', np.nan)

    processed_run['Avg Forgetting (F1)'] = avg_forgetting_val # Column name implies F1, suffix should match
    processed_run['Avg Plasticity (F1)'] = avg_plasticity_val

    # Parse strategy and buffer size from run_name
    parts = run_name.lower().split('_')
    strategy_part = "Unknown"
    buffer_part = np.nan

    # First check for baseline
    if 'baseline' in parts:
        strategy_part = 'Baseline'
        buffer_part = 'N/A'
    else:
        # Special handling for least_confidence
        if 'least_confidence' in run_name.lower():
            strategy_part = 'least_confidence'
        else:
            # Map other strategies
            strategy_mapping = {
                'random': 'random',
                'margin': 'margin',
                'least': 'least_confidence',  # handles abbreviated forms
                'entropy': 'entropy'
            }

            for part in parts:
                if part in strategy_mapping:
                    strategy_part = strategy_mapping[part]
                    break

        # Find buffer size (works for b100, buffer100, etc.)
        for part in parts:
            if part.startswith('b') and part[1:].isdigit():  # matches b100
                buffer_part = int(part[1:])
                break
            elif part.startswith('buffer') and part[6:].isdigit():  # matches buffer100
                buffer_part = int(part[6:])
                break

    processed_run['Strategy'] = strategy_part
    processed_run['Buffer Size'] = buffer_part
    all_run_data.append(processed_run)


if not all_run_data:
    LOG.warning("No data processed into all_run_data. Cannot generate tables or plots.")
    df_all = pd.DataFrame() # Create empty DataFrame to avoid errors
else:
    df_all = pd.DataFrame(all_run_data)

# --- 2. Generate Tables ---
print("\n\nResults Tables")
buffer_sizes_for_tables = [50, 100, 150] # Or derive from df_all['Buffer Size'].unique()

for bs in buffer_sizes_for_tables:
    print(f"\n\nTable for Buffer Size: {bs} (and Baseline)")
    #LOG.info(f"Generating table for Buffer Size: {bs}")

    # Filter CL methods for this buffer size (ensure 'Buffer Size' is numeric for comparison)
    df_all['Buffer Size Num'] = pd.to_numeric(df_all['Buffer Size'], errors='coerce')
    cl_bs_data = df_all[(df_all['Buffer Size Num'] == bs) & (df_all['Strategy'] != 'Baseline')].copy()

    baseline_data_table = df_all[df_all['Strategy'] == 'Baseline'].copy()
    combined_data = pd.concat([baseline_data_table, cl_bs_data], ignore_index=True)

    if combined_data.empty:
        print(f"No data found for buffer size {bs}.")
        LOG.warning(f"No data found for buffer size {bs}.")
        continue

    numeric_cols = ['F1', 'IoU', 'Recall', 'Precision', 'MCC', 'Avg Forgetting (F1)', 'Avg Plasticity (F1)', 'Time (s)', 'Peak Mem (MB)']
    for col in numeric_cols:
        if col in combined_data.columns:
            combined_data[col] = pd.to_numeric(combined_data[col], errors='coerce').round(4)

    # Sort for display (Baseline first)
    combined_data['sort_key'] = combined_data['Strategy'].apply(lambda x: 0 if x == 'Baseline' else 1)
    combined_data.sort_values(by=['sort_key', 'Strategy'], ascending=[True, True], inplace=True)
    combined_data.drop(columns=['sort_key', 'Buffer Size Num'], inplace=True) # Drop helper column

    display_cols = ['Strategy', 'Buffer Size', 'F1', 'IoU', 'MCC', 'Recall', 'Precision',
                    'Avg Forgetting (F1)', 'Avg Plasticity (F1)',
                    'Time (s)', 'Peak Mem (MB)']
    for col in display_cols: # Ensure all display_cols exist
        if col not in combined_data.columns: combined_data[col] = np.nan
    display(combined_data[display_cols].fillna('N/A'))
df_all.drop(columns=['Buffer Size Num'], inplace=True, errors='ignore') # Clean up df_all

# --- 3. Generate Plots ---
# (encoder_name and loss_type must be defined for plot titles)
if 'encoder_name' not in globals(): encoder_name = "N/A" # Fallback for plot titles
if 'loss_type' not in globals(): loss_type = "N/A"       # Fallback for plot titles

print("\n\nResults Plots")
if not df_all.empty:
    # Ensure it correctly handles 'Buffer Size' which might be 'N/A' or numeric
    plt.style.use('seaborn-v0_8-whitegrid')

    df_cl_only = df_all[df_all['Strategy'] != 'Baseline'].copy()
    if not df_cl_only.empty:
        # Ensure 'Buffer Size' is numeric for hue aesthetic; handle 'N/A' or other non-numeric
        df_cl_only['Buffer Size'] = pd.to_numeric(df_cl_only['Buffer Size'], errors='coerce').fillna(0).astype(int) # Fill NaN with 0 for plotting

        plt.figure(figsize=(12, 7))
        sns.barplot(data=df_cl_only, x='Strategy', y='F1', hue='Buffer Size', palette='viridis')
        plt.title(f'Final F1 Score by Strategy and Buffer Size\n({encoder_name}/{loss_type})', fontsize=15)
        # ... rest of plotting code from original Cell 18
        plt.ylabel('F1 Score', fontsize=12)
        plt.xlabel('Continual Learning Strategy', fontsize=12)
        plt.xticks(rotation=45, ha='right')
        plt.legend(title='Buffer Size')
        plt.tight_layout()
        plt.show()

        plt.figure(figsize=(12, 7))
        sns.barplot(data=df_cl_only, x='Strategy', y='MCC', hue='Buffer Size', palette='magma')
        plt.title(f'MCC by Strategy and Buffer Size\n({encoder_name}/{loss_type})', fontsize=15)
        plt.ylabel('Matthews Correlation Coefficient', fontsize=12)
        plt.xlabel('Continual Learning Strategy', fontsize=12)
        plt.xticks(rotation=45, ha='right')
        plt.legend(title='Buffer Size')
        plt.tight_layout()
        plt.show()

    df_plot_all = df_all.copy()
    # Ensure 'Buffer Size' is treated correctly for Strategy_Buffer string formatting
    df_plot_all['Buffer Size Display'] = df_plot_all['Buffer Size'].apply(lambda x: 'N/A' if pd.isna(x) or x == 'N/A' else str(int(pd.to_numeric(x, errors='coerce'))))

    df_plot_all['Strategy_Buffer'] = df_plot_all.apply(
        lambda row: row['Strategy'] if row['Strategy'] == 'Baseline' else f"{row['Strategy']}_b{row['Buffer Size Display']}",
        axis=1
    )

    plt.figure(figsize=(14, 8))
    sns.barplot(data=df_plot_all.sort_values(by=['Strategy', 'Buffer Size Display']), # Sort for consistent plot
                x='Strategy_Buffer', y='F1', palette='tab20')
    plt.title(f'Final F1 Score Comparison\n({encoder_name}/{loss_type})', fontsize=16)
    # ... rest of plotting code from original Cell 18
    plt.ylabel('F1 Score', fontsize=13)
    plt.xlabel('Strategy (Buffer Size for CL)', fontsize=13)
    plt.xticks(rotation=75, ha='right')
    plt.grid(axis='y', linestyle='--')
    plt.tight_layout()
    plt.show()

else:
    LOG.info("df_all is empty. Skipping plot generation.")

print("--- Consolidated Analysis Complete (using pre-calculated metrics) ---")

# Cell 18: Consolidated Results Analysis, Tables, and Plots (Using pre-calculated metrics from Cell 17)

import pandas as pd
import seaborn as sns

pd.set_option('display.max_columns', None)
pd.set_option('display.width', 1000)

all_run_data = []

# Ensure 'results' dictionary from Cell 17 exists
if 'results' not in globals() or not isinstance(results, dict) or not results:
    LOG.error("'results' dictionary not found, not a dict, or empty. Cell 17 must run successfully and populate 'results'.")
    # df_all will be empty, plots/tables will show no data or raise errors.
    results = {} # Avoid crashing, proceed with empty data
else:
    LOG.info(f"Processing {len(results)} runs from 'results' dictionary.")

cl_metric_key_suffix = 'f1'

# Sort results to process baseline first, then by run_name for consistent order
sorted_results_items = sorted(results.items(), key=lambda item: (0 if 'baseline' in item[0].lower() else 1, item[0]))


for run_name, metrics_data in sorted_results_items:
    #LOG.info(f"Compiling data for run: {run_name}")
    processed_run = {'run_name': run_name}

    # Extract metrics directly from metrics_data
    processed_run.update({
        'F1': metrics_data.get('f1'),
        'IoU': metrics_data.get('iou'),
        'Recall': metrics_data.get('recall'),
        'Precision': metrics_data.get('precision'),
        'MCC': metrics_data.get('mcc'),
        'Time (s)': metrics_data.get('time'),
        'Peak Mem (MB)': metrics_data.get('mem_mb')
    })

    # For CL metrics, get the pre-calculated values
    avg_forgetting_val = metrics_data.get(f'avg_forgetting_{cl_metric_key_suffix}', np.nan)
    avg_plasticity_val = metrics_data.get(f'avg_plasticity_{cl_metric_key_suffix}', np.nan)

    processed_run['Avg Forgetting (F1)'] = avg_forgetting_val # Column name implies F1, suffix should match
    processed_run['Avg Plasticity (F1)'] = avg_plasticity_val

    # Parse strategy and buffer size from run_name
    parts = run_name.lower().split('_')
    strategy_part = "Unknown"
    buffer_part = np.nan

    # First check for baseline
    if 'baseline' in parts:
        strategy_part = 'Baseline'
        buffer_part = 'N/A'
    else:
        # Special handling for least_confidence
        if 'least_confidence' in run_name.lower():
            strategy_part = 'least_confidence'
        else:
            # Map other strategies
            strategy_mapping = {
                'random': 'random',
                'margin': 'margin',
                'least': 'least_confidence',  # handles abbreviated forms
                'entropy': 'entropy'
            }

            for part in parts:
                if part in strategy_mapping:
                    strategy_part = strategy_mapping[part]
                    break

        # Find buffer size (works for b100, buffer100, etc.)
        for part in parts:
            if part.startswith('b') and part[1:].isdigit():  # matches b100
                buffer_part = int(part[1:])
                break
            elif part.startswith('buffer') and part[6:].isdigit():  # matches buffer100
                buffer_part = int(part[6:])
                break

    processed_run['Strategy'] = strategy_part
    processed_run['Buffer Size'] = buffer_part
    all_run_data.append(processed_run)


if not all_run_data:
    LOG.warning("No data processed into all_run_data. Cannot generate tables or plots.")
    df_all = pd.DataFrame() # Create empty DataFrame to avoid errors
else:
    df_all = pd.DataFrame(all_run_data)

# --- 2. Generate Tables ---
print("\n\nResults Tables")
buffer_sizes_for_tables = [50, 100, 150] # Or derive from df_all['Buffer Size'].unique()

for bs in buffer_sizes_for_tables:
    print(f"\n\nTable for Buffer Size: {bs} (and Baseline)")
    #LOG.info(f"Generating table for Buffer Size: {bs}")

    # Filter CL methods for this buffer size (ensure 'Buffer Size' is numeric for comparison)
    df_all['Buffer Size Num'] = pd.to_numeric(df_all['Buffer Size'], errors='coerce')
    cl_bs_data = df_all[(df_all['Buffer Size Num'] == bs) & (df_all['Strategy'] != 'Baseline')].copy()

    baseline_data_table = df_all[df_all['Strategy'] == 'Baseline'].copy()
    combined_data = pd.concat([baseline_data_table, cl_bs_data], ignore_index=True)

    if combined_data.empty:
        print(f"No data found for buffer size {bs}.")
        LOG.warning(f"No data found for buffer size {bs}.")
        continue

    numeric_cols = ['F1', 'IoU', 'Recall', 'Precision', 'MCC', 'Avg Forgetting (F1)', 'Avg Plasticity (F1)', 'Time (s)', 'Peak Mem (MB)']
    for col in numeric_cols:
        if col in combined_data.columns:
            combined_data[col] = pd.to_numeric(combined_data[col], errors='coerce').round(4)

    # Sort for display (Baseline first)
    combined_data['sort_key'] = combined_data['Strategy'].apply(lambda x: 0 if x == 'Baseline' else 1)
    combined_data.sort_values(by=['sort_key', 'Strategy'], ascending=[True, True], inplace=True)
    combined_data.drop(columns=['sort_key', 'Buffer Size Num'], inplace=True) # Drop helper column

    display_cols = ['Strategy', 'Buffer Size', 'F1', 'IoU', 'MCC', 'Recall', 'Precision',
                    'Avg Forgetting (F1)', 'Avg Plasticity (F1)',
                    'Time (s)', 'Peak Mem (MB)']
    for col in display_cols: # Ensure all display_cols exist
        if col not in combined_data.columns: combined_data[col] = np.nan
    display(combined_data[display_cols].fillna('N/A'))
df_all.drop(columns=['Buffer Size Num'], inplace=True, errors='ignore') # Clean up df_all

# --- 3. Generate Plots ---
# (encoder_name and loss_type must be defined for plot titles)
if 'encoder_name' not in globals(): encoder_name = "N/A" # Fallback for plot titles
if 'loss_type' not in globals(): loss_type = "N/A"       # Fallback for plot titles

print("\n\nResults Plots")
if not df_all.empty:
    # Ensure it correctly handles 'Buffer Size' which might be 'N/A' or numeric
    plt.style.use('seaborn-v0_8-whitegrid')

    df_cl_only = df_all[df_all['Strategy'] != 'Baseline'].copy()
    if not df_cl_only.empty:
        # Ensure 'Buffer Size' is numeric for hue aesthetic; handle 'N/A' or other non-numeric
        df_cl_only['Buffer Size'] = pd.to_numeric(df_cl_only['Buffer Size'], errors='coerce').fillna(0).astype(int) # Fill NaN with 0 for plotting

        plt.figure(figsize=(12, 7))
        sns.barplot(data=df_cl_only, x='Strategy', y='F1', hue='Buffer Size', palette='viridis')
        plt.title(f'Final F1 Score by Strategy and Buffer Size\n({encoder_name}/{loss_type})', fontsize=15)
        plt.ylabel('F1 Score', fontsize=12)
        plt.xlabel('Continual Learning Strategy', fontsize=12)
        plt.xticks(rotation=45, ha='right')
        plt.legend(title='Buffer Size')
        plt.tight_layout()
        plt.show()

        plt.figure(figsize=(12, 7))
        sns.barplot(data=df_cl_only, x='Strategy', y='MCC', hue='Buffer Size', palette='magma')
        plt.title(f'MCC by Strategy and Buffer Size\n({encoder_name}/{loss_type})', fontsize=15)
        plt.ylabel('Matthews Correlation Coefficient', fontsize=12)
        plt.xlabel('Continual Learning Strategy', fontsize=12)
        plt.xticks(rotation=45, ha='right')
        plt.legend(title='Buffer Size')
        plt.tight_layout()
        plt.show()

    df_plot_all = df_all.copy()
    # Ensure 'Buffer Size' is treated correctly for Strategy_Buffer string formatting
    df_plot_all['Buffer Size Display'] = df_plot_all['Buffer Size'].apply(lambda x: 'N/A' if pd.isna(x) or x == 'N/A' else str(int(pd.to_numeric(x, errors='coerce'))))

    df_plot_all['Strategy_Buffer'] = df_plot_all.apply(
        lambda row: row['Strategy'] if row['Strategy'] == 'Baseline' else f"{row['Strategy']}_b{row['Buffer Size Display']}",
        axis=1
    )

    plt.figure(figsize=(14, 8))
    sns.barplot(data=df_plot_all.sort_values(by=['Strategy', 'Buffer Size Display']), # Sort for consistent plot
                x='Strategy_Buffer', y='F1', palette='tab20')
    plt.title(f'Final F1 Score Comparison\n({encoder_name}/{loss_type})', fontsize=16)
    plt.ylabel('F1 Score', fontsize=13)
    plt.xlabel('Strategy (Buffer Size for CL)', fontsize=13)
    plt.xticks(rotation=75, ha='right')
    plt.grid(axis='y', linestyle='--')
    plt.tight_layout()
    plt.show()

else:
    LOG.info("df_all is empty. Skipping plot generation.")

print("--- Consolidated Analysis Complete ---")

# Cell 19: Explainable AI

!pip install -q captum
from captum.attr import LayerGradCam, LayerAttribution, IntegratedGradients, Occlusion, LayerActivation
from captum.attr import visualization as viz

LOG.info("XAI: Starting setup.")

# Default initializations
model_to_explain = None
img_to_explain = None
vis_img = None
original_raw_img_for_pred = None

LOG.info(f"XAI: Using ENCODER_NAME='{ENCODER_NAME}', ENCODER_WEIGHTS='{ENCODER_WEIGHTS}', LOSS_TYPE='{LOSS_TYPE}'")
LOG.info(f"XAI: Using EPOCHS_BASELINE='{EPOCHS_BASELINE}', SAVED_MODELS_DIR='{SAVED_MODELS_DIR}'")
LOG.info(f"XAI: Using INPUT_CHANNELS='{INPUT_CHANNELS}', OUTPUT_CLASSES='{OUTPUT_CLASSES}', DEVICE='{DEVICE}'")

if 'build_model' not in globals():
    import segmentation_models_pytorch as smp
    def build_model(encoder, weights, in_channels, out_classes):
        LOG.info(f"XAI: Defining build_model for smp.Unet with encoder: {encoder}")
        model = smp.Unet(encoder_name=encoder, encoder_weights=weights, in_channels=in_channels, classes=out_classes, activation=None)
        model.to(DEVICE)
        return model
    LOG.info("XAI: build_model function (SMP Unet) defined locally for XAI cell because it was not found globally.")

if 'model_to_explain' in globals() and model_to_explain is not None and \
   'original_raw_img_for_pred' in globals() and original_raw_img_for_pred is not None and \
   'vis_img' in globals() and vis_img is not None:
    LOG.info("XAI Advanced: Using pre-existing 'model_to_explain' and associated data found in global scope.")
else:
    LOG.info("XAI Advanced: 'model_to_explain' not pre-set or data missing. Attempting to reload the baseline model from disk...")
    model_to_explain = None
    original_raw_img_for_pred = None
    vis_img = None
    try:
        run_name_baseline = f"baseline_{ENCODER_NAME}W{ENCODER_WEIGHTS}_{LOSS_TYPE}_e{str(EPOCHS_BASELINE)}_amp"
        model_path = os.path.join(SAVED_MODELS_DIR, f"{run_name_baseline}.pth")
        LOG.info(f"XAI Advanced: Constructed baseline model path for reloading: {model_path}")

        if os.path.exists(model_path):
            reloaded_model = build_model(ENCODER_NAME, ENCODER_WEIGHTS, INPUT_CHANNELS, OUTPUT_CLASSES)
            if reloaded_model:
                LOG.info(f"XAI Advanced: Loading state_dict from {model_path}...")
                state_dict = torch.load(model_path, map_location=DEVICE)
                if 'model_state_dict' in state_dict: state_dict = state_dict['model_state_dict']
                elif 'state_dict' in state_dict: state_dict = state_dict['state_dict']
                reloaded_model.load_state_dict(state_dict)
                reloaded_model.eval()
                model_to_explain = reloaded_model
                LOG.info(f"XAI Advanced: Baseline model '{run_name_baseline}' reloaded successfully into 'model_to_explain' and set to eval mode.")
            else: LOG.error("XAI Advanced: build_model function failed during reload attempt.")
        else: LOG.error(f"XAI Advanced: Baseline model path not found for reloading: {model_path}.")

        if model_to_explain and test_loader is not None:
            LOG.info("XAI Advanced: Fetching a sample batch from test_loader for the reloaded model...")
            try:
                sample_batch = next(iter(test_loader))
                input_images_tensor, _ = sample_batch
                original_raw_img_for_pred = input_images_tensor[0:1].to(DEVICE)
                LOG.info(f"XAI Advanced: Sample input image shape for XAI: {original_raw_img_for_pred.shape}")
                temp_vis_numpy = original_raw_img_for_pred.cpu().squeeze(0).permute(1, 2, 0).numpy()
                vis_img_channels_for_display = None
                if temp_vis_numpy.shape[2] == INPUT_CHANNELS:
                    if INPUT_CHANNELS == 1: vis_img_channels_for_display = np.stack((temp_vis_numpy.squeeze(),)*3, axis=-1)
                    elif INPUT_CHANNELS == 2: vis_img_channels_for_display = np.stack((temp_vis_numpy[:,:,0], temp_vis_numpy[:,:,1], (temp_vis_numpy[:,:,0]+temp_vis_numpy[:,:,1])/2), axis=-1)
                    elif INPUT_CHANNELS == 3: vis_img_channels_for_display = temp_vis_numpy
                    else: vis_img_channels_for_display = temp_vis_numpy[:,:,:3]
                else:
                     LOG.warning(f"Channel mismatch: INPUT_CHANNELS={INPUT_CHANNELS}, image has {temp_vis_numpy.shape[2]}. Using grayscale.")
                     gray_img = temp_vis_numpy.mean(axis=2) if temp_vis_numpy.ndim == 3 and temp_vis_numpy.shape[2] > 1 else temp_vis_numpy.squeeze()
                     vis_img_channels_for_display = np.stack((gray_img,)*3, axis=-1)
                min_val, max_val = vis_img_channels_for_display.min(), vis_img_channels_for_display.max()
                vis_img = (vis_img_channels_for_display - min_val) / (max_val - min_val + 1e-9) if (max_val - min_val) > 1e-9 else np.zeros_like(vis_img_channels_for_display)
                LOG.info("XAI Advanced: Sample input image and visualization image prepared.")
            except StopIteration: LOG.error("XAI Advanced: test_loader is empty."); original_raw_img_for_pred = None; vis_img = None
            except Exception as e_load_sample: LOG.error(f"XAI Advanced: Error processing sample: {e_load_sample}"); traceback.print_exc(); original_raw_img_for_pred = None; vis_img = None
        else:
            if not model_to_explain: LOG.warning("XAI Advanced: Model not loaded, cannot prepare data.")
            if test_loader is None: LOG.error("XAI Advanced: test_loader not available.")
            original_raw_img_for_pred = None; vis_img = None
    except Exception as e_main_load: LOG.error(f"XAI Advanced: Major error during model/data loading: {e_main_load}"); traceback.print_exc(); model_to_explain = None; original_raw_img_for_pred = None; vis_img = None

def visualize_layer_activation(attr, title="Layer Activation"):
    if attr.ndim == 4: attr = attr.squeeze(0).cpu().detach()
    elif attr.ndim == 3: attr = attr.cpu().detach()
    else: LOG.error(f"Unexpected activation map dimension: {attr.ndim}."); return
    num_channels = attr.shape[0]
    if num_channels == 0: LOG.warning(f"Activation map for '{title}' has 0 channels."); return
    if num_channels > 10:
        LOG.info(f"Visualizing mean activation over {num_channels} channels for '{title}'")
        attr_display = attr.mean(dim=0).numpy()
        fig, ax = plt.subplots(1, 1, figsize=(7, 7)); im = ax.imshow(attr_display, cmap='viridis')
        ax.set_title(title + " (Mean Activation)"); ax.axis('off'); fig.colorbar(im, ax=ax)
    else:
        LOG.info(f"Visualizing first {min(num_channels, 4)} channels for '{title}'")
        cols = min(num_channels, 4); fig, axs = plt.subplots(1, cols, figsize=(cols * 5, 5))
        if cols == 1: axs = [axs]
        for i in range(cols):
            im = axs[i].imshow(attr[i].numpy(), cmap='viridis'); axs[i].set_title(f"{title} (Ch {i})")
            axs[i].axis('off'); fig.colorbar(im, ax=axs[i])
    plt.tight_layout(); plt.show()

if model_to_explain and original_raw_img_for_pred is not None and vis_img is not None:
    LOG.info("--- Starting XAI Analysis with 'model_to_explain' ---")
    model_to_explain.eval()
    center_y = original_raw_img_for_pred.shape[2] // 2
    center_x = original_raw_img_for_pred.shape[3] // 2
    pixel_attribution_target = (0, center_y, center_x)
    LOG.info(f"XAI Target Pixel for pixel-specific methods: {pixel_attribution_target}")

    def agg_segmentation_wrapper_class0(inp_tensor):
        model_out = model_to_explain(inp_tensor)
        return model_out[:, 0, :, :].sum(dim=(1,2))

    LOG.info("--- Section 3: Expanded LayerGradCam Analysis ---")
    try: # 3.a Encoder Layer4
        if hasattr(model_to_explain, 'encoder') and hasattr(model_to_explain.encoder, 'layer4'):
            target_layer_l4 = model_to_explain.encoder.layer4[-1]
            LOG.info(f"Running LGC with encoder.layer4[-1] ({target_layer_l4.__class__.__name__})...")
            lgc_l4 = LayerGradCam(model_to_explain, target_layer_l4)
            attribution_l4 = lgc_l4.attribute(original_raw_img_for_pred, target=pixel_attribution_target)
            upsampled_attribution_l4 = LayerAttribution.interpolate(attribution_l4, original_raw_img_for_pred.shape[2:], interpolate_mode='bilinear')
            heatmap_l4 = upsampled_attribution_l4.squeeze().cpu().detach().numpy()
            if heatmap_l4.ndim == 3: heatmap_l4 = np.sum(heatmap_l4, axis=0)
            heatmap_l4_for_viz = np.expand_dims(heatmap_l4, axis=-1)
            fig_l4, axs_l4 = plt.subplots(1, 2, figsize=(12, 6))
            axs_l4[0].imshow(vis_img); axs_l4[0].set_title('Input Image'); axs_l4[0].axis('off')
            viz.visualize_image_attr(heatmap_l4_for_viz, vis_img, method='blended_heat_map', sign='absolute_value',
                                     show_colorbar=True, title=f'LGC (Encoder Layer4[-1], Pxl Target)',
                                     plt_fig_axis=(fig_l4, axs_l4[1]), use_pyplot=False)
            plt.tight_layout(); plt.show()
        else: LOG.warning("Skipping LGC on encoder.layer4[-1]: Layer not found.")
    except Exception as e: LOG.error(f"Error in LGC (Encoder Layer4[-1]): {e}"); traceback.print_exc()

    try: # 3.b Encoder Layer2
        if hasattr(model_to_explain, 'encoder') and hasattr(model_to_explain.encoder, 'layer2'):
            target_layer_l2 = model_to_explain.encoder.layer2[-1]
            LOG.info(f"Running LGC with encoder.layer2[-1] ({target_layer_l2.__class__.__name__})...")
            lgc_l2 = LayerGradCam(model_to_explain, target_layer_l2)
            attribution_l2 = lgc_l2.attribute(original_raw_img_for_pred, target=pixel_attribution_target)
            upsampled_attribution_l2 = LayerAttribution.interpolate(attribution_l2, original_raw_img_for_pred.shape[2:], interpolate_mode='bilinear')
            heatmap_l2 = upsampled_attribution_l2.squeeze().cpu().detach().numpy()
            if heatmap_l2.ndim == 3: heatmap_l2 = np.sum(heatmap_l2, axis=0)
            heatmap_l2_for_viz = np.expand_dims(heatmap_l2, axis=-1)
            fig_l2, axs_l2 = plt.subplots(1, 2, figsize=(12, 6))
            axs_l2[0].imshow(vis_img); axs_l2[0].set_title('Input Image'); axs_l2[0].axis('off')
            viz.visualize_image_attr(heatmap_l2_for_viz, vis_img, method='blended_heat_map', sign='absolute_value',
                                     show_colorbar=True, title=f'LGC (Encoder Layer2[-1], Pxl Target)',
                                     plt_fig_axis=(fig_l2, axs_l2[1]), use_pyplot=False)
            plt.tight_layout(); plt.show()
        else: LOG.warning("Skipping LGC on encoder.layer2[-1]: Layer not found.")
    except Exception as e: LOG.error(f"Error in LGC (Encoder Layer2[-1]): {e}"); traceback.print_exc()

    try: # 3.c Aggregated LGC
        if hasattr(model_to_explain, 'encoder') and hasattr(model_to_explain.encoder, 'layer4'):
            target_layer_agg = model_to_explain.encoder.layer4[-1]
            LOG.info(f"Running LGC with encoder.layer4[-1] for aggregated Class 0 prediction...")
            lgc_agg = LayerGradCam(agg_segmentation_wrapper_class0, target_layer_agg)
            attribution_agg = lgc_agg.attribute(original_raw_img_for_pred, target=None) # Corrected: target=None
            upsampled_attribution_agg = LayerAttribution.interpolate(attribution_agg, original_raw_img_for_pred.shape[2:], interpolate_mode='bilinear')
            heatmap_agg = upsampled_attribution_agg.squeeze().cpu().detach().numpy()
            if heatmap_agg.ndim == 3: heatmap_agg = np.sum(heatmap_agg, axis=0)
            heatmap_agg_for_viz = np.expand_dims(heatmap_agg, axis=-1)
            fig_agg, axs_agg = plt.subplots(1, 2, figsize=(12, 6))
            axs_agg[0].imshow(vis_img); axs_agg[0].set_title('Input Image'); axs_agg[0].axis('off')
            viz.visualize_image_attr(heatmap_agg_for_viz, vis_img, method='blended_heat_map', sign='absolute_value',
                                     show_colorbar=True, title='LGC (Encoder Layer4[-1], Aggregated Class 0)',
                                     plt_fig_axis=(fig_agg, axs_agg[1]), use_pyplot=False)
            plt.tight_layout(); plt.show()
        else: LOG.warning("Skipping LGC for aggregated prediction: encoder.layer4[-1] not found.")
    except Exception as e: LOG.error(f"Error in LGC (Aggregated Class 0): {e}"); traceback.print_exc()

    LOG.info("--- Section 3.d: LayerGradCam on Decoder Layers (SMP U-Net) ---")
    decoder_layers_to_inspect = []
    if hasattr(model_to_explain, 'decoder'):
        decoder = model_to_explain.decoder
        if hasattr(decoder, 'blocks') and isinstance(decoder.blocks, torch.nn.ModuleList) and len(decoder.blocks) > 0:
            num_decoder_blocks = len(decoder.blocks)
            # Deepest decoder block conv (e.g., blocks[0] in SMP)
            if hasattr(decoder.blocks[0], 'conv1') and isinstance(decoder.blocks[0].conv1, torch.nn.Sequential) and len(decoder.blocks[0].conv1) > 0 and isinstance(decoder.blocks[0].conv1[0], torch.nn.Conv2d):
                decoder_layers_to_inspect.append( (f"Decoder Block 0 (Deepest) - Conv1[0]", decoder.blocks[0].conv1[0]) )
            # Middle decoder block conv
            if num_decoder_blocks > 2 :
                mid_block_idx = num_decoder_blocks // 2
                if hasattr(decoder.blocks[mid_block_idx], 'conv1') and isinstance(decoder.blocks[mid_block_idx].conv1, torch.nn.Sequential) and len(decoder.blocks[mid_block_idx].conv1) > 0 and isinstance(decoder.blocks[mid_block_idx].conv1[0], torch.nn.Conv2d):
                    decoder_layers_to_inspect.append( (f"Decoder Block {mid_block_idx} (Mid) - Conv1[0]", decoder.blocks[mid_block_idx].conv1[0]) )
            # Shallowest decoder block conv (e.g., blocks[-1] in SMP)
            if hasattr(decoder.blocks[-1], 'conv1') and isinstance(decoder.blocks[-1].conv1, torch.nn.Sequential) and len(decoder.blocks[-1].conv1) > 0 and isinstance(decoder.blocks[-1].conv1[0], torch.nn.Conv2d):
                decoder_layers_to_inspect.append( (f"Decoder Block {num_decoder_blocks-1} (Shallowest) - Conv1[0]", decoder.blocks[-1].conv1[0]) )
    else: LOG.warning("Model does not have a 'decoder' or 'decoder.blocks' attribute as expected for SMP U-Net.")

    if hasattr(model_to_explain, 'segmentation_head'):
        seg_head_module = model_to_explain.segmentation_head
        if isinstance(seg_head_module, torch.nn.Conv2d):
            decoder_layers_to_inspect.append( ("Final Segmentation Head Conv", seg_head_module) )
        elif isinstance(seg_head_module, torch.nn.Sequential) and len(seg_head_module) > 0 and isinstance(seg_head_module[0], torch.nn.Conv2d):
            decoder_layers_to_inspect.append( ("Final Segmentation Head Conv (from Sequential)", seg_head_module[0]) )
            LOG.info("Identified segmentation_head Conv2d within an nn.Sequential module.")
        else:
            LOG.warning(f"Could not identify nn.Conv2d in model_to_explain.segmentation_head. Type is {type(seg_head_module)}.")
    else:
        LOG.warning("Model does not have a 'segmentation_head' attribute directly.")

    if not decoder_layers_to_inspect: LOG.error("No SMP U-Net decoder layers identified for LGC.")
    for stage_name, target_layer_decoder in decoder_layers_to_inspect:
        try:
            LOG.info(f"Running LGC for {stage_name} (Layer: {target_layer_decoder.__class__.__name__})...")
            lgc_decoder = LayerGradCam(model_to_explain, target_layer_decoder)
            attribution_dec = lgc_decoder.attribute(original_raw_img_for_pred, target=pixel_attribution_target)
            upsampled_attribution_dec = LayerAttribution.interpolate(attribution_dec, original_raw_img_for_pred.shape[2:], interpolate_mode='bilinear')
            heatmap_dec = upsampled_attribution_dec.squeeze().cpu().detach().numpy()
            if heatmap_dec.ndim == 3: heatmap_dec = np.sum(heatmap_dec, axis=0)
            heatmap_dec_for_viz = np.expand_dims(heatmap_dec, axis=-1)
            fig_dec, axs_dec = plt.subplots(1, 2, figsize=(12, 6))
            axs_dec[0].imshow(vis_img); axs_dec[0].set_title('Input Image'); axs_dec[0].axis('off')
            viz.visualize_image_attr(heatmap_dec_for_viz, vis_img, method='blended_heat_map', sign='absolute_value',
                                     show_colorbar=True, title=f'LGC ({stage_name}, Pxl Target)',
                                     plt_fig_axis=(fig_dec, axs_dec[1]), use_pyplot=False)
            plt.tight_layout(); plt.show()
        except Exception as e: LOG.error(f"Error in LGC for {stage_name}: {e}"); traceback.print_exc()

    LOG.info("--- Section 3.e: Visualizing Skip Connection Activations (SMP U-Net) ---")
    skip_connection_source_layers = {}
    if hasattr(model_to_explain, 'encoder'):
        encoder = model_to_explain.encoder
        if hasattr(encoder, 'relu'): skip_connection_source_layers["Encoder Initial Conv Output (Relu)"] = encoder.relu
        if hasattr(encoder, 'layer1'): skip_connection_source_layers["Encoder Layer1 Output"] = encoder.layer1
        if hasattr(encoder, 'layer2'): skip_connection_source_layers["Encoder Layer2 Output"] = encoder.layer2
        if hasattr(encoder, 'layer3'): skip_connection_source_layers["Encoder Layer3 Output"] = encoder.layer3
    if not skip_connection_source_layers: LOG.warning("No skip connection source layers identified.")
    for desc, layer_obj in skip_connection_source_layers.items():
        try:
            LOG.info(f"Visualizing activations for Skip Connection from: {desc}")
            layer_act = LayerActivation(model_to_explain, layer_obj)
            activation_map = layer_act.attribute(original_raw_img_for_pred)
            visualize_layer_activation(activation_map, title=f"Activations from {desc}")
        except Exception as e: LOG.error(f"Error visualizing skip connection for {desc}: {e}"); traceback.print_exc()

    LOG.info("Clearing CUDA cache and collecting garbage after LGC and Activations...")
    if DEVICE.type == 'cuda': torch.cuda.empty_cache()
    gc.collect()

    LOG.info("--- Section 4: Integrated Gradients Analysis ---")
    try: # 4.a IG Zero Baseline
        LOG.info("Running IG for target pixel (Zero Baseline)...")
        ig_zero = IntegratedGradients(model_to_explain)
        baseline_zero = torch.zeros_like(original_raw_img_for_pred).to(DEVICE)
        n_steps_ig = 25
        LOG.info(f"Using n_steps = {n_steps_ig} for IG (Zero Baseline).")
        attributions_ig_pixel_zero, delta_zero = ig_zero.attribute(original_raw_img_for_pred, baselines=baseline_zero,
                                             target=pixel_attribution_target, n_steps=n_steps_ig, return_convergence_delta=True)
        LOG.info(f"IG (Zero Baseline) attributions shape: {attributions_ig_pixel_zero.shape}, Convergence Delta: {delta_zero.mean().item():.4f}")
        heatmap_ig_abs_sum_zero = attributions_ig_pixel_zero.abs().sum(dim=1).squeeze().cpu().detach().numpy()
        heatmap_ig_for_viz_zero = np.expand_dims(heatmap_ig_abs_sum_zero, axis=-1)
        fig_ig_zero, axs_ig_zero = plt.subplots(1, 2, figsize=(12, 6))
        axs_ig_zero[0].imshow(vis_img); axs_ig_zero[0].set_title('Input Image'); axs_ig_zero[0].axis('off')
        viz.visualize_image_attr(heatmap_ig_for_viz_zero, vis_img, method='blended_heat_map', sign='absolute_value',
                                 show_colorbar=True, title=f'IG (Zero Baseline, Pxl Target, Abs Sum, n_steps={n_steps_ig})',
                                 plt_fig_axis=(fig_ig_zero, axs_ig_zero[1]), use_pyplot=False)
        plt.tight_layout(); plt.show()
        if INPUT_CHANNELS == 3:
            channel_names = ['VV (IG Zero)', 'VH (IG Zero)', 'DEM (IG Zero)']
            fig_ig_ch_z, axs_ig_ch_z = plt.subplots(1, INPUT_CHANNELS + 1, figsize=(18, 5))
            axs_ig_ch_z[0].imshow(vis_img); axs_ig_ch_z[0].set_title('Input Image'); axs_ig_ch_z[0].axis('off')
            for i in range(INPUT_CHANNELS):
                attr_ch_z = attributions_ig_pixel_zero[:, i, :, :].squeeze().cpu().detach().numpy()
                viz.visualize_image_attr(np.expand_dims(attr_ch_z, axis=-1), vis_img, method='blended_heat_map', sign='all',
                                         show_colorbar=True, title=f'{channel_names[i]} (Pxl Target, n_steps={n_steps_ig})',
                                         plt_fig_axis=(fig_ig_ch_z, axs_ig_ch_z[i+1]), use_pyplot=False)
            plt.tight_layout(); plt.show()
    except Exception as e: LOG.error(f"Error in IG (Zero Baseline): {e}"); traceback.print_exc()

    try: # 4.b IG Gaussian Baseline
        LOG.info("Running IG for target pixel (Gaussian Noise Baseline)...")
        ig_gauss = IntegratedGradients(model_to_explain)
        baseline_gaussian = torch.randn_like(original_raw_img_for_pred).to(DEVICE) * 0.1
        n_steps_ig = 25
        LOG.info(f"Using n_steps = {n_steps_ig} for IG (Gaussian Baseline).")
        attributions_ig_pixel_gaussian, delta_gaussian = ig_gauss.attribute(original_raw_img_for_pred, baselines=baseline_gaussian,
                                                                  target=pixel_attribution_target, n_steps=n_steps_ig, return_convergence_delta=True)
        LOG.info(f"IG (Gaussian Baseline) attributions shape: {attributions_ig_pixel_gaussian.shape}, Convergence Delta: {delta_gaussian.mean().item():.4f}")
        heatmap_ig_abs_sum_gaussian = attributions_ig_pixel_gaussian.abs().sum(dim=1).squeeze().cpu().detach().numpy()
        heatmap_ig_for_viz_gaussian = np.expand_dims(heatmap_ig_abs_sum_gaussian, axis=-1)
        fig_ig_gauss, axs_ig_gauss = plt.subplots(1, 2, figsize=(12, 6))
        axs_ig_gauss[0].imshow(vis_img); axs_ig_gauss[0].set_title('Input Image'); axs_ig_gauss[0].axis('off')
        viz.visualize_image_attr(heatmap_ig_for_viz_gaussian, vis_img, method='blended_heat_map', sign='absolute_value',
                                 show_colorbar=True, title=f'IG (Gaussian Baseline, Pxl Target, Abs Sum, n_steps={n_steps_ig})',
                                 plt_fig_axis=(fig_ig_gauss, axs_ig_gauss[1]), use_pyplot=False)
        plt.tight_layout(); plt.show()
    except Exception as e: LOG.error(f"Error in IG (Gaussian Baseline): {e}"); traceback.print_exc()

    LOG.info("Clearing CUDA cache and collecting garbage after IG...")
    if DEVICE.type == 'cuda': torch.cuda.empty_cache()
    gc.collect()

    LOG.info("--- Section 5: Occlusion Sensitivity Analysis ---")
    try:
        LOG.info("Running Occlusion sensitivity for target pixel...")
        occlusion = Occlusion(model_to_explain)
        patch_size = 16; stride_val = 8
        sliding_window_shapes = (original_raw_img_for_pred.shape[1], patch_size, patch_size)
        strides = (original_raw_img_for_pred.shape[1], stride_val, stride_val)
        occlusion_baselines = 0
        LOG.info(f"Occlusion: Patch Size={patch_size}, Stride={stride_val}")
        attributions_occ = occlusion.attribute(original_raw_img_for_pred,
                                               sliding_window_shapes=sliding_window_shapes,
                                               strides=strides,
                                               target=pixel_attribution_target,
                                               baselines=occlusion_baselines)
        LOG.info(f"Occlusion attributions shape: {attributions_occ.shape}")
        heatmap_occ_sum = attributions_occ.sum(dim=1).squeeze().cpu().detach().numpy()
        heatmap_occ_for_viz = np.expand_dims(heatmap_occ_sum, axis=-1)
        fig_occ, axs_occ = plt.subplots(1, 2, figsize=(12, 6))
        axs_occ[0].imshow(vis_img); axs_occ[0].set_title('Input Image'); axs_occ[0].axis('off')
        viz.visualize_image_attr(heatmap_occ_for_viz, vis_img, method='blended_heat_map', sign='all',
                                 cmap='coolwarm', show_colorbar=True, title=f'Occlusion (Pxl Target)',
                                 plt_fig_axis=(fig_occ, axs_occ[1]), use_pyplot=False)
        plt.tight_layout(); plt.show()
    except Exception as e:
        LOG.error(f"Error in Occlusion Sensitivity: {e}"); traceback.print_exc()
        if "CUDA out of memory" in str(e): LOG.error("Occlusion OOM. Try smaller patch/larger stride.")

    LOG.info("--- All XAI Analyses in this cell Complete ---")
else:
    LOG.error("XAI Advanced: Skipping XAI execution due to missing model_to_explain, original_raw_img_for_pred, or vis_img.")
    if not model_to_explain: LOG.info("Reason: model_to_explain is None.")
    if not original_raw_img_for_pred: LOG.info("Reason: original_raw_img_for_pred is None.")
    if not vis_img: LOG.info("Reason: vis_img is None.")

# --- XAI Complete ---

"""# PART 4: TRAINING WITH 2 CHANNELS (SAR VV and VH) ONLY"""

# Cell 4A: Imports, Constants, Paths, Manual Device Setup

INPUT_CHANNELS = 2
SAVED_MODELS_DIR = '/content/drive/MyDrive/trained_models/new_amp/2_ch'

LOG.info(f"Configuration Constants Set. INPUT_CHANNELS = {INPUT_CHANNELS}")

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
LOG.info(f"Using device: {DEVICE}")
if DEVICE.type == 'cuda':
  torch.cuda.manual_seed_all(SEED)

os.makedirs(SAVED_MODELS_DIR, exist_ok=True)
LOG.info(f"Model weights will be saved in: {SAVED_MODELS_DIR}")

# Cell 4B: Utility Functions


def get_split_data_paths_mmflood(split_dir):
    # MODIFIED: Removed DEM path handling
    sar_dir = os.path.join(split_dir, 'sar')
    # REMOVED: dem_dir = os.path.join(split_dir, 'dem')
    mask_dir = os.path.join(split_dir, 'mask')
    LOG.info(f"Looking for data in: {sar_dir}, {mask_dir}")
    if not all(os.path.exists(d) for d in [sar_dir, mask_dir]):
        LOG.warning(f"WARNING: Missing sar or mask subdirs in {split_dir}.")
        return [], []

    sar_paths = sorted(glob.glob(os.path.join(sar_dir, '*.tif')))
    if not sar_paths:
        LOG.warning(f"WARNING: No .tif files in {sar_dir}")
        return [], [] # MODIFIED: Return two lists

    mask_paths = []
    valid_sar_paths = []
    for sar_p in sar_paths:
        base_name = os.path.basename(sar_p)
        mask_p = os.path.join(mask_dir, base_name)
        if os.path.exists(mask_p):
            valid_sar_paths.append(sar_p)
            mask_paths.append(mask_p)
    LOG.info(f"Found {len(valid_sar_paths)} valid SAR/mask pairs in {split_dir}.")
    return valid_sar_paths, mask_paths # MODIFIED: Return two lists
LOG.info("Defined get_split_data_paths_mmflood.")



LOG.info("Utilities Defined")

# Cell 4C: Verify DataPaths

LOG.info(f"--- Loading DataPaths ---")
try:
    train_sar_paths, train_mask_paths = get_split_data_paths_mmflood(TRAIN_DIR)
    val_sar_paths, val_mask_paths = get_split_data_paths_mmflood(VAL_DIR)
    test_sar_paths, test_mask_paths = get_split_data_paths_mmflood(TEST_DIR)

    if not train_sar_paths:
      LOG.warning("CRITICAL WARNING: No training DataPaths found.")
    LOG.info("DataPaths loaded.")
except Exception as e:
     LOG.error(f"ERROR loading DataPaths: {e}")
     train_sar_paths, train_mask_paths = [], []
     val_sar_paths, val_mask_paths = [], []
     test_sar_paths, test_mask_paths = [], []

LOG.info("DataPaths Loaded")

# Cell 4D: Statistics Calculation & Scaling Percentiles

# --- Helper Function: Statistics & Pos Weight ---
def calculate_stats_and_pos_weight_mmflood(sar_paths, mask_paths):
    """Calculates mean/std for SAR(VV,VH) and pos_weight using rasterio."""
    LOG.info("Calculating normalization stats (VV, VH) and pos_weight...")
    if not sar_paths or not mask_paths:
        LOG.error("ERROR: Empty data paths. Returning defaults.")
        return None, None, torch.tensor([1.0], device=DEVICE)

    # MODIFIED: pixel_sum and pixel_sum_sq will be for 2 channels (VV, VH)
    pixel_sum = torch.zeros(2, dtype=torch.float64)
    pixel_sum_sq = torch.zeros(2, dtype=torch.float64)
    total_pixels_per_channel = 0
    positive_pixels = 0
    total_mask_pixels = 0
    num_files = len(sar_paths)

    for i in tqdm(range(num_files), desc="Processing Stats (Mean/Std/Wgt)     "):
        sar_path, mask_path = sar_paths[i], mask_paths[i]
        try:
            with rasterio.open(sar_path) as src_sar:
              sar_data_np = src_sar.read().astype(np.float32)
              assert src_sar.count == 2, f"SAR file {sar_path} has {src_sar.count} channels, expected 2."

            with rasterio.open(mask_path) as src_mask:
              mask_data_np = src_mask.read(1, out_shape=(sar_data_np.shape[1], sar_data_np.shape[2])).astype(np.uint8)
              assert src_mask.count == 1
            assert sar_data_np.shape[1:] == mask_data_np.shape
            img_combined = torch.from_numpy(sar_data_np).to(torch.float64)
            img_combined = torch.nan_to_num(img_combined, nan=0.0, posinf=0.0, neginf=0.0)

            pixel_sum += img_combined.sum(dim=[1, 2])
            pixel_sum_sq += (img_combined ** 2).sum(dim=[1, 2])
            total_pixels_per_channel += img_combined.shape[1] * img_combined.shape[2]

            mask = torch.from_numpy(mask_data_np)
            positive_pixels += (mask > 0).sum().item()
            total_mask_pixels += mask.numel()
        except Exception as e: LOG.error(f"Error processing index {i} for stats ({sar_path}): {e}")
        continue

    if total_pixels_per_channel == 0:
      LOG.error("ERROR: No pixels processed for stats.")
      return None, None, torch.tensor([1.0], device=DEVICE)

    mean = (pixel_sum / total_pixels_per_channel).float()
    variance = (pixel_sum_sq / total_pixels_per_channel) - (mean.double() ** 2)
    std = torch.sqrt(torch.clamp(variance, min=1e-8)).float()
    negative_pixels = total_mask_pixels - positive_pixels

    if positive_pixels == 0 or total_mask_pixels == 0:
      LOG.warning("No positive/mask pixels. pos_weight=1.0")
      pos_weight = torch.tensor([1.0], device=DEVICE)
    else:
      pos_weight_val = negative_pixels / positive_pixels
      pos_weight = torch.tensor([pos_weight_val], device=DEVICE)
      LOG.info(f"Positive Pixels: {positive_pixels}, Negative: {negative_pixels}")

    LOG.info(f"Calculated Mean (VV, VH): {mean.tolist()}")
    LOG.info(f"Calculated Std (VV, VH): {std.tolist()}")
    LOG.info(f"Calculated pos_weight: {pos_weight.item():.4f} (device: {pos_weight.device})")
    if torch.any(std <= 1e-6):
      LOG.warning("WARNING: Calculated std near zero.")
    return mean, std, pos_weight


# --- Helper Function: Calculate Scaling Percentiles ---
def calculate_scaling_percentiles(sar_paths, q_min=1.0, q_max=99.0):
    """Calculates the q_min and q_max percentiles for SAR(VV,VH) channels."""
    LOG.info(f"Calculating scaling percentiles ({q_min}th, {q_max}th) across training data (SAR only)...")
    if not sar_paths:
        LOG.error("ERROR: Empty SAR paths for percentile calculation. Returning None.")
        return None

    vv_pixels = []
    vh_pixels = []
    num_files = len(sar_paths)

    for i in tqdm(range(num_files), desc="Processing Stats (Percentiles)        "):
        sar_path = sar_paths[i]
        try:
            with rasterio.open(sar_path) as src_sar:
                if src_sar.count != 2:
                    LOG.warning(f"SAR file {sar_path} has {src_sar.count} channels, expected 2. Skipping for percentile calc.")
                    continue
                sar_data = src_sar.read().astype(np.float32)
                vv_pixels.append(sar_data[0][~np.isnan(sar_data[0])].flatten())
                vh_pixels.append(sar_data[1][~np.isnan(sar_data[1])].flatten())
        except Exception as e:
          LOG.error(f"Error processing index {i} for percentiles ({sar_path}): {e}"); continue

    if not vv_pixels or not vh_pixels:
        LOG.error("ERROR: No pixel data collected for SAR percentile calculation.")
        return None

    try:
        all_vv = np.concatenate(vv_pixels)
        all_vh = np.concatenate(vh_pixels)

        p_vv = np.percentile(all_vv, [q_min, q_max])
        p_vh = np.percentile(all_vh, [q_min, q_max])

        percentiles = {
            'vv': {'p_min': p_vv[0], 'p_max': p_vv[1]},
            'vh': {'p_min': p_vh[0], 'p_max': p_vh[1]},
        }
        LOG.info(f"Calculated Percentiles (SAR only): VV={percentiles['vv']}, VH={percentiles['vh']}")
        del all_vv, all_vh, vv_pixels, vh_pixels; gc.collect()
        return percentiles
    except Exception as e:
        LOG.error(f"Error calculating final percentiles: {e}"); gc.collect()
        return None

# --- Execute Calculation ---
NORM_MEAN = [0.5] * INPUT_CHANNELS
NORM_STD = [0.5] * INPUT_CHANNELS
pos_weight = torch.tensor([1.0], device=DEVICE)
NORM_PERCENTILES = None

if train_sar_paths:
    # MODIFIED: Call with sar_paths and mask_paths only
    train_mean, train_std, pos_weight_calc = calculate_stats_and_pos_weight_mmflood(
        train_sar_paths, train_mask_paths # REMOVED: train_dem_paths
    )
    if train_mean is not None and train_std is not None and pos_weight_calc is not None:
         NORM_MEAN = train_mean.tolist()
         NORM_STD = train_std.tolist()
         pos_weight = pos_weight_calc
    else:
         LOG.error("Mean/Std/Weight calculation failed. Using defaults.")

    # MODIFIED: Call with sar_paths only
    NORM_PERCENTILES = calculate_scaling_percentiles(
        train_sar_paths # REMOVED: train_dem_paths
    )
    if NORM_PERCENTILES is None:
         LOG.error("Percentile calculation failed. robust_scale will use runtime calculation.")
else:
    LOG.info("Skipping stats and percentile calculation as no training paths were loaded.")

LOG.info(f"Final NORM_MEAN (for {INPUT_CHANNELS} channels): {NORM_MEAN}")
LOG.info(f"Final NORM_STD (for {INPUT_CHANNELS} channels): {NORM_STD}")
LOG.info(f"Final pos_weight: {pos_weight.item():.4f} (Device: {pos_weight.device})")
LOG.info(f"Final NORM_PERCENTILES: {NORM_PERCENTILES}")

LOG.info("--- Statistics & Percentiles Calculations Complete ---")

# Cell 4E: Augmentation Definitions

# --- Helper Function: Get Transforms ---
def get_transforms(is_train, norm_mean, norm_std):
    """Gets augmentation pipeline components."""
    # MODIFIED: INPUT_CHANNELS is now 2, this check remains valid
    if not isinstance(norm_mean, list) or len(norm_mean) != INPUT_CHANNELS or \
       not isinstance(norm_std, list) or len(norm_std) != INPUT_CHANNELS:
        LOG.warning(f"WARNING: NORM_MEAN/STD invalid for {INPUT_CHANNELS} channels. Using defaults [0.5].")
        norm_mean = [0.5] * INPUT_CHANNELS; norm_std = [0.5] * INPUT_CHANNELS

    spatial_transforms = []
    pixel_transforms = []

    if is_train:
        spatial_transforms.extend([
            A.HorizontalFlip(p=0.5), A.VerticalFlip(p=0.5), A.RandomRotate90(p=0.5),
            A.ElasticTransform(alpha=1, sigma=50, interpolation=1, p=0.2, approximate=False),
            A.GridDistortion(num_steps=5, distort_limit=(-0.3, 0.3), interpolation=1, p=0.2),
            A.Rotate(limit=30, p=0.5, border_mode=0, interpolation=1, mask_interpolation=0),
        ])
        pixel_transforms.extend([
            A.OneOf([
                A.GaussianBlur(blur_limit=(3, 7), p=0.5),
                A.RandomBrightnessContrast(p=0.2),
                A.CoarseDropout(num_holes_range=(1, 8), hole_height_range=(0.1, 0.25), hole_width_range=(0.1, 0.25), p=1.0),
            ], p=0.5),
        ])
    post_transforms = [
        A.Normalize(mean=norm_mean, std=norm_std, max_pixel_value=1.0),
        ToTensorV2()
    ]

    # MODIFIED: Removed 'dem' from additional_targets
    return A.Compose(spatial_transforms + pixel_transforms,
                     additional_targets={'mask': 'mask'}), \
           A.Compose(post_transforms)

# --- Execute Transform Definition ---
try:
    if 'NORM_MEAN' not in locals() or 'NORM_STD' not in locals(): raise NameError("Run Cell 2 first.")
    train_spatial_pixel_transform, train_post_transform = get_transforms(True, NORM_MEAN, NORM_STD)
    val_test_spatial_pixel_transform, val_test_post_transform = get_transforms(False, NORM_MEAN, NORM_STD)
    # MODIFIED: Removed 'dem' from cl_buffer_spatial_transform additional_targets
    cl_buffer_spatial_transform = A.Compose([], additional_targets={'mask': 'mask'})
    LOG.info("Transformation pipelines defined.")
except NameError as e:
    LOG.error(f"ERROR defining transforms: {e}.")
    train_spatial_pixel_transform=None; train_post_transform=None
    val_test_spatial_pixel_transform=None
    val_test_post_transform=None
    cl_buffer_spatial_transform=None
except Exception as e:
    LOG.error(f"ERROR defining transforms: {e}")
    traceback.print_exc()
    train_spatial_pixel_transform=None
    train_post_transform=None
    val_test_spatial_pixel_transform=None
    val_test_post_transform=None
    cl_buffer_spatial_transform=None

LOG.info("--- Augmentations Setup Complete ---")

# Cell 4F: Dataset Class Definition

class MMFloodDataset(Dataset):
    """Dataset for MMFlood, handles loading, resizing, scaling, augmentation."""
    # MODIFIED: Removed dem_paths from __init__
    def __init__(self, sar_paths, mask_paths, # REMOVED: dem_paths
                 spatial_pixel_transform=None, post_transform=None,
                 is_cl_buffer_data=False,
                 norm_percentiles=None):
        self.sar_paths = sar_paths
        # REMOVED: self.dem_paths = dem_paths
        self.mask_paths = mask_paths
        self.spatial_pixel_transform = spatial_pixel_transform
        self.post_transform = post_transform
        self.is_cl_buffer_data = is_cl_buffer_data
        self.norm_percentiles = norm_percentiles
        if norm_percentiles is None:
             LOG.warning("MMFloodDataset initialized without pre-calculated percentiles. Robust scaling will compute at runtime.")

        if not sar_paths: LOG.warning("Warning: Initializing MMFloodDataset with empty SAR paths.")
        # MODIFIED: Assertion for two lists
        assert len(sar_paths) == len(mask_paths), "Mismatched SAR and Mask paths!"

    def __len__(self): return len(self.sar_paths)

    def __getitem__(self, idx):
        # MODIFIED: Get only sar_path and mask_path
        sar_path, mask_path = self.sar_paths[idx], self.mask_paths[idx] # REMOVED: dem_path
        try:
            with rasterio.open(sar_path) as src:
              # Ensure SAR is read as 2 channels for INPUT_CHANNELS=2
              # MODIFIED: out_shape=(2, ...) to explicitly handle 2 channels for SAR
              sar_data = src.read(out_shape=(2, IMAGE_HEIGHT, IMAGE_WIDTH), resampling=rasterio.enums.Resampling.bilinear).astype(np.float32) if src.height != IMAGE_HEIGHT or src.width != IMAGE_WIDTH else src.read().astype(np.float32)
              # MODIFIED: Assert SAR data has 2 channels as expected by INPUT_CHANNELS=2
              assert sar_data.shape[0] == 2, f"SAR data at {sar_path} has {sar_data.shape[0]} channels, expected 2."
              sar_data = np.transpose(sar_data, (1, 2, 0)) # To HWC for albumentations

            # REMOVED: DEM data loading and processing block
            # with rasterio.open(dem_path) as src: ...

            with rasterio.open(mask_path) as src:
              mask_data = src.read(1, out_shape=(IMAGE_HEIGHT, IMAGE_WIDTH), resampling=rasterio.enums.Resampling.nearest).astype(np.uint8) if src.height != IMAGE_HEIGHT or src.width != IMAGE_WIDTH else src.read(1).astype(np.uint8)
              mask_data = np.expand_dims(mask_data, axis=-1)
              assert src.count==1

            mask_data = (mask_data > 0).astype(np.float32)

            if self.spatial_pixel_transform:
                # MODIFIED: Transform call without 'dem'
                transformed = self.spatial_pixel_transform(image=sar_data, mask=mask_data)
                sar_sp, mask_sp = transformed['image'], transformed['mask'] # REMOVED: dem_sp
            else:
                sar_sp, mask_sp = sar_data, mask_data # REMOVED: dem_sp

            vv_p_min, vv_p_max = None, None
            vh_p_min, vh_p_max = None, None
            # REMOVED: dem_p_min, dem_p_max
            if self.norm_percentiles:
                 vv_p_min = self.norm_percentiles.get('vv', {}).get('p_min')
                 vv_p_max = self.norm_percentiles.get('vv', {}).get('p_max')
                 vh_p_min = self.norm_percentiles.get('vh', {}).get('p_min')
                 vh_p_max = self.norm_percentiles.get('vh', {}).get('p_max')
                 # REMOVED: DEM percentile fetching

            # Apply robust_scale to SAR channels
            sar_sp[..., 0] = robust_scale(sar_sp[..., 0], p_min=vv_p_min, p_max=vv_p_max) # VV (index 0)
            sar_sp[..., 1] = robust_scale(sar_sp[..., 1], p_min=vh_p_min, p_max=vh_p_max) # VH (index 1)
            # REMOVED: DEM scaling: dem_sp[..., 0] = robust_scale(...)

            # MODIFIED: image_combined is now just sar_sp (which is H, W, 2)
            image_combined = sar_sp

            if self.post_transform:
                final_transformed = self.post_transform(image=image_combined)
                final_image = final_transformed['image'] # -> [C, H, W] where C is INPUT_CHANNELS (2)
                final_mask = torch.from_numpy(mask_sp.transpose(2, 0, 1)).float()
            elif self.is_cl_buffer_data:
                 final_image = torch.from_numpy(image_combined.transpose(2, 0, 1)).float()
                 final_mask = torch.from_numpy(mask_sp.transpose(2, 0, 1)).float()
            else:
                 LOG.warning(f"Warning: No post_transform/CL flag index {idx}. Raw scaled tensors.")
                 final_image = torch.from_numpy(image_combined.transpose(2, 0, 1)).float()
                 final_mask = torch.from_numpy(mask_sp.transpose(2, 0, 1)).float()

            exp_img = (INPUT_CHANNELS, IMAGE_HEIGHT, IMAGE_WIDTH); exp_mask = (OUTPUT_CLASSES, IMAGE_HEIGHT, IMAGE_WIDTH)
            if final_image.shape != exp_img or final_mask.shape != exp_mask:
                raise ValueError(f"Final shape mismatch. Img: {final_image.shape} (exp {exp_img}), Mask: {final_mask.shape} (exp {exp_mask}) for {sar_path}")

            return final_image, final_mask

        except Exception as e:
            LOG.error(f"!!! ERROR in Dataset __getitem__ index {idx} !!!")
            # MODIFIED: Print sar_path, mask_path only
            print(f"Paths: SAR={sar_path}, Mask={mask_path}") # REMOVED: DEM={dem_path}
            print(f"Error: {e}")
            traceback.print_exc()
            return None

LOG.info("--- Dataset Definition Complete ---")

# Cell 4E: Dataset Instantiation, CL Split, Initial Loaders (Modified for Percentiles)

if 'collate_fn_skip_error' not in globals():
  raise NameError("collate_fn_skip_error not defined.")

train_dataset_full = None
val_dataset = None
test_dataset = None
train_dataset_for_cl_split = None
LOG.info("Instantiating Datasets...")

try:
    # MODIFIED: Required paths changed
    required = ['train_sar_paths', 'train_mask_paths', # REMOVED: train_dem_paths
                'train_spatial_pixel_transform', 'train_post_transform',
                'cl_buffer_spatial_transform', 'val_test_spatial_pixel_transform',
                'val_test_post_transform', 'NORM_PERCENTILES']
    if not all(k in globals() for k in required):
         missing = [k for k in required if k not in globals()]
         raise NameError(f"Paths, transforms, or NORM_PERCENTILES missing: {missing}. Run previous cells.")

    if train_sar_paths:
        # MODIFIED: Pass train_mask_paths instead of train_dem_paths
        train_dataset_full = MMFloodDataset(train_sar_paths, train_mask_paths, # REMOVED: train_dem_paths
                                            train_spatial_pixel_transform, train_post_transform,
                                            norm_percentiles=NORM_PERCENTILES)
        train_dataset_for_cl_split = MMFloodDataset(train_sar_paths, train_mask_paths, # REMOVED: train_dem_paths
                                                    None, None, is_cl_buffer_data=True,
                                                    norm_percentiles=None) # NORM_PERCENTILES might be needed if scaling is done for buffer data
    if val_sar_paths:
        # MODIFIED: Pass val_mask_paths
        val_dataset = MMFloodDataset(val_sar_paths, val_mask_paths, # REMOVED: val_dem_paths
                                     val_test_spatial_pixel_transform, val_test_post_transform,
                                     norm_percentiles=NORM_PERCENTILES)
    if test_sar_paths:
        # MODIFIED: Pass test_mask_paths
        test_dataset = MMFloodDataset(test_sar_paths, test_mask_paths, # REMOVED: test_dem_paths
                                      val_test_spatial_pixel_transform, val_test_post_transform,
                                      norm_percentiles=NORM_PERCENTILES)

    LOG.info("Dataset instances created.")
    LOG.info(f"Train size: {len(train_dataset_full) if train_dataset_full else 'N/A'}")
    LOG.info(f"Val size: {len(val_dataset) if val_dataset else 'N/A'}")
    LOG.info(f"Test size: {len(test_dataset) if test_dataset else 'N/A'}")
    LOG.info(f"CL Split size: {len(train_dataset_for_cl_split) if train_dataset_for_cl_split else 'N/A'}")

except NameError as e: LOG.warning(f"ERROR creating Datasets: {e}.")
except Exception as e: LOG.warning(f"ERROR creating Datasets: {e}")

task_loaders = []
task_datasets = []
if train_dataset_full and len(train_dataset_full) > 0 and globals().get('NUM_TASKS', 0) > 0:
    num_train_samples = len(train_dataset_full)
    LOG.info(f"Splitting {num_train_samples} train samples randomly for {NUM_TASKS} tasks (for baseline CL)...")
    if num_train_samples < NUM_TASKS:
      LOG.warning(f"NUM_TASKS > num_samples. Setting NUM_TASKS = {num_train_samples}")
      NUM_TASKS = num_train_samples
    if NUM_TASKS > 0:
        samples_per_task = num_train_samples // NUM_TASKS
        task_indices = [list(range(i*samples_per_task, (i+1)*samples_per_task)) for i in range(NUM_TASKS)]
        remainder = num_train_samples % NUM_TASKS
        if remainder > 0:
             if len(task_indices) < NUM_TASKS: task_indices.append([])
             task_indices[-1].extend(range(NUM_TASKS*samples_per_task, num_train_samples))
        task_indices = [inds for inds in task_indices if inds]; LOG.info(f"Resulting in {len(task_indices)} non-empty random tasks.")
        for i, indices in enumerate(task_indices):
          LOG.info(f"Random Task {i+1}: {len(indices)} samples")
        task_datasets = [Subset(train_dataset_full, indices) for indices in task_indices]
else: LOG.info("Skipping initial random CL task splitting.")

num_workers = min(os.cpu_count() // 2, 4)
pin_memory = True if DEVICE.type == 'cuda' else False
LOG.info(f"Creating DataLoaders: workers={num_workers}, pin_memory={pin_memory}...")

train_loader_baseline = DataLoader(train_dataset_full, batch_size=BATCH_SIZE, shuffle=True, num_workers=num_workers, pin_memory=pin_memory, drop_last=True, collate_fn=collate_fn_skip_error) if train_dataset_full else None
val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers, pin_memory=pin_memory, collate_fn=collate_fn_skip_error) if val_dataset else None
test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers, pin_memory=pin_memory, collate_fn=collate_fn_skip_error) if test_dataset else None
if task_datasets:
  task_loaders = [DataLoader(tds, batch_size=BATCH_SIZE, shuffle=True, num_workers=num_workers, pin_memory=pin_memory, drop_last=True, collate_fn=collate_fn_skip_error) for tds in task_datasets]
  LOG.info(f"Created {len(task_loaders)} random task loaders.")
else:
  LOG.info("No random task datasets, skipping task loader creation.")

LOG.info(f"Loader lengths: Baseline Train={len(train_loader_baseline) if train_loader_baseline else 'N/A'}, Val={len(val_loader) if val_loader else 'N/A'}, Test={len(test_loader) if test_loader else 'N/A'}, Random Tasks={len(task_loaders)}")
LOG.info("--- Datasets and DataLoaders Complete (Using Percentiles) ---")

# Cell 4F: Model, Loss, Optimizer Definitions & Test

# --- Helper Function: Build Model ---
def build_model(encoder, weights, in_channels, out_classes): # in_channels will be 2
    """Builds an SMP U-Net model and moves it to DEVICE."""
    LOG.info(f"Building U-Net with encoder: {encoder}, weights: {weights}, input_channels: {in_channels}")
    try:
        model = smp.Unet(encoder_name=encoder, encoder_weights=weights, in_channels=in_channels, classes=out_classes, activation=None)
        model.to(DEVICE)
        LOG.info(f"Model moved to {DEVICE}")
        return model
    except Exception as e: LOG.error(f"ERROR building model: {e}"); traceback.print_exc(); return None

# --- Helper Function: Get Loss ---
# (No changes needed in get_loss_function for input channels)
def get_loss_function(loss_type, pos_weight_val=None, focal_gamma=FOCAL_GAMMA, tversky_alpha=TVERSKY_ALPHA, tversky_beta=TVERSKY_BETA):
    """Gets the specified loss function and moves it to DEVICE."""
    LOG.info(f"Creating Loss Function: {loss_type}")
    criterion = None
    try:
        if loss_type == 'bce':
            weight_tensor = pos_weight_val if isinstance(pos_weight_val, torch.Tensor) and pos_weight_val.numel() == 1 and pos_weight_val.item() > 0 else None
            if weight_tensor is not None: LOG.info(f"Using BCEWithLogitsLoss with pos_weight = {weight_tensor.item():.4f} (Device: {weight_tensor.device})")
            else: LOG.info(f"Using standard BCEWithLogitsLoss.")
            criterion = nn.BCEWithLogitsLoss(pos_weight=weight_tensor)
        elif loss_type == 'focal':
            criterion = smp.losses.FocalLoss(mode='binary', gamma=focal_gamma, reduction='mean')
            LOG.info(f"Using FocalLoss with gamma = {focal_gamma}")
        elif loss_type == 'tversky':
             criterion = smp.losses.TverskyLoss(mode='binary', alpha=tversky_alpha, beta=tversky_beta, gamma=1.0, from_logits=True, smooth=1e-6)
             LOG.info(f"Using TverskyLoss with alpha = {tversky_alpha}, beta = {tversky_beta}")
        else:
            LOG.warning(f"WARNING: Unknown loss type '{loss_type}'. Defaulting to BCEWithLogitsLoss.")
            criterion = nn.BCEWithLogitsLoss()
        if criterion:
             criterion.to(DEVICE)
             LOG.info(f"Loss function moved to {DEVICE}")
    except Exception as e: LOG.error(f"ERROR creating loss function '{loss_type}': {e}"); traceback.print_exc(); criterion = None
    return criterion

if 'create_poly_optimizer' not in globals(): raise NameError("create_poly_optimizer not defined. Run Cell 1B.") # Corrected cell ref
LOG.info("create_poly_optimizer previously defined.")

LOG.info("Building template model and criterion for testing...")
# INPUT_CHANNELS is now 2
model_template = build_model(ENCODER_NAME, ENCODER_WEIGHTS, INPUT_CHANNELS, OUTPUT_CLASSES)
criterion_template = get_loss_function(LOSS_TYPE, pos_weight_val=pos_weight)

torch.cuda.empty_cache()

if model_template is not None and criterion_template is not None:
    LOG.info("Performing test for model and loss...")
    try:
        # Dummy input will have INPUT_CHANNELS (2)
        dummy_input = torch.randn(BATCH_SIZE, INPUT_CHANNELS, IMAGE_HEIGHT, IMAGE_WIDTH, device=DEVICE)
        dummy_target = torch.randint(0, 2, (BATCH_SIZE, OUTPUT_CLASSES, IMAGE_HEIGHT, IMAGE_WIDTH), device=DEVICE).float()

        output = model_template(dummy_input)
        LOG.info(f"Model test:")
        LOG.info(f"Input shape: {dummy_input.shape}, Device: {dummy_input.device}")
        LOG.info(f"Output shape: {output.shape}, Device: {output.device}")

        LOG.info(f"Criterion test:")
        LOG.info(f"Target shape: {dummy_target.shape}, Device: {dummy_target.device}")
        if isinstance(criterion_template, nn.BCEWithLogitsLoss) and criterion_template.pos_weight is not None:
             print(f"Criterion pos_weight Device: {criterion_template.pos_weight.device}")

        loss_val = criterion_template(output, dummy_target)
        LOG.info(f"Loss function test successful. Example loss: {loss_val.item()}")

    except Exception as e: LOG.error(f"ERROR during model/loss DEVICE test: {e}"); traceback.print_exc()
    finally:
        LOG.info("Cleaning up template objects..."); del model_template, criterion_template
        if 'dummy_input' in locals(): del dummy_input
        if 'output' in locals(): del output
        if 'dummy_target' in locals(): del dummy_target
        if 'loss_val' in locals(): del loss_val
        gc.collect(); torch.cuda.empty_cache() if DEVICE.type == 'cuda' else None
else: LOG.error("ERROR: Template Model or Criterion could not be built. Cannot proceed.")

LOG.info("--- Model/Loss/Optimizer Definitions Complete ---")

# Cell 4H: Baseline Model Training and Evaluation (Load Existing JSON Results)


# --- Safely get params ---
encoder_name = globals().get('ENCODER_NAME', 'unknown_encoder')
encoder_weights = str(globals().get('ENCODER_WEIGHTS', 'unknown'))
loss_type = globals().get('LOSS_TYPE', 'unknown_loss')
epochs_baseline_str = str(globals().get('EPOCHS_BASELINE', 'unknown'))

# --- Define Experiment Details ---
experiment_details_dict = globals().setdefault('experiment_details_dict', {})
experiment_details_dict.update({ # Update with current baseline info
    "encoder": encoder_name, "encoder_weights": encoder_weights, "loss_type": loss_type,
    "baseline_epochs": epochs_baseline_str,
     # Keep other relevant details if already present
})

# --- Define Unique Run Name & Model Path ---
run_name_baseline = f"baseline_{encoder_name}W{encoder_weights}_{loss_type}_e{epochs_baseline_str}_2ch"
LOG.info(f"Attempting Baseline Run: {run_name_baseline}")
baseline_save_path = os.path.join(SAVED_MODELS_DIR, f"{run_name_baseline}.pth")
baseline_last_save_path = baseline_save_path.replace('.pth', '_last.pth') # Path for _last model

# --- Define UNIQUE JSON Filename for THIS RUN ---
json_run_filename = os.path.join(SAVED_MODELS_DIR, f"{run_name_baseline}_results.json")
LOG.info(f"Checking for existing results file: {os.path.basename(json_run_filename)}")

# --- Skip Logic (Prioritize loading valid JSON results) ---
skip_training_baseline = False
baseline_eval_needed = True # Assume needed unless valid JSON loaded
# Default metrics placeholder
_def_metrics = {'loss': np.nan, 'dice': np.nan, 'iou': np.nan, 'f1': np.nan, 'precision': np.nan, 'recall': np.nan, 'mcc': np.nan, 'time': 0, 'mem_mb': 0}
results.setdefault(run_name_baseline, _def_metrics.copy()) # Ensure entry exists
histories.setdefault(run_name_baseline, {}) # Ensure entry exists

if os.path.exists(json_run_filename):
    LOG.warning(f"Results JSON exists: {os.path.basename(json_run_filename)}. Attempting to load.")
    try:
         with open(json_run_filename, 'r') as f:
              previous_data = json.load(f)
         prev_metrics = previous_data.get("final_test_metrics", {})
         # --- Check for a key metric (like f1) to validate ---
         if prev_metrics and 'f1' in prev_metrics and not math.isnan(float(prev_metrics['f1'])): # Check if F1 is present and not NaN
             LOG.info(f"Valid previous results loaded from JSON for '{run_name_baseline}'. Skipping training and evaluation.")
             skip_training_baseline = True
             baseline_eval_needed = False
             # --- Load data into memory dictionaries ---
             results[run_name_baseline] = prev_metrics
             histories[run_name_baseline] = previous_data.get("validation_history", {})
             # Try to find corresponding model path and store it
             if os.path.exists(baseline_save_path):
                  saved_model_paths[run_name_baseline] = baseline_save_path
             elif os.path.exists(baseline_last_save_path):
                  saved_model_paths[run_name_baseline] = baseline_last_save_path
             else:
                  LOG.warning(f"Loaded results from JSON, but corresponding model file (.pth or _last.pth) not found for {run_name_baseline}")
         else:
              LOG.warning("Previous JSON found but contained invalid/incomplete metrics. Will proceed to check for model file.")
    except Exception as json_e:
         LOG.error(f"Error loading or parsing previous JSON {os.path.basename(json_run_filename)}: {json_e}. Will proceed to check for model file.")

# Check model file only if JSON didn't lead to skipping everything
if not skip_training_baseline and not baseline_eval_needed: # Should only happen if JSON loaded successfully
     pass # Already decided to skip both
elif os.path.exists(baseline_save_path):
    LOG.warning(f"Model file exists: {os.path.basename(baseline_save_path)}. JSON not found/invalid. Training skipped, evaluation needed.")
    skip_training_baseline = True
    baseline_eval_needed = True # Ensure eval is needed
    saved_model_paths[run_name_baseline] = baseline_save_path
elif os.path.exists(baseline_last_save_path): # Check for _last only if .pth not found
     LOG.warning(f"Last model file exists: {os.path.basename(baseline_last_save_path)}. JSON not found/invalid. Training skipped, evaluation needed.")
     skip_training_baseline = True
     baseline_eval_needed = True # Ensure eval is needed
     saved_model_paths[run_name_baseline] = baseline_last_save_path
else:
    LOG.info("No existing model file or valid results JSON found. Training & Evaluation required.")
    skip_training_baseline = False
    baseline_eval_needed = True


# --- Execute Training (if needed) ---
model_out_base = None; time_baseline=0; peak_mem_baseline=0 # Init vars
if can_proceed and not skip_training_baseline:
    # ... (Training logic remains the same) ...
    # It calculates model_out_base, history_baseline, time_baseline, peak_mem_baseline
    LOG.info(f"=== Training Baseline Model ({run_name_baseline}) ===")
    baseline_model = build_model(ENCODER_NAME, ENCODER_WEIGHTS, INPUT_CHANNELS, OUTPUT_CLASSES)
    if baseline_model:
        optimizer_baseline = create_poly_optimizer(baseline_model, base_lr=LEARNING_RATE)
        if optimizer_baseline:
            scheduler_baseline = None
            try:
              num_batches_epoch = len(train_loader_baseline)
              num_training_steps = EPOCHS_BASELINE * num_batches_epoch
              scheduler_baseline = optim.lr_scheduler.PolynomialLR(optimizer_baseline, total_iters=num_training_steps, power=0.9)
              LOG.info(f"Baseline scheduler iters={num_training_steps}")
            except Exception as e_sch:
              LOG.error(f"ERROR Baseline scheduler: {e_sch}")
            if scheduler_baseline:
                if DEVICE.type == 'cuda':
                  torch.cuda.reset_peak_memory_stats(DEVICE)
                model_out_base, history_baseline, time_baseline = train_baseline(baseline_model, train_loader_baseline, val_loader, criterion, optimizer_baseline, scheduler_baseline, EPOCHS_BASELINE, DEVICE, model_save_path=baseline_save_path, patience=PATIENCE)
                histories[run_name_baseline] = history_baseline
                peak_mem_baseline = torch.cuda.max_memory_allocated(DEVICE)/(1024**2) if DEVICE.type == 'cuda' else 0.0
                # Update saved path map after training
                if os.path.exists(baseline_save_path):
                  saved_model_paths[run_name_baseline] = baseline_save_path
                elif os.path.exists(baseline_last_save_path):
                  saved_model_paths[run_name_baseline] = baseline_last_save_path
            else:
              LOG.error("Baseline skip: scheduler fail.")
              baseline_eval_needed=False
              results[run_name_baseline]=_def_metrics.copy()
        else:
          LOG.error("Baseline skip: optimizer fail.")
          baseline_eval_needed=False
          results[run_name_baseline]=_def_metrics.copy()
    else:
      LOG.error("Baseline skip: model build fail.")
      baseline_eval_needed=False
      results[run_name_baseline]=_def_metrics.copy()
elif not can_proceed:
    LOG.error(f"Cannot run baseline '{run_name_baseline}': Preconditions failed.")
    baseline_eval_needed=False # Can't eval if preconditions fail
elif skip_training_baseline and baseline_eval_needed:
    LOG.info(f"Baseline training skipped for {run_name_baseline}, proceeding to evaluation.")
    # Get time/mem from results dict if previously loaded, otherwise keep 0
    time_baseline = results[run_name_baseline].get('time', 0)
    peak_mem_baseline = results[run_name_baseline].get('mem_mb', 0)
elif skip_training_baseline and not baseline_eval_needed:
     LOG.info(f"Baseline training AND evaluation skipped for {run_name_baseline} based on loaded JSON.")


# --- Execute Evaluation (if needed and possible) ---
evaluation_performed_this_run = False # Flag to check if eval was actually done
if baseline_eval_needed and can_proceed and test_loader is not None:
    LOG.info(f"--- Evaluating Baseline ({run_name_baseline}) on TEST set ---")
    eval_model = None
    try:
        # Load model (either from training or file)
        if model_out_base: # Use model trained in this session if available
            eval_model = model_out_base
            LOG.info("Evaluating model trained this session.")
        # Otherwise, load from saved path if it exists in our map
        elif saved_model_paths.get(run_name_baseline) and os.path.exists(saved_model_paths[run_name_baseline]):
             model_file_to_load = saved_model_paths[run_name_baseline]
             LOG.info(f"Loading existing baseline model from: {os.path.basename(model_file_to_load)}")
             eval_model = build_model(ENCODER_NAME, ENCODER_WEIGHTS, INPUT_CHANNELS, OUTPUT_CLASSES)
             if eval_model:
                  state_dict = torch.load(model_file_to_load, map_location=DEVICE, weights_only=False) # Set weights_only
                  if 'model_state_dict' in state_dict: state_dict = state_dict['model_state_dict'] # Handle checkpoint dicts
                  eval_model.load_state_dict(state_dict)
                  eval_model.to(DEVICE)
                  LOG.info("Loaded weights.")
             else:
              LOG.error("Failed to build model for evaluation.")
        else:
          LOG.error("Cannot evaluate baseline: No model from training and no valid saved path found/exists.")

        # Perform evaluation if model is ready
        if eval_model:
            eval_model.eval() # Ensure eval mode
            eval_output = evaluate_model(eval_model, test_loader, criterion, DEVICE)
            loss_base_test, dice_base_test, iou_base_test, f1_base_test, precision_base_test, recall_base_test, mcc_base_test, \
            tp_base, tn_base, fp_base, fn_base = eval_output

            # --- Store results in memory dictionary ---
            current_run_results = {
                'loss': loss_base_test, 'dice': dice_base_test, 'iou': iou_base_test, 'f1': f1_base_test,
                'precision': precision_base_test, 'recall': recall_base_test, 'mcc': mcc_base_test,
                'tp': tp_base, 'tn': tn_base, 'fp': fp_base, 'fn': fn_base,
                # Make sure time/mem reflect actual run (0 if only eval)
                'time': time_baseline if not skip_training_baseline else 0, # Time is training time
                'mem_mb': peak_mem_baseline if not skip_training_baseline else 0 # Mem is peak during training
            }
            results[run_name_baseline] = current_run_results # Update global dict

            # --- Log results ---
            LOG.info("--------------------------------------------------------------------------------------")
            LOG.info(f"Final Baseline TEST -> L:{loss_base_test:.4f}, F1:{f1_base_test:.4f}, IoU:{iou_base_test:.4f}, MCC:{mcc_base_test:.4f}")
            LOG.info(f"P:{precision_base_test:.4f}, R:{recall_base_test:.4f}, PeakMem:{current_run_results['mem_mb']:.2f} MB, Time:{current_run_results['time']:.2f} s") # Use stored results
            LOG.info(f"Confusion Matrix (Test): TP={tp_base:.0f}, TN={tn_base:.0f}, FP={fp_base:.0f}, FN={fn_base:.0f}")
            LOG.info("--------------------------------------------------------------------------------------")
            evaluation_performed_this_run = True # Mark that eval happened
        else:
             LOG.warning("Skipping evaluation as model could not be loaded or trained.")
             # Ensure results dict has defaults if eval skipped
             results[run_name_baseline].update(_def_metrics.copy())

    except Exception as e_eval:
        LOG.error(f"ERROR Baseline TEST evaluation: {e_eval}", exc_info=True)
        results[run_name_baseline].update(_def_metrics.copy())

    # --- Cleanup loaded evaluation model ---
    if eval_model and not model_out_base: del eval_model


# --- Save results to JSON if evaluation was performed or training happened ---
if evaluation_performed_this_run or not skip_training_baseline:
    experiment_details_dict["last_update"] = datetime.now().isoformat()
    save_metrics_to_json(
         filename=json_run_filename, # Use the unique filename
         run_name=run_name_baseline,
         run_result_summary=results.get(run_name_baseline),
         run_cl_detail_matrix=None,
         run_history=histories.get(run_name_baseline),
         experiment_details=experiment_details_dict
    )
else:
     LOG.info(f"Skipping JSON save for {run_name_baseline} as neither training nor evaluation was performed in this session.")


# --- Final Cleanup after Baseline ---
if 'model_out_base' in locals() and model_out_base: del model_out_base;
if 'baseline_model' in locals() and baseline_model: del baseline_model;
if 'optimizer_baseline' in locals() and optimizer_baseline: del optimizer_baseline;
if 'scheduler_baseline' in locals() and scheduler_baseline: del scheduler_baseline;
gc.collect(); torch.cuda.empty_cache() if DEVICE.type == 'cuda' else None

LOG.info(f"===== Baseline Processing Complete for {run_name_baseline} =====")